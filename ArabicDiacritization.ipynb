{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "arkp5fRSugqX",
        "6FYOh6DJIN8c",
        "-3TFVs4cMxzB",
        "gdmbb6BZE-bZ",
        "8Fe3iAsJBvul",
        "jQaeTHGFu5jP",
        "6gabh-nEBvum",
        "AKtWMj_8Bvuo",
        "CaiGj1iABvuo",
        "jhjGwLnMBvup",
        "IxFC7MGkBvup",
        "RkuXIf9gBvuq",
        "Wxjny4NYBvur",
        "zBq-Q3BbBvus",
        "IRip94S8Bvut",
        "rwToeMTlNj1h",
        "VU5H0m34wr5l",
        "acmOxZoKxJcc",
        "HwSgLjRz1qJ-",
        "3c1bk6OR5Z5C",
        "vtUnWyf_28gM",
        "Trw1jYUG-FoJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **About**\n",
        "\n",
        "### **Arabic Diacritization**\n",
        "\n",
        "This project aims to develop and compare different models for Arabic diacritization, a crucial task in natural language processing for the Arabic language. The project explores the use of various deep learning architectures, including biLSTM, CBHG + HMM, and HMM alone to predict the correct diacritics for Arabic text.\n",
        "\n",
        "**Team Members:**\n",
        "\n",
        "* Ayman Sahyoun - 202110003\n",
        "* Mohannad AlBasyouni - 202010589\n",
        "* Aws AlHantouli - 202110250\n",
        "\n",
        "--------------\n",
        "\n",
        "\n",
        "#### 1. **HMM**: Our first model, Hidden Markov Model (HMM) alone, is a statistical model that uses a Markov chain to predict the most likely sequence of diacritics for a given Arabic text.\n",
        "- Used 5-gram model\n",
        "- Word Error Rate (WER): ~33.30%\n",
        "- Diacritization Error Rate (DER): ~66.70%\n",
        "- Reference: [Anwarvic/Arabic-Tashkeela-Model](https://github.com/Anwarvic/Arabic-Tashkeela-Model)\n",
        "\n",
        "#### 2. **CBHG + HMM**: Our second model, the Convolutional Neural Network (CNN) - Bidirectional Gated Recurrent Unit (GRU) - Hidden Markov Model (HMM) hybrid model combines the strengths of deep learning and traditional machine learning to achieve high accuracy in diacritization.\n",
        "- Epochs: 20\n",
        "- Word Error Rate (WER): ~19.24%\n",
        "- Diacritization Error Rate (DER): ~24.68%\n",
        "- Reference: [almodhfer/Arabic_Diacritization](https://github.com/almodhfer/Arabic_Diacritization)\n",
        "\n",
        "#### 3. **BiLSTM**: Our third model, Bidirectional Long Short-Term Memory (BiLSTM), is a type of recurrent neural network that uses two LSTM layers to learn the patterns in Arabic text and predict diacritics.\n",
        "- Epochs: 2\n",
        "- Average Training Loss: ~0.0210\n",
        "- Average Validation Loss: ~0.0141\n",
        "- Reference: [MAHMOUDRR707/Arabic-Diacritization-Text](https://github.com/MAHMOUDRR707/Arabic-Diacritization-Text)\n",
        "\n",
        "#### 4. **RNN-Based**: Our fourth (simple) model, Recurrent Neural Network-Based, is a derivative of the BiLSTM model base, but now with actual testing and evaluation values\n",
        "- Epochs: 5\n",
        "- Word Error Rate (WER): ~81.05%\n",
        "- Diacritization Error Rate (DER): ~18.95%\n",
        "- Reference: [MAHMOUDRR707/Arabic-Diacritization-Text](https://github.com/MAHMOUDRR707/Arabic-Diacritization-Text)\n",
        "--------------"
      ],
      "metadata": {
        "id": "FqGzK0c6qpH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "arkp5fRSugqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy>=1.24.0 matplotlib seaborn plotly wordcloud arabic-reshaper python-bidi nltk jupyter arabic-reshaper[with-fonttools] fonttools graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV6KltkmKlA0",
        "outputId": "d58b8a45-a275-497f-8a2a-12cd2b76c9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrUDpdX0HmoE",
        "outputId": "6b6e914d-5b8f-4bb9-821f-fc63b730518e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "#Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#PreProcessing\n",
        "import re\n",
        "from glob import glob\n",
        "\n",
        "#Combined Model (CBHG + HMM)\n",
        "from typing import Optional, List, Dict, Tuple, Union, Any\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.optim import Adam  # Correct PyTorch Adam optimizer import\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict, deque\n",
        "from enum import Enum\n",
        "\n",
        "#LSTM\n",
        "from tensorflow.keras.utils import to_categorical # for the LSTM it will be used to convert the diacritics integers to one-hot encodings.\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, GRU, TimeDistributed , Input\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the required dependancies\n",
        "WITH_EXTRA_TRAIN = False\n",
        "\n",
        "with open('/content/ARABIC_LETTERS_LIST.pickle', 'rb') as file:  #Contains a list of Arabic letters\n",
        "    ARABIC_LETTERS_LIST = pickle.load(file)\n",
        "with open('/content/DIACRITICS_LIST.pickle', 'rb') as file:  #Contains a list of all diacritics\n",
        "    DIACRITICS_LIST = pickle.load(file)\n",
        "if not WITH_EXTRA_TRAIN:\n",
        "    with open('/content/RNN_SMALL_CHARACTERS_MAPPING.pickle', 'rb') as file:  #Contains a dictionary, mapping each diactritics class to its unique integer\n",
        "        CHARACTERS_MAPPING = pickle.load(file)\n",
        "else:\n",
        "    with open('/content/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:  #Contains a dictionary, mapping each integer to its unique diactritics class\n",
        "        CHARACTERS_MAPPING = pickle.load(file)\n",
        "with open('/content/RNN_CLASSES_MAPPING.pickle', 'rb') as file:  #Contains a dictionary, mapping each character to its unique integer (Without using using the extra training dataset)\n",
        "    CLASSES_MAPPING = pickle.load(file)\n",
        "with open('/content/RNN_REV_CLASSES_MAPPING.pickle', 'rb') as file:  #Contains a dictionary, mapping each character to its unique integer (Using using the extra training dataset)\n",
        "    REV_CLASSES_MAPPING = pickle.load(file)"
      ],
      "metadata": {
        "id": "bhURSvPoHwxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/raw_data'):\n",
        "    os.makedirs('/content/raw_data')\n",
        "\n",
        "!mv /content/train.txt /content/raw_data/\n",
        "\n",
        "\n",
        "# Read the data that will be used for training\n",
        "train_raw = None\n",
        "with open('/content/raw_data/train.txt', 'r') as file:\n",
        "    train_raw = file.readlines()\n",
        "\n",
        "# Read the data that will be used for validation\n",
        "val_raw = None\n",
        "with open('/content/val.txt', 'r') as file:\n",
        "    val_raw = file.readlines()"
      ],
      "metadata": {
        "id": "voq2dBu5NIw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EDA & Visuals"
      ],
      "metadata": {
        "id": "6FYOh6DJIN8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    entries = [e for e in text.split('\\n') if e.strip()]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'text': entries,\n",
        "        'length': [len(e) for e in entries],\n",
        "        'is_arabic': [any('\\u0600' <= c <= '\\u06FF' for c in e) for e in entries],\n",
        "        'word_count': [len(e.split()) for e in entries]\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "df = load_and_process_text(\"/content/raw_data/train.txt\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FWqFMQApJ5OI",
        "outputId": "5c0f6f20-77c9-46c9-edd4-7b234054a877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  length  is_arabic  \\\n",
              "0  وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ ...     226       True   \n",
              "1  قَالَ أَبُو زَيْدٍ أَهْلُ تِهَامَةَ يُؤَنِّثُو...     162       True   \n",
              "2  بِمَنْزِلَةِ أَهْلِ الذِّمَّةِ إذَا دَخَلُوا ق...     178       True   \n",
              "3  الْمَسْأَلَةُ الْخَامِسَةُ قَوْله تَعَالَى : {...     408       True   \n",
              "4  قَوْلُهُ ( وَاَلَّذِي لَا يَتَغَابَنُ النَّاسُ...     553       True   \n",
              "\n",
              "   word_count  \n",
              "0          30  \n",
              "1          21  \n",
              "2          23  \n",
              "3          55  \n",
              "4          75  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a950b61-1eff-4d01-880d-4931c2d63f04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "      <th>is_arabic</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ ...</td>\n",
              "      <td>226</td>\n",
              "      <td>True</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قَالَ أَبُو زَيْدٍ أَهْلُ تِهَامَةَ يُؤَنِّثُو...</td>\n",
              "      <td>162</td>\n",
              "      <td>True</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>بِمَنْزِلَةِ أَهْلِ الذِّمَّةِ إذَا دَخَلُوا ق...</td>\n",
              "      <td>178</td>\n",
              "      <td>True</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>الْمَسْأَلَةُ الْخَامِسَةُ قَوْله تَعَالَى : {...</td>\n",
              "      <td>408</td>\n",
              "      <td>True</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>قَوْلُهُ ( وَاَلَّذِي لَا يَتَغَابَنُ النَّاسُ...</td>\n",
              "      <td>553</td>\n",
              "      <td>True</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a950b61-1eff-4d01-880d-4931c2d63f04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a950b61-1eff-4d01-880d-4931c2d63f04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a950b61-1eff-4d01-880d-4931c2d63f04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9743b34-7e8d-4eba-9f00-d3b4e90d463c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9743b34-7e8d-4eba-9f00-d3b4e90d463c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9743b34-7e8d-4eba-9f00-d3b4e90d463c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50000,\n        \"samples\": [\n          \"\\u0648\\u064e\\u0642\\u064e\\u0648\\u0652\\u0644\\u064f\\u0647\\u064f : \\u0648\\u064e\\u0627\\u0646\\u0652\\u0642\\u0650\\u0636\\u064e\\u0627\\u0626\\u0650\\u0647\\u064e\\u0627 \\u0623\\u064e\\u064a\\u0652 \\u0627\\u0644\\u0652\\u0639\\u0650\\u062f\\u0651\\u064e\\u0629\\u0650 \\u0628\\u0650\\u0623\\u064e\\u0646\\u0652 \\u0634\\u064e\\u0647\\u0650\\u062f\\u064f\\u0648\\u0627 \\u0623\\u064e\\u0646\\u0651\\u064e \\u0641\\u064f\\u0644\\u064e\\u0627\\u0646\\u064b\\u0627 \\u0645\\u064f\\u0631\\u064e\\u0627\\u062f\\u064f\\u0647\\u064f \\u064a\\u064f\\u0631\\u064e\\u0627\\u062c\\u0650\\u0639\\u064f \\u0632\\u064e\\u0648\\u0652\\u062c\\u064e\\u062a\\u064e\\u0647\\u064f \\u0628\\u064e\\u0639\\u0652\\u062f\\u064e \\u0627\\u0646\\u0652\\u0642\\u0650\\u0636\\u064e\\u0627\\u0621\\u0650 \\u0639\\u0650\\u062f\\u0651\\u064e\\u062a\\u0650\\u0647\\u064e\\u0627 .\",\n          \"\\u0627\\u0644\\u062a\\u0651\\u064e\\u0635\\u064e\\u0631\\u0651\\u064f\\u0641\\u064f \\u0641\\u0650\\u064a\\u0647\\u0650 \\u0648\\u064e\\u0644\\u064e\\u0648\\u0652 \\u0644\\u0650\\u062c\\u0650\\u0647\\u064e\\u0629\\u0650 \\u0627\\u0644\\u0652\\u0648\\u064e\\u0642\\u0652\\u0641\\u0650 \\u0648\\u064e\\u0644\\u064e\\u0627 \\u064a\\u064e\\u0628\\u0652\\u0639\\u064f\\u062f\\u064f \\u0623\\u064e\\u0646\\u0652 \\u064a\\u064e\\u0641\\u0652\\u0639\\u064e\\u0644\\u064e \\u0628\\u0650\\u0647\\u0650 \\u0645\\u064e\\u0627 \\u064a\\u064e\\u0641\\u0652\\u0639\\u064e\\u0644\\u064f \\u0628\\u0650\\u0627\\u0644\\u062b\\u0651\\u064e\\u0645\\u064e\\u0631\\u064e\\u0629\\u0650 \\u0627\\u0644\\u0652\\u063a\\u064e\\u064a\\u0652\\u0631\\u0650 \\u0627\\u0644\\u0652\\u0645\\u064f\\u0624\\u064e\\u0628\\u0651\\u064e\\u0631\\u064e\\u0629\\u0650 \\u0625\\u0630\\u064e\\u0627 \\u062f\\u064e\\u062e\\u064e\\u0644\\u064e\\u062a\\u0652 \\u0641\\u0650\\u064a \\u0627\\u0644\\u0652\\u0648\\u064e\\u0642\\u0652\\u0641\\u0650 .\",\n          \"\\u062d\\u064e\\u0635\\u064e\\u0644\\u064e \\u0622\\u062e\\u0650\\u0631\\u064f \\u0627\\u0644\\u0652\\u0643\\u064e\\u0644\\u064e\\u0627\\u0645\\u0650 \\u0648\\u064e\\u0647\\u0650\\u064a\\u064e \\u0623\\u064e\\u062c\\u0652\\u0646\\u064e\\u0628\\u0650\\u064a\\u0651\\u064e\\u0629\\u064c .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 395,\n        \"min\": 6,\n        \"max\": 12456,\n        \"num_unique_values\": 1930,\n        \"samples\": [\n          562,\n          1915,\n          1941\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_arabic\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50,\n        \"min\": 1,\n        \"max\": 1640,\n        \"num_unique_values\": 319,\n        \"samples\": [\n          268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_patterns(df):\n",
        "    \"\"\"Create pattern visualizations using Plotly\"\"\"\n",
        "    # Calculate various text patterns\n",
        "    pattern_data = {\n",
        "        'Has numbers': df['text'].str.contains('\\d').sum(),\n",
        "        'Has parentheses': df['text'].str.contains('[()]').sum(),\n",
        "        'Has punctuation': df['text'].str.contains('[،.:؛]').sum(),\n",
        "        'Pure Arabic': df['is_arabic'].sum()\n",
        "    }\n",
        "\n",
        "    fig = px.sunburst(\n",
        "        names=list(pattern_data.keys()),\n",
        "        parents=[''] * len(pattern_data),\n",
        "        values=list(pattern_data.values()),\n",
        "        title='Text Pattern Distribution'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "visualize_patterns(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "bs_oiq9EINQb",
        "outputId": "eecae289-ecdd-4fc8-dbd6-10df797edd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1be73daa-14cb-4724-91a7-9d7c38bb3537\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1be73daa-14cb-4724-91a7-9d7c38bb3537\")) {                    Plotly.newPlot(                        \"1be73daa-14cb-4724-91a7-9d7c38bb3537\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}\\u003cbr\\u003evalue=%{value}\\u003cbr\\u003eparent=%{parent}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"Has numbers\",\"Has parentheses\",\"Has punctuation\",\"Pure Arabic\"],\"name\":\"\",\"parents\":[\"\",\"\",\"\",\"\"],\"values\":[16168,29768,45208,50000],\"type\":\"sunburst\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Text Pattern Distribution\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1be73daa-14cb-4724-91a7-9d7c38bb3537');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = {\n",
        "    'Total Entries': len(df),\n",
        "    'Average Length': df['length'].mean(),\n",
        "    'Average Word Count': df['word_count'].mean(),\n",
        "    'Max Length': df['length'].max(),\n",
        "    'Min Length': df['length'].min()\n",
        "}\n",
        "\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(values=['Metric', 'Value'],\n",
        "                fill_color='paleturquoise',\n",
        "                align='left'),\n",
        "    cells=dict(values=[list(summary.keys()),\n",
        "                      [f\"{v:.2f}\" if isinstance(v, float) else v for v in summary.values()]],\n",
        "               fill_color='lavender',\n",
        "               align='left'))\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Text Analysis Summary')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "QurXaxOwJwEs",
        "outputId": "1897c77e-2a09-4955-8b31-2bebde1f49e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"317c7498-c0a9-47eb-8788-69cd367abfc9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"317c7498-c0a9-47eb-8788-69cd367abfc9\")) {                    Plotly.newPlot(                        \"317c7498-c0a9-47eb-8788-69cd367abfc9\",                        [{\"cells\":{\"align\":\"left\",\"fill\":{\"color\":\"lavender\"},\"values\":[[\"Total Entries\",\"Average Length\",\"Average Word Count\",\"Max Length\",\"Min Length\"],[50000,\"369.53\",\"49.16\",12456,6]]},\"header\":{\"align\":\"left\",\"fill\":{\"color\":\"paleturquoise\"},\"values\":[\"Metric\",\"Value\"]},\"type\":\"table\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Text Analysis Summary\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('317c7498-c0a9-47eb-8788-69cd367abfc9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(df, x='length',\n",
        "                  title='Distribution of Text Lengths',\n",
        "                  nbins=50,\n",
        "                  color='is_arabic',\n",
        "                  labels={'length': 'Text Length', 'count': 'Frequency'})\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "FsHRqM7oLoCh",
        "outputId": "9df024d4-4505-436d-a944-a5bc854f36f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c22297e0-a625-4ff2-a763-5caf1b84f840\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c22297e0-a625-4ff2-a763-5caf1b84f840\")) {                    Plotly.newPlot(                        \"c22297e0-a625-4ff2-a763-5caf1b84f840\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"is_arabic=True\\u003cbr\\u003eText Length=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"True\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"True\",\"nbinsx\":50,\"offsetgroup\":\"True\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[226,162,178,408,553,1736,61,472,900,187,101,327,388,66,715,75,73,995,581,95,370,20,343,82,174,726,387,142,247,536,45,564,270,521,302,45,661,164,126,21,885,104,592,175,80,72,135,61,167,14,615,123,661,360,39,208,612,460,985,371,582,123,543,296,192,184,38,212,831,90,453,369,99,76,74,300,941,168,208,210,1022,51,52,59,75,1712,441,85,642,252,670,745,611,109,88,249,380,1069,108,242,153,522,55,49,592,532,270,190,512,26,206,53,659,94,265,686,43,471,999,294,164,138,220,829,358,93,277,120,347,239,543,107,185,1003,168,133,71,30,2003,98,262,213,217,311,514,539,137,235,520,136,13,338,469,67,113,37,573,73,2011,398,548,534,167,66,58,37,817,113,63,250,122,328,307,785,12,99,96,587,85,114,1004,87,31,155,502,309,75,35,462,247,294,199,480,708,26,414,113,519,678,46,98,26,146,559,117,228,193,220,2009,32,1729,95,2004,328,133,95,116,365,109,585,167,372,114,260,431,287,666,261,354,86,329,307,361,508,692,102,90,755,470,463,47,265,419,275,101,30,218,73,978,110,758,384,365,277,124,169,611,506,66,284,570,263,173,46,280,873,54,130,99,716,343,620,42,62,58,507,385,529,47,105,47,405,610,812,190,62,653,984,117,1606,288,1608,370,77,421,133,22,91,50,296,312,536,70,222,145,404,163,121,241,25,91,639,584,583,224,71,18,622,156,26,21,293,557,177,374,401,194,460,318,490,313,515,63,124,126,807,74,773,30,437,144,139,275,92,89,74,1100,310,306,131,177,233,137,309,944,222,711,402,157,257,24,332,165,154,35,387,67,77,1273,150,590,776,241,314,463,76,1250,209,2005,106,661,276,1788,555,62,444,594,28,40,426,372,30,815,47,239,285,370,352,66,250,1329,60,51,781,425,350,329,39,752,144,1352,325,227,172,106,34,124,151,113,295,117,731,149,264,115,128,79,69,78,1386,328,458,1585,203,351,458,412,398,256,1994,410,116,84,83,371,109,23,118,701,132,63,263,676,998,658,80,950,234,311,87,215,300,426,55,73,155,95,33,48,216,1501,550,55,508,188,37,234,83,314,305,399,110,859,749,301,133,101,29,354,910,58,106,61,58,106,1092,98,1002,134,442,534,89,119,189,126,667,364,54,220,168,65,864,123,187,312,79,253,43,330,200,665,926,388,80,276,123,293,546,100,344,776,189,205,327,172,268,94,111,1057,182,279,268,81,163,280,146,102,229,139,30,306,624,64,99,440,640,86,158,860,230,354,92,719,144,176,664,32,150,120,107,581,53,624,382,329,255,538,253,375,215,573,604,1919,1077,64,515,35,1784,1508,541,322,576,181,277,726,415,369,338,420,72,147,97,204,95,375,380,101,126,329,45,199,57,690,104,233,248,565,100,129,284,199,307,1363,230,637,153,109,657,650,85,306,1007,374,787,108,764,342,440,464,86,1092,86,383,17,22,283,141,201,108,667,699,578,100,114,382,475,1804,236,92,432,333,434,117,70,437,614,166,536,633,1206,123,162,118,204,48,65,76,379,1033,1247,391,297,25,499,386,336,21,111,255,58,54,354,568,212,105,2001,373,309,43,442,110,194,153,745,184,100,897,718,654,126,743,113,146,237,250,411,30,128,898,217,39,233,14,154,64,91,276,300,624,521,307,51,94,45,1031,1510,23,32,977,1764,258,63,171,46,347,359,197,199,37,392,593,757,484,729,954,809,1303,289,136,273,294,68,54,229,205,155,148,140,54,115,61,13,243,29,652,118,206,233,103,261,203,13,64,171,706,597,310,584,48,1291,164,979,1001,326,206,250,83,533,158,549,154,894,332,845,174,79,27,282,242,25,422,591,241,29,1109,724,570,97,221,58,156,476,565,412,189,802,80,616,2000,823,144,78,310,354,91,281,443,410,1465,533,138,110,170,146,138,894,155,220,41,908,86,1843,134,152,642,600,964,246,284,81,309,156,137,185,77,131,170,99,101,270,333,1705,318,59,170,352,261,262,569,1123,592,44,328,322,132,127,198,417,24,591,336,272,52,461,75,97,18,67,993,104,484,45,366,374,82,22,273,2008,399,52,37,158,95,10,58,459,361,138,943,311,433,56,316,28,290,377,135,278,39,83,74,101,126,70,43,276,731,1066,1768,140,24,53,693,272,301,130,642,132,22,229,231,59,371,259,862,364,86,270,53,113,44,18,67,398,481,216,412,58,364,281,210,545,441,190,84,264,371,274,234,217,302,182,23,167,58,423,183,84,292,99,89,1389,82,91,132,336,190,614,1998,381,131,1493,40,98,207,47,107,234,87,624,464,235,886,539,109,45,375,144,755,94,213,1429,2009,55,126,374,131,384,165,36,573,490,1714,141,87,605,1052,564,169,1193,84,897,142,716,301,313,292,581,358,350,264,59,395,305,452,1195,121,116,676,1162,60,796,1364,33,49,135,53,119,451,72,276,42,374,271,44,263,349,259,74,2003,580,94,138,587,39,1534,351,132,211,61,315,88,1016,230,111,105,422,302,144,140,429,322,354,92,434,102,582,518,587,102,182,24,217,158,484,44,675,400,273,250,87,84,133,64,190,736,195,521,1546,181,63,110,421,508,613,78,112,152,854,125,43,157,2000,66,82,33,1215,142,241,691,112,69,79,1555,19,266,432,36,52,1261,309,447,165,41,658,117,184,222,1130,131,16,325,15,153,210,393,654,218,473,61,69,2004,571,322,146,506,73,663,317,48,640,188,617,332,27,687,63,430,158,80,140,548,560,535,231,240,184,264,235,146,327,321,97,66,351,74,288,333,320,56,99,99,15,1884,119,603,291,12,178,134,546,988,190,45,197,309,21,435,230,50,295,105,626,307,470,263,56,461,308,191,443,390,65,354,310,132,558,187,419,70,737,442,79,430,201,722,56,102,862,210,665,793,154,46,134,309,190,184,170,122,398,337,296,15,811,371,1222,135,203,169,160,150,658,131,1025,83,696,36,253,1180,117,425,79,194,206,73,420,91,160,50,81,35,225,855,515,1369,318,128,436,235,134,359,664,792,217,1745,151,45,300,1055,398,34,111,1001,473,32,1340,75,89,426,51,77,552,879,925,164,23,173,181,573,152,200,73,308,359,2007,140,902,228,245,40,208,410,399,197,161,166,293,65,385,127,457,145,54,1350,609,967,799,118,539,355,438,65,937,179,567,1138,65,359,309,120,143,270,438,74,976,288,193,273,252,359,65,418,46,333,540,181,514,162,77,65,517,543,676,376,378,25,30,147,429,882,334,268,124,788,261,28,209,112,97,223,157,599,53,591,21,88,294,346,133,433,272,374,77,780,382,489,93,150,316,363,118,668,449,420,354,332,59,153,73,226,420,712,484,344,96,529,182,332,155,310,83,477,872,219,355,25,272,412,118,224,50,306,139,90,160,472,380,367,205,1182,85,57,104,371,519,142,406,188,570,359,200,202,234,88,40,120,611,180,146,1342,21,23,228,592,355,450,73,233,98,560,1042,1563,83,381,405,538,439,920,449,144,47,206,27,34,44,1270,794,165,409,321,144,365,80,643,391,121,424,460,1169,145,535,41,281,222,310,229,116,661,91,985,405,53,1456,86,413,43,87,1444,93,736,430,2006,51,191,67,239,262,738,327,666,294,192,28,483,1272,34,372,110,620,145,1006,74,1220,281,70,94,356,1407,80,74,195,1502,43,225,247,80,258,1147,14,49,328,2005,205,954,65,122,298,44,203,187,489,296,399,572,134,572,33,95,310,78,163,301,413,329,227,214,734,113,45,361,665,159,85,635,223,101,55,276,141,230,215,329,135,190,237,253,1651,103,116,1572,162,386,413,229,156,902,85,462,99,87,124,313,156,339,1169,180,650,587,799,891,77,941,457,1835,25,85,31,322,23,143,140,629,689,27,88,206,704,381,166,559,532,263,409,156,56,316,53,498,23,290,57,802,71,476,494,92,821,845,431,222,358,194,865,115,48,1093,931,178,968,371,94,169,696,214,655,24,492,146,282,33,110,32,2000,327,228,782,306,227,337,598,134,275,889,358,667,92,173,159,65,40,868,221,508,293,264,949,931,363,76,80,31,401,77,463,163,516,158,807,818,420,322,158,617,1419,429,832,407,741,545,263,189,559,490,790,421,187,53,527,155,60,589,160,268,80,194,328,597,367,273,82,232,502,313,203,1157,953,239,722,497,711,106,78,1015,594,105,490,198,1163,120,632,2006,671,174,331,29,183,277,81,639,99,609,745,685,617,603,484,196,241,287,438,44,463,140,80,97,198,125,75,27,85,101,1037,84,300,77,31,140,75,273,254,67,79,971,308,236,450,221,341,698,82,289,77,333,1730,2005,369,956,1433,261,240,348,77,98,1285,281,522,299,40,98,273,482,93,61,23,1041,1048,90,660,530,809,539,44,46,106,292,74,1549,250,79,73,690,729,198,74,173,611,602,2006,203,11,155,2005,107,647,263,1289,119,223,835,853,302,386,641,29,147,454,205,107,398,917,172,75,115,73,128,363,209,131,122,132,384,182,160,97,800,30,422,231,157,497,25,498,1216,92,66,124,1501,74,49,84,283,454,311,72,53,550,724,1500,261,354,147,353,50,487,272,69,699,418,899,76,176,286,182,39,354,1840,417,580,124,98,321,283,701,38,79,765,37,269,109,295,373,44,131,164,169,337,43,317,56,95,743,54,57,1032,387,303,250,172,228,233,78,51,108,37,219,39,1680,522,211,75,24,236,47,335,323,121,168,289,153,502,370,737,408,338,915,404,123,293,1029,237,1655,89,174,315,205,319,244,161,352,491,37,961,137,100,756,164,954,115,26,84,1998,34,1108,169,1256,149,232,108,531,86,57,157,355,227,274,602,97,143,322,46,136,822,34,1539,23,1840,311,817,36,233,909,200,638,1589,761,88,279,81,54,354,359,78,13,181,163,312,414,182,155,47,363,31,368,1840,201,66,114,187,144,166,210,33,91,205,381,117,768,583,144,321,138,61,452,324,12,913,87,353,86,517,187,263,701,141,171,49,186,315,87,138,235,413,454,179,517,252,66,29,383,286,366,105,182,549,619,376,155,464,109,622,565,451,56,57,178,64,201,23,91,297,104,115,47,346,423,63,383,214,1001,455,1621,72,60,113,2003,85,36,280,397,31,23,754,273,540,310,57,1442,159,737,262,166,157,235,61,1244,601,74,168,82,195,87,885,144,162,68,311,88,588,487,113,56,148,154,1237,451,679,183,852,424,672,542,19,127,166,1069,238,161,144,65,639,361,304,495,195,596,338,786,122,1174,64,423,936,1542,66,423,616,291,93,94,273,191,229,599,363,89,157,107,52,42,424,73,98,28,149,364,30,508,586,515,94,465,344,63,97,680,614,215,186,734,537,364,328,31,64,345,102,402,199,95,701,68,246,1372,145,543,64,303,764,110,1201,478,196,34,338,288,984,71,418,71,292,158,241,494,14,244,331,320,47,162,961,46,367,31,25,478,92,296,146,441,437,211,129,242,653,251,181,225,149,257,686,345,495,96,580,252,50,278,126,146,278,534,282,226,392,88,342,124,179,161,601,951,1323,1406,125,47,135,155,69,68,126,155,163,1651,73,284,59,1151,130,640,402,2005,471,40,66,90,761,118,63,792,434,115,316,140,126,83,167,1464,69,34,1173,30,154,41,62,280,89,104,955,614,53,56,433,350,109,79,590,197,82,30,582,115,61,1209,97,64,23,1281,334,318,71,836,178,80,274,363,281,152,279,1076,157,74,384,407,312,63,30,397,226,351,63,87,179,183,123,2004,280,23,44,196,348,850,117,461,71,141,48,375,81,97,598,506,257,61,769,297,83,354,49,341,335,344,327,277,133,1417,136,49,157,51,893,736,1133,240,262,1194,592,1585,124,117,351,88,142,420,814,214,158,304,152,177,695,103,86,575,578,88,550,956,277,193,398,179,786,379,81,95,218,200,746,306,108,75,51,2000,171,375,85,96,72,196,300,137,260,879,237,208,394,226,180,133,253,1010,495,230,27,991,314,76,194,227,387,99,36,875,158,1509,864,48,73,1778,370,39,268,1571,46,101,146,537,25,483,83,1660,447,36,109,77,327,286,173,252,148,91,874,201,221,13,125,119,637,408,218,87,293,556,201,151,102,262,607,2006,264,177,242,126,50,121,501,272,194,30,162,175,437,41,86,2006,437,188,58,124,204,41,172,432,258,55,242,939,96,1567,460,60,198,59,144,321,57,71,310,114,759,74,679,77,49,238,330,40,931,220,110,25,169,51,1085,334,1243,367,129,108,47,186,902,241,227,107,238,103,62,150,581,196,29,1012,123,241,1998,92,177,53,367,29,91,110,413,505,103,657,2006,179,86,154,1195,21,231,187,72,949,51,1516,240,536,366,393,1326,170,169,496,145,11,753,243,540,129,166,805,65,152,212,56,1294,56,23,16,143,161,363,105,535,178,123,891,111,536,1217,824,190,714,105,1097,57,128,53,122,49,242,113,57,129,355,90,104,249,79,618,266,535,1396,166,372,1185,46,360,406,271,69,70,72,90,512,75,339,24,478,889,173,660,152,791,1002,297,352,31,49,550,81,256,119,763,61,142,278,304,80,1400,194,566,305,149,112,121,134,162,429,152,67,783,409,66,1283,615,75,44,265,136,98,1406,2006,309,211,188,502,560,997,266,638,192,364,227,173,133,12,147,1062,384,253,158,738,451,229,24,47,1847,51,213,426,50,643,39,139,890,46,1156,18,97,199,171,315,48,435,678,433,62,518,170,40,591,99,267,166,164,157,344,517,378,74,61,66,152,1107,127,583,329,230,45,82,209,2000,144,247,147,63,44,188,89,62,823,112,93,294,603,483,198,194,74,392,141,67,99,459,66,523,73,371,466,39,188,182,57,1993,205,422,332,513,1442,85,262,179,932,63,155,256,2006,421,13,587,43,481,326,197,93,273,323,190,579,309,84,46,441,486,618,265,346,521,167,87,204,494,81,434,209,62,105,400,68,42,876,487,1064,315,192,149,227,1115,1219,680,87,353,258,135,77,577,59,564,157,165,25,522,735,375,96,182,184,176,358,126,113,404,158,104,1149,109,281,1087,31,26,280,1398,75,146,392,271,26,109,203,333,158,104,546,136,62,1638,19,248,143,102,50,411,150,186,558,77,140,250,21,712,31,124,420,126,119,959,836,87,86,451,184,55,436,644,570,70,622,1063,279,160,230,195,208,252,55,1999,65,197,70,1220,172,490,261,665,44,99,71,122,360,731,82,140,308,188,66,182,270,124,97,1947,167,251,547,597,48,69,1242,111,452,559,139,351,141,1489,361,442,165,291,171,59,446,823,835,329,340,230,76,273,1073,485,393,247,99,266,221,655,251,471,289,37,276,62,920,633,556,449,633,394,287,383,333,639,1048,52,945,79,188,204,41,82,449,327,98,31,163,39,125,108,14,1406,372,290,2001,23,769,185,224,58,303,81,656,39,182,33,70,143,651,72,189,425,1653,155,388,249,23,201,195,45,1445,32,858,694,243,942,360,197,435,64,51,1262,588,359,30,76,544,26,41,65,53,301,318,1884,436,23,318,439,103,865,161,39,37,104,114,1001,210,238,1946,315,64,523,92,303,846,189,119,58,588,184,599,424,47,27,109,299,1345,85,152,168,402,213,111,520,1170,37,73,72,1470,297,116,396,195,321,398,578,279,660,108,39,306,1144,869,130,447,189,1369,219,220,136,631,2000,793,121,397,145,283,262,263,62,226,334,443,48,80,33,46,105,927,120,224,436,575,45,53,294,162,419,425,556,40,78,159,41,1756,141,81,206,916,237,218,227,1322,97,188,572,822,252,156,272,419,109,938,1028,233,750,328,738,33,165,58,754,284,494,305,1320,271,54,134,542,2003,84,195,1999,1038,597,87,452,70,450,143,305,163,343,557,1511,332,782,244,481,129,196,56,189,154,81,115,73,135,101,389,600,717,444,120,1107,179,177,228,1674,253,53,491,46,292,114,259,1571,294,62,817,227,531,149,68,540,168,66,625,1134,669,151,49,479,485,81,112,52,112,323,225,220,54,977,696,164,81,73,404,359,100,277,176,47,221,604,717,286,180,117,259,511,348,146,27,121,1691,162,88,63,252,95,183,466,1175,137,715,227,692,861,1662,228,369,67,216,77,161,304,189,155,653,326,419,105,108,877,135,948,215,286,1167,124,260,1257,786,171,221,62,275,2006,39,503,635,329,190,99,1661,70,88,31,299,44,290,178,89,52,305,344,213,296,127,765,122,97,642,73,30,36,151,515,69,130,314,296,87,38,184,70,162,62,462,291,172,22,404,132,93,766,62,60,238,891,90,1184,662,68,96,35,343,104,828,43,128,59,247,198,140,38,94,45,1232,403,541,689,177,26,190,140,267,214,86,157,281,162,175,476,177,434,49,84,166,1167,132,800,227,192,135,86,109,396,134,800,74,147,61,276,135,64,284,149,1693,72,346,1377,187,416,116,580,186,1610,256,386,153,786,434,716,587,375,641,894,1315,400,259,400,399,651,150,82,1642,398,855,370,137,294,361,102,159,274,463,311,302,54,295,673,69,871,1317,341,1633,195,377,191,90,152,110,24,696,175,726,56,649,58,193,47,327,62,414,69,1253,471,186,244,301,901,82,106,545,484,111,490,107,92,62,322,149,324,73,44,340,37,218,170,40,139,1014,386,314,481,85,301,87,946,31,180,24,64,667,119,230,948,33,162,331,67,94,376,1118,1231,170,1755,79,389,785,19,175,110,274,137,110,381,54,425,531,165,185,583,327,210,74,402,11,93,99,912,885,316,485,1140,1095,167,240,70,56,73,31,88,681,334,670,87,78,477,132,2008,125,1040,183,971,23,24,615,190,94,89,1074,207,2003,108,248,2009,377,262,203,150,713,313,227,898,146,1268,254,195,66,192,50,33,354,239,655,237,285,715,26,135,426,334,233,183,385,265,1335,580,442,94,221,405,292,118,1494,89,216,111,338,359,230,141,272,37,34,369,996,347,210,43,193,140,261,77,178,182,77,1986,1617,383,235,56,470,445,59,538,109,580,350,77,313,15,607,173,167,439,559,443,83,222,583,1550,42,65,44,102,117,52,143,406,60,701,31,1657,563,75,73,402,49,768,215,310,91,427,499,378,55,29,149,580,2002,100,69,1102,131,236,167,25,292,151,188,337,406,166,111,18,290,312,405,207,106,251,33,143,1932,86,304,1683,576,196,53,18,140,66,728,66,10,37,28,1180,224,163,3460,16,41,587,311,487,107,211,320,109,114,694,94,576,275,68,305,730,142,77,277,390,473,432,68,656,1232,63,458,360,27,124,297,178,301,695,435,440,63,79,289,294,1055,78,153,1091,120,733,156,275,52,34,79,87,69,251,1486,200,248,819,222,403,139,179,82,652,1031,295,57,290,311,149,150,325,324,669,1714,271,71,47,93,201,74,175,402,1930,328,36,43,892,90,106,888,729,659,61,93,64,158,119,453,239,60,99,325,196,112,625,230,23,57,94,343,42,427,189,199,687,103,1770,492,94,22,160,347,187,84,165,1113,721,366,56,433,52,62,327,392,660,135,73,88,216,220,56,132,147,169,66,682,124,79,100,45,610,140,82,204,80,22,56,613,326,381,145,178,357,155,63,2009,117,838,70,23,205,45,743,124,1465,45,113,60,418,258,259,268,422,423,497,242,131,785,67,295,72,328,118,484,70,390,105,261,1214,181,38,356,267,265,1231,784,244,492,107,198,36,1997,269,118,101,344,16,185,240,198,97,280,296,123,875,61,90,245,1065,202,370,105,30,506,345,166,802,27,540,82,127,175,815,44,281,291,597,60,224,145,159,328,550,355,599,483,161,699,307,170,231,22,236,67,42,10,66,66,29,62,209,72,202,291,520,269,214,405,376,279,163,758,219,1157,97,37,85,160,79,187,148,178,402,159,125,215,51,632,310,106,169,193,312,228,2004,466,1017,87,78,256,55,98,727,560,224,403,588,474,80,128,96,122,95,409,381,376,134,133,317,180,418,392,49,169,351,1618,306,429,241,200,188,204,2003,80,61,240,35,198,142,954,426,55,92,473,209,136,675,93,405,175,1903,424,936,810,229,43,130,693,125,200,16,632,264,99,185,214,93,66,76,168,164,143,114,57,68,817,64,131,110,353,71,22,80,62,499,259,1041,142,1317,288,662,456,485,145,197,141,339,390,243,1223,123,877,375,161,138,202,91,243,1093,739,742,116,306,176,84,245,72,179,31,105,312,212,484,388,670,871,139,241,92,266,256,185,58,57,445,393,734,134,21,672,235,639,143,302,83,269,1136,138,417,112,54,273,55,205,501,170,58,144,405,278,1316,277,1007,129,539,41,121,546,444,134,194,698,56,177,277,43,462,795,1304,65,80,282,173,901,238,128,247,483,1254,164,113,144,181,83,66,290,332,339,834,146,870,377,147,116,53,137,314,318,229,164,240,71,90,47,422,137,987,254,236,536,31,48,126,120,568,59,593,127,282,44,139,427,149,179,195,461,36,85,456,68,577,1013,367,90,1067,2009,85,87,789,124,573,183,146,79,121,130,510,63,285,120,55,287,131,722,333,256,69,233,121,159,64,848,561,374,229,724,131,195,490,317,57,1265,351,41,26,152,100,302,395,75,288,363,25,103,405,375,129,213,733,97,44,169,50,194,389,87,133,1087,275,94,209,1414,101,19,173,33,408,565,147,1704,358,12,131,30,224,179,249,810,112,20,61,609,2005,193,1411,43,138,69,58,98,774,319,192,1193,60,109,41,575,214,1246,212,887,711,438,968,65,686,428,137,621,188,219,2004,234,74,16,71,178,532,72,206,149,1378,45,194,337,217,228,563,230,918,101,145,717,282,1046,54,163,1077,362,415,276,1405,503,1082,62,54,778,158,380,875,179,93,228,1374,211,206,340,634,11,661,178,209,45,303,58,1324,372,114,559,55,52,268,760,493,17,2103,398,59,293,398,56,410,999,263,1349,369,490,448,325,1215,697,125,310,641,816,624,76,203,91,173,562,638,53,324,298,342,1452,968,24,397,251,334,123,314,109,93,207,655,337,89,81,675,24,32,246,277,273,220,173,1992,72,69,1424,1169,665,27,59,297,102,357,640,174,281,163,41,484,221,339,1152,676,83,54,154,38,138,237,38,704,85,84,251,129,366,1136,329,481,139,290,304,163,265,71,327,155,386,231,164,265,430,96,126,328,341,221,424,250,527,791,537,60,180,25,62,492,18,307,186,77,92,262,52,59,34,319,283,153,206,98,156,754,14,187,685,1056,274,90,571,406,138,747,173,155,70,205,41,96,535,126,319,486,304,140,638,163,253,193,34,212,301,718,738,567,1552,111,160,217,402,1062,171,105,160,1158,372,80,1598,121,404,674,176,156,206,55,235,228,110,459,678,694,96,98,593,197,121,55,150,229,90,227,426,243,288,20,662,45,379,201,659,293,1000,498,22,134,221,25,283,57,86,218,344,995,157,18,508,372,489,442,1996,128,107,232,222,2000,166,50,164,310,78,376,1118,37,121,191,216,248,111,268,42,92,347,106,877,504,179,277,355,306,1364,283,1130,251,176,92,108,356,113,60,48,695,88,213,110,175,399,1237,445,168,2006,139,144,474,531,292,624,26,1387,290,123,520,119,73,308,62,263,1067,246,689,284,175,655,334,739,60,246,133,116,24,147,1276,1010,15,247,413,112,575,84,152,153,403,13,1640,206,71,177,1788,94,59,187,402,70,64,84,235,834,98,122,44,410,440,99,76,585,119,68,80,145,409,131,194,283,303,1085,899,2001,58,222,93,831,32,30,210,517,74,64,144,660,169,78,248,939,453,1113,365,72,521,520,267,58,125,119,315,639,612,358,143,242,708,213,52,785,693,282,727,172,16,202,301,262,146,238,301,191,531,280,192,554,104,334,68,260,828,192,341,796,349,102,495,64,53,447,86,369,702,825,131,298,912,228,1129,96,176,154,465,324,619,72,159,246,104,656,467,18,323,847,1459,164,873,532,153,745,1187,157,263,30,164,93,327,85,34,151,770,55,189,305,178,123,123,544,45,300,50,139,554,263,79,128,158,21,32,350,600,291,115,116,34,749,1168,171,260,50,53,409,456,156,343,144,2013,805,150,293,475,108,726,227,133,1706,403,672,294,1385,897,392,167,53,1155,64,210,117,322,30,145,213,35,203,230,73,330,17,993,191,71,422,1127,414,363,366,154,1045,151,400,1166,44,342,93,359,128,301,374,170,369,48,287,719,305,169,189,128,298,221,588,123,48,1406,1962,155,686,97,134,2007,393,65,189,387,66,113,874,892,2005,513,95,58,846,160,65,86,80,470,359,879,117,385,530,575,67,1240,203,60,379,166,61,640,307,141,325,383,319,340,172,219,1092,35,501,168,222,1977,244,159,74,609,315,908,193,169,2009,1013,649,317,79,81,60,108,668,153,1456,844,111,68,131,156,94,121,287,368,86,287,775,112,94,883,174,303,171,85,36,80,316,260,259,121,23,660,320,1114,157,620,274,461,27,141,151,128,267,84,119,593,535,631,76,540,178,210,259,342,232,330,71,100,266,130,1983,31,104,108,43,219,843,484,112,1088,278,1739,2010,366,31,698,398,216,1112,288,111,129,38,47,25,137,167,300,196,523,406,114,364,783,290,237,350,168,171,833,203,257,101,614,127,113,258,124,49,131,121,97,261,119,193,77,178,709,209,414,341,1433,672,185,787,234,234,67,183,1462,100,361,388,49,61,46,1190,76,114,87,1155,504,120,463,205,334,146,331,90,283,620,920,1377,59,49,132,201,231,903,91,2003,771,57,336,270,357,155,193,82,572,380,132,36,116,467,1984,311,24,204,728,126,166,348,200,322,52,96,761,203,394,2006,2005,535,109,422,50,213,46,44,119,107,111,101,466,213,173,120,332,22,624,42,96,451,530,1172,406,1041,214,478,280,696,376,464,325,1002,255,66,273,813,539,212,16,353,166,1154,79,130,360,789,183,153,307,707,109,1286,1007,535,251,356,53,66,1605,131,1031,168,428,402,129,107,203,543,264,53,16,252,30,740,316,812,115,163,109,208,342,572,142,93,264,684,61,110,377,279,1874,189,187,248,566,633,788,71,1724,401,446,396,460,35,258,47,921,583,838,683,67,135,800,701,127,126,140,459,2007,46,534,318,1206,93,179,13,190,523,827,856,852,131,320,519,171,407,49,133,429,149,45,301,198,308,327,960,379,288,136,764,599,267,375,203,671,160,172,2008,44,15,37,66,1056,48,99,74,37,899,402,482,36,236,176,347,998,504,1039,420,1014,678,81,145,391,286,60,280,591,56,40,36,303,18,45,27,188,77,111,1118,178,46,386,245,1326,299,70,76,300,966,138,344,33,67,124,127,451,599,107,32,738,52,1659,314,227,286,642,119,87,83,35,547,138,260,276,704,476,101,103,294,190,219,887,1222,314,360,408,404,812,151,204,54,306,558,130,616,86,183,1012,64,296,209,210,258,642,283,839,164,108,605,440,43,1076,412,61,899,95,351,1067,98,186,176,203,940,1163,79,82,282,104,51,708,219,69,154,119,1782,104,334,237,285,126,94,130,738,30,294,569,48,473,696,108,268,1155,612,353,664,56,327,441,68,270,266,211,82,211,21,322,40,173,215,575,132,73,122,151,687,282,672,124,284,231,232,180,234,541,214,14,132,1078,274,131,46,195,133,1278,440,144,1779,115,176,1521,525,359,522,517,38,1308,206,345,152,74,131,42,832,304,827,1694,645,365,54,95,105,327,627,357,116,272,251,461,63,412,75,501,59,122,74,283,212,77,156,256,1752,48,32,95,100,81,62,204,2004,540,98,331,649,148,85,922,380,2004,139,41,613,622,521,40,637,100,47,251,65,85,62,115,260,392,132,528,508,81,699,35,47,338,323,58,78,126,408,836,144,36,335,458,61,643,208,172,114,1787,100,580,82,81,147,385,252,70,80,149,234,1337,44,255,115,711,724,86,87,30,64,52,522,457,27,71,165,134,1119,268,324,226,626,513,98,63,739,71,225,477,173,499,115,13,114,126,211,484,84,72,1657,471,513,996,1167,19,1287,66,368,232,146,139,529,147,376,22,370,63,335,285,464,2002,71,97,51,445,822,170,574,248,24,97,33,52,1604,85,395,355,28,420,249,359,176,745,101,605,762,256,100,38,334,32,420,393,263,72,1009,264,150,606,91,72,918,87,1983,504,140,11,265,173,459,926,1729,741,323,145,594,632,190,270,236,158,60,1013,119,211,1045,56,569,180,548,781,333,552,99,402,421,128,369,992,183,784,468,50,585,218,406,421,43,13,624,122,871,641,213,318,160,56,93,685,784,73,269,163,216,250,245,115,944,70,13,432,279,310,165,673,35,361,31,160,254,108,716,848,25,165,353,175,245,391,77,97,112,803,348,433,26,39,163,383,57,86,520,85,248,60,71,63,23,108,561,52,298,1998,1133,446,663,497,509,133,78,412,21,181,408,104,218,101,82,36,187,1999,59,261,234,74,619,160,165,195,1342,201,845,328,110,124,293,988,826,299,711,282,106,79,261,123,261,131,362,155,96,298,331,52,330,543,443,81,261,55,51,72,255,124,46,115,57,53,316,169,133,120,19,600,340,81,97,918,89,97,1339,524,270,446,577,351,298,932,1089,90,595,69,330,98,272,405,88,899,826,140,301,778,53,211,484,455,787,149,634,155,117,78,27,66,142,123,148,130,31,65,92,1337,122,50,555,144,72,626,590,170,411,1803,368,293,46,1487,367,787,587,62,75,646,97,498,18,111,866,402,237,614,152,1029,203,988,439,43,453,183,89,320,46,118,437,83,233,461,78,961,497,191,1046,236,387,100,281,13,430,140,164,321,676,27,194,663,194,946,115,122,153,285,495,162,161,13,100,206,115,319,939,341,141,261,789,727,973,454,389,153,96,172,1023,276,518,115,1080,206,422,386,31,54,514,521,116,221,491,88,210,200,2003,58,353,448,298,228,62,45,72,481,509,92,94,2001,69,746,423,353,226,355,1003,854,179,240,235,252,82,2004,108,121,264,1628,91,131,64,84,608,211,987,98,743,89,237,1053,1053,168,151,728,730,36,725,68,473,79,146,1070,280,288,383,658,24,270,353,196,148,512,26,319,2001,237,37,158,29,35,554,794,314,833,668,460,84,51,516,527,49,90,357,107,125,233,394,453,125,161,244,143,415,484,374,1257,33,509,163,266,685,26,146,194,585,198,31,602,283,1040,1629,28,63,462,905,1562,206,265,72,431,52,226,210,424,912,111,806,104,554,158,254,226,109,1623,141,91,87,272,257,644,622,137,137,1339,22,537,333,306,344,381,133,867,201,316,601,112,1323,151,194,271,142,500,210,315,313,158,19,229,253,265,238,243,212,142,530,140,249,212,335,211,93,1446,475,1011,584,11,323,402,1172,110,199,25,164,227,157,213,389,168,151,398,87,1229,468,235,288,90,1999,146,371,395,275,48,549,192,107,130,258,1108,594,526,244,443,110,266,64,336,41,221,79,200,732,201,377,42,966,382,126,339,436,1651,1813,139,332,678,667,731,719,154,120,590,1193,142,923,2005,223,196,86,102,336,364,262,399,1034,887,73,18,144,79,413,880,177,656,34,644,728,346,71,400,737,514,51,112,263,357,511,135,944,315,144,172,88,1323,69,119,237,226,50,176,58,132,96,842,294,199,70,166,382,284,120,54,111,561,298,317,359,49,334,18,185,480,179,251,266,724,90,239,327,510,384,157,743,98,1543,424,341,970,286,348,2001,92,130,417,170,82,365,61,21,509,320,457,165,103,267,501,303,73,83,77,50,552,242,109,68,513,104,298,349,230,238,1148,454,20,224,2003,843,365,250,483,141,352,435,72,252,86,212,46,79,177,37,423,90,112,1436,545,907,61,295,409,269,401,62,221,155,511,22,725,499,245,420,69,491,73,774,93,24,1008,741,64,655,280,32,56,11,708,43,1112,151,211,580,229,24,80,759,1879,492,144,57,412,416,1032,277,119,190,884,294,220,927,237,799,25,1887,325,20,1884,91,119,236,31,569,283,430,759,132,323,176,174,454,85,137,376,285,391,147,74,432,333,198,69,1673,182,54,193,1198,218,285,165,409,101,1109,78,958,1129,516,104,182,172,249,56,51,294,301,132,203,594,466,72,510,23,424,907,38,1943,1072,522,46,539,477,154,719,79,2006,1779,191,410,84,26,67,92,188,602,94,114,87,382,160,472,1459,445,74,440,421,263,161,66,98,152,78,120,136,375,19,134,270,275,16,662,162,460,355,79,202,128,406,943,260,2002,317,79,50,55,36,278,391,249,71,522,89,27,770,139,145,479,264,29,511,31,556,32,739,1053,711,209,513,50,183,416,1290,202,283,474,81,206,48,1788,66,112,761,171,63,90,113,337,1063,798,154,14,563,421,998,576,420,546,85,13,66,498,155,79,83,216,356,88,962,201,1471,100,641,495,199,127,147,363,272,110,253,68,123,1079,157,170,17,50,235,216,91,398,61,70,1022,58,149,1226,1487,24,1226,128,709,155,40,52,356,51,134,777,123,94,557,253,274,118,292,77,907,521,265,333,1382,566,63,335,184,95,159,92,77,313,104,112,100,728,74,50,619,54,464,286,69,354,50,312,159,206,274,163,302,321,199,185,377,384,221,156,134,267,516,71,501,121,181,101,958,491,143,123,328,46,228,247,859,89,107,18,214,160,122,78,315,452,241,149,443,870,480,223,245,333,549,283,251,143,184,69,159,92,342,101,491,414,732,304,1662,67,87,195,239,30,91,617,470,44,119,450,644,447,59,282,514,242,172,154,714,408,167,489,34,151,63,705,35,616,122,497,738,189,105,21,155,35,377,399,242,169,330,1298,717,80,619,100,30,417,162,224,95,89,139,143,441,271,12,330,559,35,111,71,732,174,70,323,670,109,139,254,605,1105,25,53,307,1448,314,1337,233,577,63,256,312,133,161,650,986,262,554,664,496,410,281,70,565,149,309,155,184,31,593,81,1083,341,550,221,262,416,130,248,1120,105,415,63,241,169,168,1600,1622,505,330,160,507,77,674,559,275,983,92,133,250,330,286,160,31,537,68,336,369,354,841,91,66,863,132,1311,30,150,182,413,260,1011,57,234,65,318,212,252,238,99,542,99,1088,214,79,325,421,127,43,72,66,139,1254,414,1526,323,393,249,1167,502,2006,107,54,918,1163,64,117,186,404,73,862,194,325,19,428,116,58,65,82,356,38,52,687,27,223,664,1729,464,337,95,47,312,158,803,151,63,100,65,414,69,310,307,109,482,744,351,59,968,84,359,31,286,549,114,215,504,44,513,125,165,93,74,204,639,274,1115,31,278,700,235,56,162,199,13,403,376,79,253,184,669,644,114,86,346,286,129,611,218,124,22,503,945,834,61,43,604,406,191,181,203,398,73,150,1359,658,951,56,89,331,173,247,204,463,56,47,517,61,92,71,265,48,145,1141,305,62,178,209,379,134,268,723,301,1552,53,142,93,131,1498,702,714,128,318,274,294,118,39,266,469,104,511,150,15,25,563,22,483,24,249,38,187,531,81,816,41,112,254,616,203,590,98,402,48,148,72,116,298,143,491,972,227,48,1419,230,61,79,332,1971,283,292,137,237,1523,1539,225,702,678,278,194,211,682,84,797,321,376,450,70,634,572,149,787,121,132,95,287,611,478,80,47,238,187,1240,176,778,621,103,217,690,68,479,199,242,333,357,409,148,217,312,126,82,353,123,177,370,287,48,388,74,303,318,122,73,51,108,137,325,59,37,408,480,66,53,68,89,508,138,77,632,88,641,531,148,359,243,70,78,588,105,205,369,241,150,591,303,121,488,1391,13,235,157,128,87,344,81,406,193,315,55,626,491,477,156,187,42,262,519,96,272,256,286,174,267,333,29,60,693,163,34,155,333,449,90,39,199,189,1098,107,284,745,102,170,53,351,387,369,293,168,237,201,1192,1406,189,351,45,968,34,466,1202,525,95,73,170,252,114,268,1020,584,462,480,201,2009,240,43,74,65,215,113,519,149,131,166,387,868,430,185,209,88,652,1613,805,97,20,38,23,136,45,151,388,78,48,43,411,575,417,19,204,409,146,68,287,154,110,1146,129,350,50,1105,381,197,384,103,203,140,1738,613,360,52,776,169,170,116,217,50,352,507,31,699,216,443,578,73,1010,598,297,492,382,421,717,63,132,146,1043,526,125,211,261,203,89,110,176,88,188,318,711,1473,177,407,154,545,102,747,320,72,38,142,78,1216,24,21,442,523,1152,184,379,527,384,279,82,842,296,306,781,227,76,278,140,126,1035,65,123,127,746,387,75,235,987,24,53,1680,138,47,207,278,104,670,315,210,928,35,1871,166,961,602,140,117,808,86,332,664,66,164,896,330,511,146,415,209,638,47,382,259,144,102,488,79,131,182,59,47,280,448,54,151,435,286,1393,1169,838,336,74,409,912,103,432,1072,184,182,116,442,1999,93,557,620,313,460,71,217,2002,152,195,347,263,101,976,379,66,71,385,208,30,449,191,98,118,26,371,79,270,59,273,183,304,158,93,252,564,398,245,72,280,325,396,1417,857,243,268,104,1998,62,199,334,74,1537,181,37,58,26,80,154,566,517,395,493,183,161,636,97,222,151,70,39,118,210,240,211,50,70,597,133,61,323,23,100,127,420,120,219,108,1337,38,351,478,372,44,395,638,58,81,142,763,144,79,273,82,242,178,91,64,372,639,1232,339,485,857,922,221,439,181,545,491,436,255,49,257,344,285,105,298,242,69,504,637,445,343,262,321,1820,273,264,110,23,813,107,54,37,267,913,641,610,698,179,42,33,184,273,111,491,68,1053,261,43,317,1000,150,34,99,97,124,748,16,106,25,525,109,299,1341,218,213,642,392,37,236,149,214,105,17,57,85,218,138,391,240,925,99,44,118,466,126,36,53,106,74,529,493,720,363,302,71,97,41,112,251,281,126,739,86,170,731,112,128,223,854,655,843,204,177,577,706,273,287,454,120,41,523,274,860,140,662,510,108,394,291,499,538,660,260,117,346,1068,55,403,164,27,225,1016,560,195,418,61,317,780,148,170,79,255,306,522,84,299,1259,421,242,42,472,219,1586,27,101,148,101,185,953,663,167,986,111,93,476,656,74,101,102,64,209,196,143,85,32,290,441,66,1381,970,277,121,1099,644,62,548,143,1118,182,129,356,504,451,389,267,49,55,94,1232,510,30,199,244,136,37,222,185,401,111,209,2733,1071,47,41,141,325,198,834,761,261,39,58,88,122,91,62,246,80,236,86,549,778,865,404,108,110,405,504,459,1480,439,379,1552,262,99,99,1068,123,541,585,1116,282,70,585,307,376,421,65,1072,236,401,126,487,62,1350,683,590,292,910,1713,56,1111,661,612,585,785,167,40,94,87,787,265,302,33,441,656,353,33,229,488,631,190,678,407,165,623,215,50,102,189,79,439,66,233,264,703,471,468,286,98,623,472,274,566,155,396,150,455,144,96,150,249,95,210,131,120,64,1037,93,172,117,281,129,223,152,380,187,70,1102,267,932,217,115,600,1103,440,2003,71,1019,63,39,50,465,547,210,42,800,66,234,113,202,762,80,199,72,450,47,81,404,380,445,480,182,41,351,22,21,170,205,80,248,378,288,558,450,2002,49,33,459,560,210,1791,98,237,227,92,43,43,602,71,365,606,75,346,364,110,155,87,1092,454,43,820,81,752,168,506,308,189,641,560,265,84,1218,186,807,296,402,354,764,558,77,823,481,142,372,56,366,640,237,162,36,155,1056,640,307,1713,201,225,56,2007,495,2002,409,88,483,906,76,424,1100,769,1214,463,60,359,128,212,463,288,285,77,200,487,21,215,161,53,11,152,201,793,25,69,150,42,559,727,1551,507,52,1038,20,263,1436,579,313,297,206,215,22,1372,146,207,134,342,115,212,634,103,539,98,179,207,434,165,271,524,269,64,216,155,43,254,202,47,230,221,185,540,136,367,1515,137,156,61,231,217,18,71,199,82,202,401,50,144,395,96,189,193,134,206,816,316,196,241,995,65,267,67,133,18,140,36,322,56,102,375,254,778,1397,77,398,362,75,269,252,46,31,702,477,1006,666,535,130,180,897,256,244,696,97,844,293,158,157,642,396,1993,235,617,351,213,1596,62,474,89,620,72,106,372,744,617,2001,584,62,27,138,273,1433,203,16,212,95,370,193,162,317,934,337,295,52,88,1831,1987,350,135,390,571,289,256,403,72,85,145,76,937,1194,631,350,116,211,510,457,691,119,496,107,605,17,27,1003,118,72,297,122,448,329,674,698,23,76,68,457,85,46,286,110,717,407,516,183,375,878,423,58,398,79,1462,74,116,99,288,180,792,249,653,523,408,511,114,66,570,756,256,89,192,1219,774,521,559,132,103,247,92,569,26,183,166,1018,114,216,447,96,215,403,242,42,223,382,210,51,29,13,135,225,210,556,242,152,62,372,93,313,169,30,41,203,61,916,51,48,115,249,104,99,58,226,343,49,118,259,470,138,450,235,1959,1428,126,132,426,319,436,230,1998,1548,148,81,218,55,276,219,78,434,555,425,822,175,504,1021,436,100,46,318,293,968,162,551,552,591,298,665,739,458,1188,625,243,37,260,47,747,173,333,619,320,130,259,463,177,320,149,161,1434,496,843,142,299,558,916,251,327,112,1083,1063,221,752,841,146,339,301,12,228,966,189,554,415,69,107,2007,2008,468,17,523,226,175,201,845,92,342,316,400,379,87,236,898,95,355,475,159,877,154,44,574,83,244,296,417,310,110,80,241,689,170,114,167,384,653,698,21,182,194,330,215,1627,196,2009,165,119,592,1704,278,196,127,461,685,1753,91,539,41,88,1185,634,372,556,418,92,55,441,200,1625,1112,69,1956,36,532,381,102,156,104,499,136,109,79,953,961,633,44,53,450,64,199,168,234,387,191,47,171,313,51,207,191,112,496,81,132,659,300,116,54,303,193,287,13,203,403,817,205,369,812,334,373,613,766,303,26,817,728,524,782,278,105,151,132,87,174,528,146,193,166,581,461,195,291,22,65,144,67,90,156,51,1027,147,259,435,120,73,63,636,26,24,185,385,720,558,28,386,85,611,35,317,472,466,200,762,139,792,25,225,2185,80,1115,331,416,286,91,560,84,448,94,476,272,83,1972,420,434,661,232,129,139,46,93,191,62,155,19,268,187,240,220,339,128,404,231,1129,203,141,209,399,295,516,862,286,566,87,32,23,489,337,190,509,92,55,44,288,146,240,73,179,33,108,106,21,481,581,1986,155,91,290,126,1344,516,409,104,509,108,217,118,759,52,710,181,207,27,236,105,61,57,142,593,71,540,90,113,51,839,442,718,302,139,153,440,142,399,164,633,207,41,22,302,122,201,42,58,772,33,210,242,46,630,48,384,404,502,83,470,22,558,146,340,217,102,438,358,379,40,279,242,222,846,125,63,525,360,1470,202,61,272,43,28,188,1095,428,363,293,160,2000,1188,476,74,146,129,446,48,918,113,70,602,114,1230,575,1066,236,747,80,68,555,47,280,150,178,141,57,79,663,712,515,144,358,778,294,408,883,169,85,160,163,397,6,343,681,275,685,276,320,711,143,622,41,538,59,464,140,329,829,421,88,201,636,392,203,172,182,1199,557,287,313,430,441,84,199,26,1107,255,61,1226,468,216,1759,575,200,2003,135,802,47,1400,337,180,1082,219,293,421,570,384,564,37,228,615,130,508,51,314,586,494,150,561,147,36,811,139,38,487,583,483,427,316,148,237,708,95,265,24,28,113,364,142,180,1294,602,109,67,213,170,220,140,103,519,175,74,697,99,379,175,160,177,90,232,868,95,240,314,227,1184,269,27,644,194,364,142,320,294,321,251,2003,21,151,834,166,246,61,146,312,59,58,641,66,142,367,10,423,283,221,793,622,63,356,409,38,608,251,962,76,1053,288,484,46,300,96,320,52,389,282,194,229,193,44,275,136,180,169,505,280,992,175,344,120,273,172,482,604,73,1463,374,90,95,489,427,98,216,26,42,81,70,738,351,932,2001,211,1613,256,276,100,141,235,1238,204,633,119,1257,12,464,751,584,57,453,42,23,351,292,285,390,154,268,364,695,114,975,1994,783,376,591,64,463,206,136,896,304,182,356,91,283,170,139,283,128,1270,1097,660,35,480,254,624,182,204,246,223,96,99,225,422,167,380,49,188,316,23,983,447,84,222,1131,1356,242,464,493,221,1224,28,281,21,283,360,52,24,627,277,1004,106,115,256,1110,66,453,208,395,161,430,142,46,24,53,459,286,466,188,363,50,63,122,126,76,197,327,898,238,403,199,67,26,283,934,118,309,103,33,966,315,320,78,2001,269,71,386,61,96,206,50,88,263,503,309,174,224,121,1339,429,564,377,63,66,692,297,893,2009,241,351,57,348,207,309,337,1183,595,104,453,457,308,112,294,1009,252,714,356,81,466,347,31,306,57,388,141,754,902,514,357,124,212,48,249,13,125,681,137,862,265,670,324,287,93,24,330,200,137,94,115,377,85,107,2003,55,633,463,243,345,34,89,121,413,214,652,58,151,54,70,218,421,118,727,125,140,735,159,554,470,204,181,2008,107,162,110,251,1709,2009,243,420,44,537,156,445,267,50,1027,472,773,178,73,290,280,784,117,211,223,536,770,169,288,51,138,183,150,1446,73,827,406,95,228,83,237,64,162,201,300,272,79,90,28,649,241,397,295,244,56,228,213,333,344,426,468,266,221,2214,186,414,1398,29,366,121,449,22,67,71,207,315,105,545,106,109,40,80,116,256,154,182,490,115,168,123,309,799,157,20,523,325,553,402,108,105,50,359,1108,50,63,335,401,136,70,207,194,696,163,147,689,509,168,446,909,165,71,879,308,332,411,788,72,19,483,303,136,86,235,316,115,779,480,150,145,223,562,129,859,328,216,42,410,87,675,66,49,503,245,77,538,82,31,906,129,91,264,46,389,402,636,2114,592,764,600,328,95,163,96,621,526,43,113,806,734,490,1332,379,80,278,667,1562,566,893,199,276,343,65,37,333,64,198,213,838,249,274,598,523,281,439,130,943,10,475,648,66,117,106,689,165,2002,131,327,636,194,236,179,518,31,135,16,1044,195,401,71,588,172,146,297,283,230,174,219,31,82,108,78,71,142,364,67,90,269,495,47,283,358,164,29,355,244,151,159,66,26,822,72,117,723,75,206,80,119,139,276,266,353,837,65,487,327,35,310,447,81,57,572,756,998,705,115,148,540,669,1038,67,95,115,126,251,158,1125,602,101,107,787,1365,1995,126,715,152,180,190,1737,69,78,174,239,904,6,180,350,168,247,121,564,103,1671,235,235,65,389,649,186,86,110,499,80,39,570,453,51,256,108,567,79,219,466,308,773,1372,134,35,643,176,61,326,225,29,204,69,260,713,82,1056,478,658,350,1911,45,616,356,361,161,164,495,221,100,468,29,509,724,717,92,302,48,123,355,713,395,783,148,670,83,62,516,60,1597,889,143,339,151,299,72,174,851,410,312,112,350,316,389,425,201,298,653,513,35,66,230,379,103,473,103,177,69,35,365,196,124,655,466,880,72,94,81,284,81,156,741,71,134,1339,62,416,419,156,186,66,207,1055,61,143,941,296,179,32,54,666,230,438,791,541,55,126,58,86,113,306,735,90,280,75,80,57,814,56,297,427,638,446,693,207,81,1255,96,1518,532,202,48,203,815,238,116,172,2004,198,364,16,100,163,582,2001,96,210,495,222,149,1422,813,479,410,393,30,553,280,307,96,61,104,76,27,128,153,1313,32,157,376,368,908,385,91,32,194,115,689,259,1491,269,562,357,433,336,57,798,168,200,71,120,430,485,457,446,91,344,255,13,674,145,40,496,183,144,120,164,196,108,282,80,221,160,1320,1267,179,12,603,54,208,103,46,239,568,154,499,489,234,1179,356,149,102,183,145,392,160,143,144,248,86,30,664,380,204,65,308,80,158,411,218,34,416,88,17,132,246,848,1393,152,312,77,121,149,81,271,114,47,559,1214,523,47,583,96,529,138,109,233,1858,88,515,278,77,49,322,304,908,94,963,10,99,251,74,340,31,178,213,479,42,1999,1986,376,194,1348,273,324,366,58,71,243,264,252,1603,779,197,86,51,1090,121,144,418,319,266,291,145,389,51,228,123,492,129,512,1776,129,117,436,948,53,200,115,654,260,417,108,98,384,642,13,1242,240,387,1742,577,66,187,772,136,85,680,75,595,21,1006,1606,442,41,95,99,588,79,392,337,394,2002,244,39,76,41,268,37,578,382,866,66,60,427,122,59,211,325,59,150,26,160,165,370,42,23,133,179,80,1967,365,179,64,1968,166,107,428,101,131,27,102,91,420,129,296,745,359,415,524,473,126,277,1706,30,251,94,621,497,779,361,149,638,248,256,656,70,238,135,990,2005,133,232,492,1756,949,33,329,391,259,540,251,33,2008,822,187,140,164,460,111,35,16,19,78,160,369,24,411,359,276,141,80,427,25,126,134,126,426,596,16,134,78,159,620,238,150,258,309,298,95,68,707,1744,89,507,310,543,149,322,94,78,301,620,962,32,792,41,1511,148,355,851,246,484,793,88,68,240,493,1591,457,420,236,68,134,27,88,190,139,118,75,207,258,31,88,1834,191,420,39,114,23,26,205,99,1416,31,48,176,385,46,71,208,214,461,444,508,71,1094,2007,279,78,505,156,156,512,97,1106,371,303,30,223,146,423,87,125,84,69,266,73,63,123,1205,1838,2000,70,81,136,305,957,457,675,334,1574,41,263,270,36,93,808,18,428,292,95,430,356,141,139,1009,212,150,189,374,204,38,462,360,128,36,157,394,143,873,271,198,103,210,146,363,1244,155,185,1337,87,374,149,425,802,18,346,937,569,240,40,269,89,457,570,121,273,218,738,51,78,273,313,32,115,59,193,941,133,547,304,210,114,202,6,93,37,574,110,762,62,1042,178,943,142,197,195,468,96,1079,304,77,53,481,418,264,252,48,99,374,29,1040,343,248,283,87,86,170,64,229,34,240,201,67,408,449,138,470,87,36,166,1541,322,222,232,22,178,207,56,642,1493,120,697,79,379,153,208,2001,26,76,305,714,723,263,335,369,571,109,210,1087,1427,117,715,498,539,18,310,514,112,121,370,543,773,141,227,279,482,90,9,490,421,139,443,90,68,31,1267,171,554,365,39,80,628,248,53,974,521,178,119,400,530,788,155,182,426,1796,89,93,49,41,73,389,73,682,318,78,53,348,818,210,56,209,70,200,768,90,2007,1352,322,227,1525,760,15,333,662,162,117,308,228,807,252,80,441,622,717,292,143,312,408,2008,569,308,1038,145,95,50,1104,69,152,408,383,139,826,73,423,865,821,558,255,1149,85,619,106,48,340,1220,474,222,350,1382,644,175,1096,263,337,61,2004,396,113,229,1404,179,352,1039,300,1967,822,340,57,69,320,370,1514,72,45,515,551,295,352,93,23,685,116,93,502,201,149,90,338,183,329,36,357,276,674,536,24,74,28,321,277,45,162,83,95,513,1274,1372,399,284,374,51,186,595,926,1018,88,266,41,20,494,384,1056,720,215,108,205,327,417,213,358,1034,440,205,2009,142,551,42,672,274,177,305,85,257,127,114,159,205,467,142,83,42,615,194,2006,1048,141,156,462,347,266,172,236,123,188,275,619,33,545,235,906,90,34,158,163,48,47,230,109,95,107,312,67,727,281,449,103,99,27,12,111,36,67,265,247,135,44,46,437,137,493,902,231,109,317,58,363,104,831,40,395,45,66,639,340,677,71,68,147,319,40,341,67,168,655,493,1734,61,1000,383,186,416,767,94,201,224,48,656,627,282,77,99,1285,383,113,82,156,398,170,2004,666,41,698,43,82,718,360,311,205,512,573,71,384,1398,189,815,199,16,134,20,34,919,62,81,81,242,670,166,116,81,195,751,123,453,89,1227,339,73,431,119,36,535,83,257,506,77,326,262,142,84,117,419,134,72,86,619,1737,162,163,35,18,508,122,195,69,417,139,333,71,608,480,17,63,52,284,55,86,96,510,748,40,849,65,212,69,281,329,1135,46,34,160,99,1158,544,105,585,64,216,266,276,81,98,101,430,452,149,39,46,84,286,510,542,446,440,29,234,39,1738,298,287,256,298,75,324,317,108,497,228,700,261,270,93,938,878,470,205,301,162,599,229,42,212,361,83,146,40,146,974,628,413,98,284,248,130,1304,195,214,162,346,1025,459,604,436,79,77,255,41,263,1016,365,181,244,402,130,1656,223,29,1483,112,170,77,139,430,390,353,423,110,393,1388,215,94,97,212,736,268,67,65,38,359,104,168,135,24,852,1817,33,341,563,31,188,308,167,955,722,1212,377,88,602,250,725,140,88,978,120,72,39,1121,510,558,26,51,499,22,648,468,1170,779,706,126,2002,219,412,456,1482,181,360,69,644,543,269,279,121,228,527,32,217,174,289,68,301,124,927,129,74,118,554,334,981,409,57,1308,1503,166,499,398,164,514,30,1565,97,151,206,530,42,205,39,479,94,583,158,53,57,111,140,1134,244,29,124,120,61,256,905,173,79,139,1593,269,435,1137,19,101,260,1117,69,388,1120,1550,872,350,65,25,125,678,211,58,184,74,143,82,575,533,230,66,287,82,32,95,102,132,177,326,123,537,1112,13,551,190,159,165,273,450,36,251,128,123,108,131,400,186,85,111,158,127,76,52,151,73,164,1584,440,177,50,1221,239,208,125,936,122,239,107,269,445,405,508,509,1677,374,394,146,30,721,67,1062,157,1149,574,319,1228,84,478,439,606,353,1218,128,532,397,216,91,657,163,136,44,101,157,309,62,130,388,266,63,220,203,440,507,196,782,29,528,170,258,67,54,170,804,1440,409,270,72,269,1188,609,318,136,514,1146,441,347,238,230,21,154,54,74,220,440,520,278,104,488,33,568,1431,575,258,324,196,85,111,46,202,462,259,562,36,925,156,1867,274,83,363,56,86,35,511,37,275,205,353,49,75,91,418,672,189,288,830,635,64,169,161,278,49,380,36,312,81,214,54,61,134,147,217,77,178,454,64,267,466,106,69,62,642,391,1109,103,39,179,44,641,43,132,1239,252,168,194,247,105,250,167,513,350,1027,85,142,554,1070,225,191,1309,28,1799,335,154,196,640,698,308,185,147,360,124,186,129,76,21,1115,153,39,1481,100,556,229,53,463,618,2382,383,110,490,109,1106,292,603,130,31,286,1376,31,229,80,49,178,145,109,73,44,53,41,95,422,947,170,266,106,230,116,259,221,943,269,139,227,847,120,1086,197,314,188,97,78,213,364,91,745,187,142,1381,463,351,36,137,615,258,538,46,728,908,78,218,207,1102,1104,311,170,440,179,103,138,274,430,238,242,135,251,330,1116,340,108,391,219,98,56,102,84,96,206,376,494,113,90,160,367,821,118,44,1397,477,490,315,47,67,202,434,36,111,659,110,109,908,173,1292,266,271,266,104,471,274,793,945,161,463,9,1210,1113,276,546,128,153,66,191,309,131,515,1019,1128,923,111,811,251,123,149,16,99,23,72,1377,168,95,58,414,129,81,486,294,250,233,65,289,2007,194,215,306,681,659,165,73,228,684,439,501,354,28,45,90,455,321,165,403,144,84,1539,622,110,30,313,106,857,159,203,59,500,331,74,1113,127,267,115,1198,104,209,20,520,841,188,1290,126,239,1998,193,434,156,115,230,1678,75,154,225,26,577,21,546,169,168,1004,207,597,49,84,323,156,653,115,1019,42,32,597,398,174,116,204,158,75,273,80,288,1284,122,190,235,127,419,150,97,68,948,67,83,194,395,294,75,638,628,1126,178,34,792,379,160,251,340,655,146,196,170,1356,32,92,41,39,22,1718,407,690,117,159,168,931,193,118,165,455,49,234,450,548,127,110,352,629,126,855,265,1035,106,748,188,15,480,545,52,68,231,495,1431,127,976,397,392,145,618,431,172,163,47,345,467,488,343,191,40,1345,256,330,80,229,474,79,328,294,180,868,56,166,107,178,190,145,17,136,213,91,481,2002,60,33,2002,152,331,155,2000,132,228,163,261,914,293,182,197,209,584,265,398,457,195,97,806,970,1994,185,175,1543,103,759,80,632,481,8,101,314,594,175,545,128,122,68,131,32,49,34,614,233,121,84,560,106,118,81,180,823,269,75,37,407,531,235,370,465,458,468,166,315,172,427,69,352,210,13,134,422,43,174,66,147,322,92,1262,70,178,33,89,66,195,233,1190,291,190,471,212,71,33,77,2016,373,250,150,191,256,256,1167,69,22,215,167,98,80,142,283,740,76,69,624,225,555,768,328,98,66,370,166,119,71,10,809,313,234,456,50,209,98,311,295,214,135,383,1116,422,167,125,520,454,239,159,399,222,495,551,1081,633,550,108,1997,87,552,411,497,1040,161,85,180,141,260,512,169,188,94,689,104,489,64,86,386,142,74,81,296,159,98,184,202,670,60,140,119,725,238,13,67,101,136,78,64,580,55,72,361,644,565,512,447,145,81,46,799,411,36,1400,592,75,176,398,1725,90,213,132,90,418,116,16,56,103,2005,362,78,611,306,30,115,72,298,486,94,148,477,283,417,305,1019,1999,113,98,87,173,122,691,387,538,597,277,101,544,308,29,87,81,65,18,124,234,56,182,387,45,233,1400,971,75,102,117,381,415,95,321,209,130,283,1404,207,534,123,499,314,70,174,220,1231,132,657,37,1019,580,99,377,1438,40,88,115,661,616,123,1681,47,345,325,505,281,42,148,16,437,37,204,1702,577,690,23,619,107,322,1106,387,597,101,238,1401,55,2004,122,99,83,64,664,78,187,243,39,298,490,280,117,290,511,122,134,623,203,39,32,375,1064,1006,89,155,338,142,270,1489,34,72,1588,108,476,444,168,25,655,300,136,23,35,101,48,219,429,440,2003,303,107,137,369,869,95,432,576,2001,20,80,258,44,1457,49,57,185,486,456,195,92,98,642,2006,106,369,44,41,86,329,469,367,52,559,505,47,557,269,392,568,119,57,109,61,586,31,183,714,72,401,96,1144,450,190,55,112,532,301,93,124,172,712,1414,563,1431,307,904,214,100,153,30,72,31,112,347,228,341,80,708,103,180,1021,905,525,471,147,357,774,36,22,29,992,34,66,219,319,360,135,61,588,104,256,613,429,418,249,56,369,228,762,45,347,230,360,27,182,481,91,189,20,128,71,191,75,549,432,83,112,182,445,52,315,127,771,180,540,84,859,45,266,59,640,343,801,139,336,422,81,472,82,125,79,315,112,177,339,164,331,116,104,69,62,482,582,109,328,415,262,957,218,25,135,492,54,304,45,380,143,52,320,345,411,243,258,40,158,318,229,247,644,64,134,188,147,20,607,235,579,1398,890,723,55,40,206,939,72,213,973,847,190,430,987,167,424,546,280,317,78,162,2010,476,264,244,82,182,780,377,72,212,138,116,138,410,859,184,427,278,150,36,97,290,372,1297,52,246,831,78,119,388,1228,151,1649,123,359,168,548,790,114,962,568,78,459,203,826,109,459,69,134,101,193,448,35,30,592,54,201,430,329,301,781,130,146,110,97,1134,183,99,112,165,1113,963,218,213,263,100,63,714,230,324,75,11,150,131,141,317,129,18,142,420,216,122,237,130,1995,108,703,560,109,544,86,251,453,114,241,44,907,397,286,28,1264,149,142,284,43,285,564,1859,53,543,103,177,447,169,141,118,57,283,136,559,188,137,290,235,179,111,541,316,432,655,704,107,166,444,2003,157,362,23,86,2006,217,149,156,246,321,85,1292,150,135,728,110,84,182,428,412,172,571,368,423,303,67,435,684,58,85,73,186,1026,385,101,555,202,696,114,122,55,1192,1493,151,782,265,225,303,549,52,288,770,100,449,722,117,437,818,77,40,168,234,76,456,1715,31,2010,212,149,89,629,58,232,59,46,804,1365,145,81,301,276,1923,68,130,361,648,10,85,15,882,545,291,803,1155,1677,118,421,770,1530,300,10,325,649,89,383,297,76,1162,284,155,194,35,32,61,367,42,715,383,347,844,379,263,900,471,55,543,397,452,645,60,154,351,93,771,40,29,157,1481,97,154,256,548,436,1637,143,145,472,1717,368,237,239,2009,82,117,97,397,120,241,1156,83,593,51,190,38,470,52,1606,41,1047,68,234,134,1002,101,260,203,971,561,106,64,132,118,37,783,218,301,334,91,512,187,211,226,951,325,97,714,317,353,613,779,226,209,386,276,1193,42,1606,136,174,815,193,271,70,64,547,69,189,13,122,449,117,84,638,970,1140,86,85,581,32,190,1153,68,1962,169,597,336,1031,251,98,65,64,1127,1067,815,87,308,203,53,566,627,376,802,57,915,136,191,96,246,403,253,88,337,78,158,469,149,68,586,462,182,259,255,40,27,39,97,2008,186,269,188,922,455,73,287,162,1006,430,21,328,792,440,414,45,1021,303,91,96,29,596,174,80,400,170,345,43,190,329,94,600,231,39,75,40,42,202,62,52,111,210,156,160,450,38,778,94,404,58,52,303,192,173,244,326,125,41,509,127,708,77,385,102,51,42,194,164,381,444,279,46,108,132,167,623,91,835,158,23,44,268,31,161,107,41,90,169,32,222,953,29,22,58,726,33,104,163,134,305,931,96,568,681,1144,330,713,430,371,470,114,99,173,977,67,125,47,74,401,146,237,82,404,311,40,207,451,179,412,453,379,685,526,63,635,101,36,468,172,78,173,559,664,53,360,181,1925,656,239,106,227,398,1359,25,159,41,107,220,559,622,99,372,203,21,455,710,46,526,403,1353,52,660,37,57,31,45,191,212,366,225,76,66,57,80,103,93,86,30,246,2002,53,2005,107,452,467,2006,541,322,57,326,107,52,131,340,1195,49,83,1590,313,272,57,82,1114,200,29,435,1556,302,328,998,1037,411,520,276,153,211,292,1178,432,200,178,531,166,333,313,620,286,601,24,127,1996,124,352,112,140,842,634,160,590,327,1193,146,1504,65,107,434,42,134,39,20,88,1085,596,65,289,333,184,95,466,332,329,1195,476,415,259,412,102,689,219,65,219,209,143,49,23,855,230,2003,133,85,214,301,157,290,338,134,22,214,133,607,92,247,167,543,388,380,115,130,238,161,51,418,97,555,102,466,128,351,1229,124,56,851,65,774,133,513,228,102,208,852,454,812,111,274,257,10,916,274,245,169,81,151,411,1376,489,121,318,608,147,42,123,137,72,253,143,98,523,528,209,221,475,124,48,350,900,137,285,125,60,218,984,1285,256,205,206,104,592,68,76,241,1155,116,75,164,205,247,60,413,280,277,242,526,1868,74,271,131,421,63,23,78,45,140,57,1849,266,298,180,1780,60,202,878,570,54,1830,212,249,381,104,660,253,292,866,212,138,233,28,671,703,97,437,123,286,154,110,474,486,183,76,739,157,294,238,636,127,276,94,894,656,320,231,125,291,107,170,190,744,155,888,75,502,114,241,68,99,164,680,372,325,243,142,52,35,162,766,828,51,445,420,629,1085,291,351,336,253,98,137,206,30,1249,338,217,81,206,2003,306,161,343,84,1541,1068,92,71,406,211,18,54,271,40,137,96,2004,43,81,44,590,72,69,189,354,399,293,11,290,220,30,179,1454,1143,171,339,915,326,2004,113,584,541,447,75,38,193,505,31,418,598,407,287,186,472,132,202,495,152,97,379,291,821,454,767,218,434,1466,87,321,500,226,638,234,141,2008,228,283,122,1084,539,402,313,311,287,1056,512,145,397,422,263,413,655,283,209,185,591,56,521,943,724,69,235,116,140,111,116,379,59,41,283,130,181,121,49,226,249,238,116,38,127,1009,81,189,187,388,84,153,330,640,648,132,112,272,54,551,571,795,208,127,335,250,130,129,110,142,736,223,283,294,106,229,340,130,92,792,458,82,245,274,193,1574,776,142,441,397,1593,87,229,336,1186,632,135,198,239,332,322,501,11,340,763,89,373,831,133,191,64,585,25,202,82,385,481,628,1147,293,484,284,178,420,82,125,68,177,303,249,308,51,2004,46,81,312,69,1357,1538,52,277,95,308,303,537,70,171,84,230,115,97,236,85,361,44,215,155,62,106,170,78,807,773,255,429,73,421,100,287,951,169,453,790,480,253,191,559,34,663,385,175,81,2000,226,170,77,320,175,581,141,114,975,283,58,483,620,140,435,82,230,815,221,39,472,904,347,729,100,312,75,480,58,40,75,42,47,122,492,102,290,81,1833,171,454,158,225,28,120,578,255,65,579,56,431,53,1617,190,207,243,102,1135,179,177,217,317,103,876,137,117,268,826,36,989,130,524,358,227,166,328,342,313,367,514,66,676,84,24,226,129,57,85,606,166,753,209,449,2007,195,85,314,272,239,113,89,98,200,103,1031,100,219,246,468,332,135,129,266,166,98,1953,147,141,265,189,248,18,740,79,1608,699,436,57,154,145,398,165,356,757,245,185,37,125,53,239,388,1131,330,215,146,447,386,423,112,820,673,77,1243,53,59,69,520,436,147,90,346,91,144,734,918,541,52,278,302,748,314,112,26,45,49,662,189,234,721,573,363,954,16,95,128,61,166,52,529,425,552,1554,145,285,74,386,129,73,235,670,386,158,679,244,20,1645,1023,234,499,683,74,60,19,662,456,63,1406,555,1585,868,100,316,425,179,43,117,71,103,403,149,57,882,54,581,210,178,487,153,1149,82,321,1256,1134,419,289,913,134,202,303,363,22,1227,130,172,60,31,1267,573,256,697,59,187,406,1078,226,600,339,12,49,178,31,236,294,120,915,333,863,223,209,55,1134,412,46,77,63,160,311,519,57,41,718,149,149,929,237,282,102,91,53,139,1011,27,400,518,107,592,223,174,500,268,629,300,55,97,372,72,85,410,142,212,36,351,39,11,760,319,35,64,108,237,323,82,146,204,2000,126,238,136,687,272,512,300,110,71,38,254,478,30,112,107,79,201,1333,388,885,733,131,551,108,303,706,448,157,407,528,459,93,624,90,581,125,591,174,229,463,206,901,135,53,81,40,119,152,477,101,262,88,2006,277,439,480,496,38,263,129,62,515,596,561,31,1058,2011,617,639,44,313,104,199,64,290,69,238,468,83,239,283,87,1079,827,13,1798,109,161,639,2003,648,293,640,684,798,576,284,587,226,719,147,277,944,577,137,96,149,11,135,588,205,327,258,1478,609,118,411,58,107,154,239,330,774,426,257,903,131,1449,127,508,1006,75,117,695,282,78,260,721,493,41,68,314,409,72,112,130,159,114,127,277,276,371,242,769,155,279,74,108,79,205,339,37,153,466,283,193,539,531,75,617,105,1059,87,602,125,58,240,324,474,196,68,107,565,170,55,70,317,1344,462,80,860,100,808,195,483,286,155,409,882,277,66,91,38,43,200,183,106,1111,148,431,158,177,1129,785,96,70,772,84,50,68,266,81,271,258,421,26,248,1725,1444,315,191,519,147,28,319,359,284,722,542,186,18,474,550,2005,437,393,227,642,131,288,779,61,531,1166,246,263,123,376,68,203,369,146,72,1325,56,59,234,459,56,36,147,28,810,206,145,331,282,415,492,34,173,98,342,32,57,65,30,921,109,223,47,407,355,317,698,648,67,52,97,251,236,25,169,390,291,196,357,133,195,502,59,260,462,430,134,104,498,283,145,279,520,166,516,1233,53,233,1903,107,574,2002,174,39,103,861,315,241,236,164,170,341,60,65,783,138,82,1452,184,299,70,129,155,354,187,332,247,98,72,75,205,346,307,355,223,162,147,74,353,247,207,310,67,128,549,192,35,484,67,378,496,77,553,484,181,22,72,108,205,216,337,61,213,129,1004,473,1112,112,183,583,291,1788,172,107,24,600,274,115,69,235,192,709,135,819,742,269,350,184,209,443,25,284,28,119,194,177,754,418,301,281,27,110,265,417,88,107,180,342,64,1526,66,610,207,141,573,271,20,23,577,944,57,59,210,595,569,381,242,243,88,2009,246,281,421,270,1410,31,30,23,39,124,67,1131,307,488,69,32,284,278,371,47,272,2006,97,447,273,1284,280,1660,143,1349,162,515,79,761,416,749,103,121,848,692,57,691,123,2081,139,220,66,195,350,244,372,378,68,286,411,178,154,40,308,92,89,284,11,881,153,264,103,141,39,359,1260,239,319,320,610,499,195,987,174,723,941,43,195,225,110,401,78,129,86,207,1418,438,294,458,908,449,166,283,89,118,1040,524,647,230,98,869,590,374,1027,923,62,23,359,140,306,248,431,437,122,217,492,119,66,56,1175,349,312,269,53,321,788,1013,110,1085,68,446,44,380,241,636,396,1109,49,119,1062,151,78,1057,46,58,448,436,525,178,309,707,396,904,188,541,495,93,145,172,198,71,324,281,1523,1332,15,63,59,1247,110,28,23,289,792,100,307,25,214,143,490,51,569,1259,43,195,724,293,63,564,51,338,113,938,112,45,39,120,1164,54,297,684,30,148,472,261,151,220,253,776,370,2000,197,215,75,70,211,355,243,63,85,366,321,86,290,52,99,41,423,249,77,246,65,804,17,225,654,560,113,97,202,502,68,1058,450,746,496,627,604,1463,403,38,772,57,298,798,423,98,499,79,191,341,175,479,107,155,296,221,448,169,103,1590,413,430,518,643,1161,1066,258,123,308,344,188,863,84,490,577,228,58,249,450,707,237,505,231,821,181,1467,239,479,609,24,1416,86,144,131,155,511,391,227,48,445,21,259,492,473,252,97,95,1085,187,31,45,514,73,354,163,123,714,73,319,357,166,364,251,155,311,214,144,219,61,876,44,62,72,220,416,87,701,150,49,332,182,90,386,99,264,371,65,1618,196,400,2000,846,635,603,430,1018,811,94,882,176,73,28,1606,48,928,632,306,275,66,451,235,1631,73,174,892,115,404,255,2005,930,76,45,120,224,135,111,777,957,38,471,89,108,103,649,200,442,34,29,382,220,921,352,96,39,214,137,1133,282,1549,261,393,55,1159,75,186,196,135,141,106,327,496,123,39,209,943,576,23,188,53,515,130,219,160,263,867,943,60,125,478,148,1169,305,336,373,565,65,1460,435,342,973,40,442,1385,258,265,347,106,903,504,259,291,515,112,739,276,56,428,68,227,599,311,244,77,152,144,303,272,1451,1005,79,375,294,292,92,340,65,192,1027,156,42,36,1391,656,242,557,16,195,655,37,961,53,20,2003,1542,88,265,106,632,995,82,250,159,173,401,200,87,66,523,355,795,747,83,908,515,76,384,98,465,237,498,110,83,347,385,87,440,932,62,270,151,694,639,494,1464,112,371,381,387,41,310,176,427,447,77,557,393,257,2005,844,240,568,76,445,655,1072,407,532,34,11,518,292,197,90,101,310,720,130,325,37,362,529,73,81,599,1112,285,132,64,26,100,581,115,715,308,283,75,45,247,261,122,224,93,61,177,41,110,170,652,1345,453,253,72,1604,508,12,840,479,2006,911,254,74,526,129,93,1999,322,360,36,1359,179,316,2009,379,98,316,71,236,57,154,253,64,351,444,664,59,147,97,85,155,28,79,1250,493,106,367,436,406,630,1628,407,183,1042,160,52,207,132,152,306,93,243,88,42,525,145,967,333,176,110,96,120,819,214,223,411,236,76,163,210,67,22,94,28,179,174,84,173,39,308,613,36,58,248,145,147,84,179,116,253,63,221,959,22,117,163,42,590,517,224,50,828,688,61,138,242,42,196,255,163,100,212,40,434,882,1274,401,168,66,973,183,182,130,1517,534,155,1152,828,159,2003,125,152,524,176,407,961,616,193,300,26,989,953,99,131,339,127,405,38,265,372,254,393,200,217,28,108,59,231,32,660,75,357,16,172,353,19,49,158,93,1143,344,85,788,79,17,53,269,155,2005,779,86,520,262,174,262,66,499,106,701,30,423,585,107,99,103,504,1455,42,430,393,693,90,688,64,807,732,72,55,29,315,152,104,42,184,90,329,228,703,89,81,76,162,872,374,112,522,74,113,231,91,1104,69,506,506,129,612,49,175,81,561,274,135,100,72,652,134,1193,259,439,107,69,110,290,711,27,489,108,1444,73,143,121,60,247,116,480,366,239,175,2006,125,304,1308,199,49,161,40,90,930,349,768,510,345,60,199,183,44,1119,112,1675,59,114,346,317,45,417,258,117,79,823,118,144,90,205,1584,171,93,220,359,93,106,89,190,227,44,109,90,97,81,508,204,545,110,142,131,293,265,273,538,460,31,264,43,668,317,71,38,49,103,311,38,524,128,250,250,50,38,535,419,616,243,22,155,189,1448,357,86,275,376,760,326,320,149,29,154,254,146,263,102,298,1997,112,35,822,415,1996,63,54,49,317,111,673,52,1431,55,428,205,441,217,643,497,479,75,226,530,98,71,484,133,168,397,301,56,476,29,89,10,223,129,27,1092,962,59,23,144,313,1584,322,239,335,621,370,319,742,113,61,762,244,130,532,122,77,417,93,575,149,188,283,166,90,216,904,390,104,195,69,97,163,123,279,439,43,95,265,24,538,855,637,324,451,604,300,142,212,1999,64,225,348,192,303,324,463,383,111,689,457,143,50,290,928,134,176,137,350,31,1229,209,423,95,92,32,108,109,1652,1837,531,24,89,298,81,177,100,568,30,383,96,85,86,231,674,60,122,295,219,422,176,111,1213,99,81,305,730,368,159,1078,227,833,54,91,90,96,458,1116,224,403,132,203,708,168,51,98,315,148,790,182,111,145,329,2008,92,291,89,445,589,174,12,37,1536,1325,250,147,53,188,1079,510,40,34,100,528,67,615,491,63,98,221,1301,62,198,300,85,129,127,612,179,703,93,98,219,15,596,71,255,24,48,1341,82,596,314,2008,89,514,243,268,444,156,696,104,507,235,49,118,736,297,2010,1999,601,300,127,196,53,345,112,755,1339,492,438,124,289,1466,507,75,100,256,521,113,14,92,797,61,207,865,194,558,148,113,105,118,223,865,31,353,952,410,700,74,296,27,277,84,212,96,370,1693,223,63,1458,94,87,982,123,496,403,224,2005,165,142,1772,115,30,182,622,38,202,123,92,273,2005,68,170,168,62,48,555,805,494,62,546,1344,22,195,340,145,72,46,414,66,127,101,177,658,329,61,119,193,91,148,390,202,495,61,548,1473,370,37,832,64,776,95,167,153,57,591,65,869,41,342,480,503,1485,221,804,1109,402,71,59,864,520,705,52,255,89,343,1993,1578,158,518,181,250,1129,144,54,215,62,428,439,602,292,437,68,93,184,183,75,115,415,345,184,626,191,295,149,301,189,868,112,174,158,54,849,127,136,157,99,2006,1368,1316,1473,245,1558,368,663,26,288,2008,851,141,1321,1254,69,65,227,67,32,835,101,156,92,2000,138,268,297,59,393,426,65,115,45,172,51,711,85,84,181,58,185,364,169,81,85,410,6,203,812,66,271,55,1718,963,504,361,526,13,235,2000,119,202,85,174,501,57,190,553,31,182,73,75,362,79,149,382,371,355,22,652,769,14,94,75,130,116,49,452,228,97,228,83,893,34,230,34,750,1061,685,77,142,138,1121,1998,166,247,947,381,267,523,391,692,344,265,381,2001,356,1057,176,57,417,455,317,57,132,780,555,199,409,1560,20,440,1576,499,211,183,128,28,35,621,44,72,114,475,325,184,310,166,32,133,258,168,312,929,230,158,630,347,18,66,197,172,869,531,60,198,244,170,242,74,475,124,442,1771,378,100,121,34,261,176,1163,668,543,767,152,812,52,2006,110,298,133,275,54,126,583,85,314,985,1871,231,500,354,135,613,199,548,411,619,267,24,64,1324,111,201,2010,337,248,22,14,325,102,99,632,37,187,250,293,44,85,212,71,176,16,82,42,252,528,1438,250,44,302,46,560,332,201,151,740,107,81,216,138,727,720,65,177,697,834,29,1467,506,91,120,1701,2004,207,110,344,216,195,50,220,827,1135,236,348,119,202,233,574,256,95,787,123,103,383,85,737,168,269,517,139,354,302,256,207,49,400,297,256,212,902,405,769,278,468,422,203,478,1631,1591,130,1811,61,116,75,128,76,651,297,149,306,124,148,148,130,1228,412,69,298,145,2007,299,113,440,52,749,222,272,501,378,469,298,141,80,421,244,138,118,81,159,64,476,753,80,16,59,116,22,453,625,108,147,507,381,66,17,220,32,448,278,149,62,362,1630,30,418,32,385,994,1718,210,305,845,1143,436,242,1255,915,33,76,37,584,227,66,661,64,633,36,893,35,261,205,489,95,796,528,416,116,468,47,23,122,135,313,495,61,35,393,483,177,471,533,131,321,165,983,37,65,130,415,420,358,137,55,270,713,109,59,290,48,91,248,165,624,207,89,454,127,168,1023,155,121,194,433,577,322,965,24,201,187,603,998,166,167,29,87,888,382,81,812,80,108,141,230,883,287,102,166,307,84,277,1366,174,669,236,384,213,115,961,834,26,504,27,34,78,60,594,270,610,160,820,27,73,283,117,458,204,354,166,367,786,68,192,391,155,393,204,318,437,118,21,256,149,541,557,237,191,185,2009,45,506,144,1604,205,130,1044,1343,172,48,403,25,134,187,1404,72,254,1181,129,223,165,289,415,393,49,321,844,898,47,420,68,543,751,333,505,266,456,46,646,20,45,20,647,107,117,773,443,66,468,78,408,244,126,323,370,277,82,2004,303,139,26,378,293,50,21,456,220,293,39,194,35,285,351,1600,318,63,304,30,452,86,127,533,62,238,237,94,804,1114,57,30,193,395,284,237,324,181,53,77,54,86,185,89,246,336,13,222,277,575,71,1090,36,255,15,80,112,676,253,254,184,373,230,399,453,278,151,160,287,326,435,2069,218,289,319,574,1050,409,507,265,61,675,137,24,518,67,1575,211,428,118,229,334,366,90,62,76,106,70,91,818,381,123,1267,194,397,54,1241,1165,278,168,1563,206,240,299,374,347,194,436,280,255,273,170,129,52,73,227,182,244,247,92,1265,95,118,101,376,138,11,139,484,664,148,119,116,506,145,48,57,398,49,133,168,242,683,675,389,335,2719,128,144,77,263,382,152,290,522,50,1711,90,225,34,82,72,249,102,73,676,2005,703,154,106,657,204,210,47,461,42,254,71,46,68,115,1103,43,47,52,70,287,527,71,710,186,84,760,51,971,310,106,135,562,364,81,337,86,1259,273,124,173,1012,285,77,98,66,287,289,29,466,336,251,130,390,127,48,262,643,199,441,168,149,165,165,49,696,147,67,54,1353,321,2006,100,111,259,237,399,640,1105,168,201,40,86,393,283,299,757,264,850,113,162,220,98,189,107,340,39,1041,489,123,700,1052,236,421,52,188,429,297,228,534,85,45,384,540,76,132,47,509,59,93,414,205,69,374,52,417,1489,163,281,205,46,211,528,337,153,55,100,163,704,1239,17,71,466,181,880,738,68,102,284,35,421,898,111,769,296,1117,98,1604,227,1751,147,1063,374,408,847,223,280,338,106,879,1294,1547,247,2017,726,35,59,46,526,346,288,572,122,307,80,1366,195,1492,191,1166,497,213,365,977,587,113,170,638,526,480,447,169,435,214,210,156,702,296,114,332,400,484,752,159,525,157,134,8,68,149,46,201,504,98,586,301,100,173,35,784,902,402,116,983,109,471,34,85,202,304,512,292,1053,248,151,159,137,32,679,68,412,610,114,76,270,1715,46,988,147,61,857,868,740,365,172,1211,207,89,1058,67,312,67,27,357,519,696,211,304,42,106,97,30,488,56,29,373,571,368,178,26,644,70,814,107,39,366,174,174,75,46,1241,1535,82,100,75,704,1998,250,237,407,660,1490,139,423,117,85,531,220,75,216,2671,150,666,144,99,241,381,67,291,117,182,590,1993,223,287,201,1342,72,379,37,71,98,335,170,140,321,18,73,333,141,583,145,891,233,129,992,640,222,119,582,681,121,589,1344,222,49,94,597,266,487,67,744,140,963,53,704,962,783,105,97,82,299,173,144,267,241,707,140,132,112,188,107,62,220,62,286,60,124,1081,493,419,238,39,518,998,129,27,1021,168,61,30,195,205,484,309,103,121,23,137,187,115,187,367,1057,31,172,536,620,737,285,255,89,1314,77,853,1086,77,121,400,16,763,544,129,274,185,999,100,266,109,1676,441,498,832,1157,56,245,221,209,123,464,728,351,13,60,180,196,291,510,20,579,361,492,205,543,949,169,18,238,106,770,673,31,573,275,89,2004,235,337,21,317,200,23,1495,329,621,183,92,67,82,432,366,187,1998,356,129,823,166,443,1074,752,427,357,281,218,87,138,290,92,88,492,83,95,219,59,619,195,91,317,44,1083,62,584,112,311,244,21,204,419,48,26,580,326,456,194,115,197,464,226,172,838,534,537,226,51,1426,134,17,284,402,338,333,153,95,302,40,557,246,138,284,48,253,59,590,420,177,315,220,243,310,139,87,209,229,235,80,634,76,64,281,151,1458,1057,84,61,366,14,621,163,591,527,79,110,1155,57,917,138,279,545,410,397,60,183,118,437,858,179,1997,91,13,1183,82,1152,61,90,335,178,878,236,812,272,165,105,734,210,195,178,40,356,1034,107,714,82,28,61,206,248,31,166,517,210,200,201,139,305,1172,1728,43,93,292,203,80,416,80,215,88,758,79,41,1718,23,253,110,714,580,1730,177,121,366,221,57,548,13,22,811,219,332,89,31,108,131,1016,961,119,107,477,22,121,310,397,430,130,247,211,148,147,1112,360,548,78,436,88,1216,84,327,1212,461,1783,49,216,138,426,116,798,203,627,332,23,245,258,1164,962,186,381,21,63,846,332,185,346,298,35,130,535,1213,1207,227,170,190,1148,285,1286,406,347,444,230,802,867,642,76,599,1414,305,112,256,188,457,121,146,1111,275,88,331,234,43,71,66,320,79,444,1633,206,68,219,56,867,108,89,104,43,34,747,588,187,294,239,56,366,44,51,398,61,2001,1067,453,78,317,403,291,157,95,467,139,167,126,60,113,407,176,100,64,326,36,28,291,472,61,446,18,464,1105,106,169,331,49,1072,754,182,200,47,121,582,123,248,111,507,555,14,64,255,1999,325,53,765,494,988,97,101,347,23,216,41,166,96,659,499,181,152,181,416,1374,30,186,729,660,298,240,109,339,1852,100,730,443,134,317,174,67,124,356,765,443,727,999,582,304,125,87,473,324,584,327,563,298,254,31,1768,106,45,458,58,33,96,10,214,623,787,338,328,2001,194,86,1831,374,653,934,491,148,291,157,219,76,340,70,22,467,1433,281,847,424,509,762,279,1509,330,11,788,119,31,736,640,195,75,1883,559,79,1739,16,550,163,418,416,535,34,184,80,135,615,669,1660,250,764,62,32,181,40,959,177,96,150,301,235,1304,564,81,671,432,2009,367,1045,42,605,295,114,451,188,476,1116,1380,295,320,428,810,416,413,87,597,123,136,164,396,211,116,42,148,82,972,443,80,785,56,25,73,238,892,923,145,667,245,33,418,395,97,334,58,235,687,188,535,375,49,82,612,238,64,44,37,811,314,67,680,372,273,232,127,505,223,492,101,212,383,113,963,56,78,295,112,213,266,59,212,332,137,254,229,37,94,310,229,316,763,292,56,18,93,283,86,158,109,240,531,625,48,1900,30,93,473,23,912,839,407,69,302,393,383,959,503,35,274,198,439,92,176,188,543,1464,1361,183,188,650,293,230,120,232,153,697,232,42,174,281,263,59,993,493,256,398,86,137,216,32,274,839,10,216,1053,459,1008,74,919,83,1992,92,37,717,853,90,685,1684,53,647,1817,847,93,92,321,663,300,153,684,50,164,214,288,531,111,146,212,81,69,53,76,618,134,176,162,269,448,133,318,241,334,52,88,507,177,1767,155,240,502,174,907,40,447,57,1024,128,47,156,517,461,58,52,2006,104,392,92,205,669,240,446,92,2003,398,1310,65,391,221,389,295,206,13,66,214,82,58,1126,273,181,89,79,176,189,136,200,280,23,336,30,481,342,106,166,343,178,700,568,134,760,1484,70,112,291,160,1616,138,192,756,1352,1829,285,81,1194,1185,527,812,52,503,102,22,137,77,522,100,797,117,540,197,257,266,94,227,434,39,363,269,722,498,99,891,610,174,523,127,613,559,460,195,88,650,133,229,127,746,488,17,113,182,142,27,331,1110,157,287,610,251,561,732,657,154,582,43,363,91,146,753,321,602,272,43,289,76,89,18,77,537,395,788,166,117,112,520,91,135,260,170,1229,204,428,1271,227,298,154,516,195,851,145,151,285,570,63,526,2004,103,1203,220,74,2096,595,422,427,43,265,146,55,42,764,428,136,254,398,595,731,283,66,150,468,189,1490,383,152,171,2009,856,122,636,740,119,160,82,124,1383,2005,105,140,89,44,884,169,1851,108,1732,54,249,70,439,370,49,33,223,156,243,186,114,146,281,364,68,102,252,661,107,369,153,922,494,442,61,434,86,1131,157,76,248,68,1245,710,168,162,291,63,90,373,778,234,64,997,480,241,548,54,418,142,921,113,680,75,537,738,169,176,119,515,317,360,475,264,1011,299,192,900,90,69,617,75,503,451,314,124,121,216,590,202,657,219,96,321,173,400,883,349,126,208,56,363,159,312,48,1232,234,81,55,1182,112,261,401,1016,151,748,211,378,49,26,1858,680,106,45,389,274,51,67,591,398,38,501,43,178,631,81,47,632,406,920,430,91,460,86,40,179,1381,100,337,67,1662,1349,282,220,335,177,397,248,100,576,83,370,221,14,56,70,105,884,86,402,68,568,14,456,85,116,79,84,378,238,497,213,237,91,489,89,21,110,822,387,1115,51,1825,235,128,481,88,204,359,132,269,157,65,362,1410,148,693,248,1497,494,25,1253,418,776,118,98,43,744,1151,158,208,233,584,56,1089,305,117,814,840,26,254,125,120,176,126,287,284,81,127,114,214,779,647,248,469,198,298,238,99,231,107,76,360,76,186,62,253,151,232,94,130,26,1581,207,855,259,280,15,108,167,98,318,107,134,584,257,94,142,58,158,662,74,59,173,1071,332,20,800,919,255,170,655,24,253,24,323,255,437,93,9,417,64,32,163,289,533,635,428,73,16,71,251,550,235,1069,52,416,168,146,213,645,64,1312,98,1532,521,322,262,96,679,280,935,591,120,153,34,177,268,158,2000,279,304,485,390,247,188,298,123,158,424,151,476,554,128,2000,73,854,228,84,248,94,437,284,246,74,125,74,73,41,94,272,438,124,238,1783,147,66,79,593,901,566,695,255,32,95,797,70,589,199,307,63,68,955,343,57,42,606,346,833,244,1284,278,684,71,238,227,1273,549,667,396,256,541,512,41,198,762,98,184,61,1164,147,15,62,38,32,1079,73,2005,723,176,88,547,112,681,381,556,62,67,948,57,36,338,274,171,117,2005,216,106,113,72,332,69,470,1228,1222,48,238,149,394,828,27,1008,260,111,147,161,170,139,24,126,647,106,616,146,66,140,524,790,399,643,342,338,330,502,224,265,126,187,669,170,28,75,81,215,1974,237,60,565,457,319,154,262,19,333,717,20,52,574,267,547,107,129,201,519,198,719,140,485,1071,304,713,197,891,1067,892,57,21,118,345,1092,233,189,96,454,81,355,186,277,81,1133,299,1501,1536,267,117,121,99,141,13,716,22,29,360,737,215,594,40,638,703,128,1307,1168,252,93,80,129,310,1948,990,174,841,384,76,855,86,119,205,144,2000,153,156,173,690,343,655,483,133,251,57,245,302,288,56,66,452,841,739,133,42,1130,885,1584,118,91,73,451,642,90,316,1107,47,38,702,57,210,126,181,938,325,75,521,204,223,160,282,916,678,1062,707,263,93,479,569,506,167,67,560,187,530,522,200,51,828,669,217,170,30,391,361,152,33,786,685,213,237,476,912,87,419,130,151,597,873,368,1750,365,368,180,57,306,256,555,66,803,195,286,455,67,88,212,264,727,822,100,419,608,405,289,543,305,136,1952,207,283,69,104,428,33,44,393,218,331,77,389,180,362,381,258,680,663,114,400,140,300,293,391,441,402,147,83,314,144,90,1515,479,601,56,780,233,81,507,98,298,301,182,667,391,123,147,246,73,501,984,139,82,628,471,69,978,249,100,120,1473,404,500,385,155,111,23,137,770,210,56,72,1908,696,669,611,1089,217,1709,788,138,66,207,176,884,714,575,35,310,80,107,45,299,220,119,323,182,342,293,108,144,128,13,77,49,262,427,153,48,81,92,173,66,610,1636,866,1572,485,212,381,86,58,263,134,139,646,438,216,586,569,624,229,481,336,80,125,780,250,63,303,91,101,391,68,166,125,889,233,353,121,285,315,534,375,63,1256,209,1143,498,2009,159,543,633,1469,766,39,449,84,42,1609,695,114,287,240,680,101,133,376,222,45,118,262,130,82,129,436,156,159,208,133,126,44,37,43,83,678,194,243,967,762,116,199,68,855,119,258,1325,959,103,304,67,132,433,110,440,85,234,576,223,239,403,427,368,362,1301,29,1931,74,1173,903,54,272,473,75,229,171,22,374,177,327,141,19,83,47,207,281,485,284,167,14,438,297,165,228,74,454,308,224,1057,478,24,263,315,425,644,115,106,186,104,340,617,244,214,279,1077,470,253,116,114,334,394,197,96,304,470,61,289,50,227,196,209,277,223,812,97,269,318,389,266,204,257,82,51,291,1958,89,237,99,31,302,87,206,490,358,464,58,71,41,628,271,123,2005,197,81,215,362,132,267,17,154,489,114,25,330,153,776,337,299,1002,441,304,294,904,115,78,260,312,168,515,2005,49,94,110,46,157,112,255,307,340,776,568,2004,285,70,264,50,90,93,553,299,33,202,168,409,1339,282,420,255,320,177,58,447,115,185,656,281,160,139,801,1248,29,43,194,270,1280,83,129,36,94,802,83,417,186,567,130,1155,395,114,40,43,64,61,597,615,96,79,1021,402,535,273,30,173,251,585,696,603,138,127,432,21,170,182,697,50,57,164,290,696,1997,332,47,912,133,143,79,204,79,407,146,143,1283,210,48,663,610,171,193,631,62,167,650,936,323,515,345,123,217,69,41,323,242,322,844,366,236,124,69,344,287,92,1179,192,61,173,260,111,254,155,436,412,136,233,410,70,347,259,146,80,650,1128,300,306,724,84,2004,214,125,56,1385,84,313,969,654,684,374,21,702,85,76,26,2007,280,212,232,703,262,114,243,1329,483,58,49,91,317,118,713,144,130,471,410,528,76,402,517,106,131,284,59,67,257,531,1994,1071,208,90,206,1100,53,794,30,22,104,217,458,363,344,197,2007,95,25,24,725,98,155,561,265,214,32,245,37,177,262,84,185,155,265,2007,201,292,585,37,626,194,988,441,218,243,345,90,74,274,2006,448,70,1997,239,886,575,252,205,882,262,36,917,232,12,695,101,1776,493,362,162,398,445,39,464,97,56,75,723,45,409,116,1195,107,110,203,288,43,263,601,32,86,1419,63,303,328,210,590,525,480,2003,278,77,297,453,54,408,763,56,235,373,347,437,404,344,174,426,113,122,1072,36,54,585,43,148,340,123,720,74,135,130,66,939,322,1176,323,83,134,240,189,327,297,254,58,52,1135,66,113,37,296,260,164,691,174,423,885,392,148,211,47,126,464,436,1269,236,1143,29,1751,552,1031,550,292,176,1258,84,50,117,983,127,453,176,128,270,381,244,85,308,115,161,442,204,56,80,371,1003,532,179,94,274,265,80,683,563,83,116,308,106,135,133,316,517,404,128,146,102,115,155,66,208,295,61,82,175,47,882,40,1302,105,144,534,673,45,227,263,124,95,171,616,213,53,76,88,22,307,256,852,325,115,194,38,372,498,746,775,603,301,1340,439,605,145,111,16,210,604,161,515,382,419,117,454,114,242,192,64,398,183,96,92,271,57,92,69,689,33,153,178,272,159,1512,232,319,291,63,133,658,122,187,110,1316,697,107,558,91,285,117,147,106,111,1141,188,157,68,1049,582,69,354,244,145,527,1432,391,121,80,464,91,557,189,639,300,108,77,71,72,116,768,465,225,24,391,1369,158,754,101,100,67,360,94,293,1084,180,62,75,33,737,122,184,104,213,242,1078,453,279,63,270,227,44,56,269,79,279,504,134,478,205,266,79,286,61,103,88,112,8,752,1905,341,73,280,237,912,238,52,136,153,68,798,1481,664,784,214,498,535,199,422,95,192,572,90,200,45,772,529,153,267,270,95,60,452,157,74,173,205,634,934,148,131,137,401,186,341,349,1129,815,1041,622,126,101,90,124,333,185,417,44,88,45,217,402,133,261,204,245,51,72,407,107,1427,76,49,118,68,45,1269,1533,58,635,1282,504,677,201,172,874,767,976,1431,755,274,86,236,77,124,334,652,400,122,884,355,90,1613,335,105,818,358,282,529,596,55,472,93,412,284,549,184,127,140,66,59,47,2003,290,103,165,121,47,237,63,813,1915,266,54,146,610,238,863,911,301,985,1678,188,608,125,1733,70,668,188,2002,374,791,581,31,210,1074,565,259,62,158,85,94,140,858,218,377,244,93,86,410,174,242,350,129,514,304,582,357,147,94,244,160,79,65,272,13,494,90,461,162,343,158,809,34,2006,340,122,142,1009,113,452,54,140,1070,876,1347,1108,61,245,169,169,148,66,66,154,127,196,1664,516,89,352,2008,1467,582,829,42,45,15,199,663,778,299,79,633,37,329,77,74,370,690,1492,80,105,203,82,1520,505,211,34,562,1112,206,659,1372,484,169,336,696,787,658,484,122,831,108,405,356,171,83,279,100,259,267,338,216,1996,356,306,387,969,200,139,110,28,105,1329,148,102,55,334,161,214,341,52,808,482,231,161,176,392,84,1253,314,88,1073,89,100,2005,227,158,280,1130,741,270,44,91,465,1125,230,56,238,1035,102,177,170,58,303,431,865,399,250,327,53,652,85,72,649,32,250,39,126,73,75,411,523,161,120,15,433,679,218,268,1376,347,636,1063,69,49,636,297,10,1173,187,105,190,546,680,25,286,425,76,232,1088,239,596,1602,120,144,62,539,303,1699,377,255,50,404,751,919,172,602,168,96,412,682,42,1515,190,299,504,193,30,477,52,952,406,252,56,378,44,28,244,100,213,586,1081,185,61,899,231,93,23,1860,98,81,586,129,709,189,187,188,1539,113,278,582,442,17,157,255,295,193,16,1050,685,515,347,340,364,244,142,1417,26,186,445,1535,60,312,33,385,90,492,232,161,56,821,161,49,524,62,510,410,103,459,249,9,415,805,314,616,115,1013,116,458,570,109,233,105,167,278,294,954,549,173,1319,493,360,301,25,52,188,208,338,259,420,137,89,78,103,43,682,387,314,120,554,333,352,177,107,106,232,31,420,217,48,587,375,301,233,207,133,175,124,1300,70,157,751,117,1138,1760,73,118,1351,194,103,972,71,52,352,189,109,227,827,587,415,86,259,130,27,214,41,712,155,539,25,2003,225,73,796,86,59,669,105,67,116,302,119,726,69,58,410,114,95,281,940,324,250,583,73,73,69,453,74,44,276,29,1003,614,141,299,496,248,206,600,44,88,576,144,173,1843,457,78,481,289,224,632,205,380,74,41,516,64,77,85,120,268,123,221,414,107,406,286,418,239,427,428,694,45,1146,117,75,1815,222,104,424,266,230,136,952,323,284,158,248,1173,50,90,764,192,140,272,251,94,116,1152,1184,174,162,156,1260,115,63,134,339,73,204,391,47,372,358,434,67,370,158,623,70,172,202,706,652,1306,47,11,62,21,190,193,140,98,35,673,145,398,146,149,599,50,1133,152,82,165,299,93,545,796,1605,197,153,418,485,162,630,442,48,207,356,255,423,54,321,1592,88,94,141,221,154,897,154,175,506,77,793,122,303,60,322,129,628,225,201,184,528,761,89,202,1461,275,112,136,50,33,387,257,21,1026,487,618,348,1718,1118,312,361,879,413,154,32,524,135,1797,57,170,601,202,61,382,260,396,858,49,149,232,229,511,1434,621,87,132,168,54,69,423,722,1435,100,90,44,978,94,458,191,504,25,15,727,49,163,440,221,22,275,142,1113,319,806,35,96,431,286,77,242,209,234,231,416,573,139,461,611,205,692,35,147,271,338,199,242,494,455,288,102,91,177,432,416,80,159,2005,54,134,426,484,178,180,914,179,880,495,616,43,464,116,171,39,199,85,733,416,24,1539,133,413,183,426,117,166,572,57,83,945,42,251,63,780,676,41,133,190,342,393,100,276,1802,53,17,589,899,161,102,864,143,699,98,554,232,237,178,213,978,634,33,16,221,1297,304,408,67,186,144,144,96,214,618,26,328,171,104,738,72,61,2008,381,460,780,93,748,252,38,66,701,121,165,71,1466,90,536,197,176,199,218,36,246,53,48,277,82,1134,772,120,821,440,892,120,204,465,822,104,392,187,25,61,198,1100,76,733,614,218,840,127,187,431,42,52,640,103,699,159,424,104,43,133,607,59,419,155,76,265,1460,381,234,422,290,564,1614,257,91,27,34,299,46,80,59,1444,43,156,340,308,200,1390,47,76,313,97,114,201,572,167,239,1998,302,208,1046,57,129,306,1704,186,277,30,72,832,224,239,279,683,87,130,107,828,1018,118,394,45,351,207,171,90,468,209,31,251,279,123,579,264,59,193,151,298,76,301,1279,1414,162,1074,656,429,586,210,68,416,157,1013,149,95,67,60,97,447,129,959,1469,48,173,137,138,27,77,315,312,80,73,129,80,1480,207,20,58,83,175,69,31,104,38,462,258,329,162,614,127,113,80,70,65,53,229,875,22,79,327,169,253,203,243,604,111,537,110,111,498,120,253,22,509,123,470,130,89,163,74,261,145,500,562,255,362,832,247,224,219,179,701,403,468,1603,91,278,797,465,706,394,70,2004,57,740,117,129,130,42,358,1921,594,588,91,35,149,173,585,387,318,87,315,234,88,279,1106,407,268,40,150,992,312,400,54,329,468,62,51,241,339,367,123,194,1074,184,266,381,1113,42,97,202,531,191,150,128,109,335,294,55,617,251,1044,351,30,78,116,145,58,266,411,66,233,1359,328,142,257,228,1018,704,377,467,86,291,700,344,762,876,432,150,415,461,513,226,2002,290,263,105,71,178,759,1105,45,488,339,471,714,305,86,330,241,301,572,180,305,56,95,548,144,1386,160,32,321,236,58,1026,53,1295,1077,160,122,50,282,40,182,228,91,365,137,291,101,275,100,864,170,34,296,109,149,154,138,1126,86,73,842,915,367,355,434,82,296,249,161,167,77,149,188,63,49,518,89,389,1437,588,719,686,110,1447,1368,225,131,118,118,577,376,1864,88,24,75,410,152,284,145,305,71,188,356,300,164,561,103,337,413,507,571,80,152,152,215,1042,291,204,123,471,230,530,139,887,28,623,152,305,443,70,1198,1016,97,75,483,87,147,111,823,724,399,1516,844,462,402,226,202,67,1116,56,227,558,26,744,95,319,150,418,734,1424,521,107,171,466,352,705,432,467,211,85,758,208,254,128,607,43,1038,385,940,1255,601,169,117,154,94,124,188,419,314,17,100,561,354,139,797,265,85,106,103,121,56,78,374,41,244,1202,1109,998,384,382,52,183,534,21,1076,289,893,118,72,197,396,1989,658,59,215,314,119,481,13,504,175,69,502,197,82,11,354,137,1710,1091,182,168,161,52,399,442,266,89,367,220,329,100,1326,406,49,210,701,323,90,336,197,46,972,544,120,67,618,343,287,48,36,61,1108,111,269,74,58,191,257,16,269,213,208,255,71,376,373,82,768,299,276,92,423,398,180,165,419,823,156,179,767,451,201,81,306,166,1355,203,271,172,328,544,54,116,225,190,92,39,107,289,45,137,913,586,38,97,144,638,94,558,627,281,371,757,537,241,76,46,75,623,258,72,1817,1507,564,397,684,61,1288,88,68,55,59,859,548,551,43,596,382,346,114,171,147,157,90,752,975,34,79,824,798,638,329,39,464,234,788,155,328,2214,28,83,79,1376,283,173,155,1570,2004,182,150,32,873,1149,234,535,17,190,243,844,420,720,109,993,56,23,176,131,383,105,338,1136,85,129,25,428,248,304,77,50,85,82,326,749,2005,1022,240,87,387,32,66,350,1016,325,12,628,509,1042,259,1299,278,55,463,984,273,1334,13,392,38,60,485,276,56,82,169,69,252,783,316,139,389,1442,307,62,183,424,277,285,11,482,59,1131,108,111,160,116,614,464,1398,26,355,226,172,182,219,85,302,514,40,584,387,266,378,151,391,13,43,14,1893,663,359,410,286,199,419,352,198,1998,88,297,506,285,580,133,209,1864,473,450,176,691,364,971,609,208,1605,315,141,324,58,266,218,349,622,640,125,469,184,6,406,114,421,186,365,448,75,1084,244,350,1299,480,600,738,60,142,45,802,249,264,1779,257,283,1032,195,72,27,164,76,288,763,43,25,185,565,171,66,103,14,248,163,232,509,977,265,81,96,35,102,224,669,278,461,196,523,1062,820,148,256,310,371,131,109,299,521,355,126,1557,295,419,143,149,550,105,193,63,718,203,604,119,27,52,355,95,179,257,300,342,820,139,469,166,40,92,94,41,284,146,35,172,193,91,42,178,112,1783,735,369,201,108,343,827,779,1617,68,96,193,149,233,257,501,246,292,704,401,383,174,1403,143,230,1111,99,202,633,40,231,1381,1012,919,384,928,87,210,34,904,755,67,102,110,135,582,400,119,106,757,393,99,585,92,187,52,193,146,46,254,550,863,184,146,131,601,451,221,67,951,996,678,2008,180,127,166,1326,206,249,252,163,1618,413,502,589,640,798,242,141,838,285,527,886,467,292,150,386,129,42,376,734,414,306,255,1352,238,81,406,335,1247,138,218,871,274,55,354,186,359,1057,621,81,240,239,1139,1802,777,685,277,384,934,113,89,122,252,419,239,70,337,27,361,558,235,49,65,525,197,100,437,87,87,344,137,60,127,320,155,180,156,320,53,1081,175,1104,72,85,15,84,456,113,413,460,882,443,332,116,150,1160,397,165,39,334,32,408,128,1091,586,267,265,312,727,531,357,1471,115,2008,247,920,107,207,346,90,203,12,157,1728,1111,257,50,261,1195,376,50,194,603,890,829,148,47,188,71,63,40,189,64,326,100,91,100,414,463,901,451,215,67,92,1133,38,66,830,466,536,343,981,91,345,199,355,567,739,201,291,302,271,393,97,453,1508,258,233,582,207,203,313,979,594,614,126,132,56,222,1011,208,801,45,161,643,793,220,259,23,78,47,205,320,105,82,410,129,256,295,112,1219,427,129,196,433,288,107,340,84,115,1086,75,42,118,516,54,333,446,84,1057,236,28,513,134,34,267,54,293,309,844,44,325,83,122,568,461,489,45,202,138,92,453,553,21,54,491,70,856,323,777,532,288,713,72,77,829,106,442,859,607,442,85,32,275,128,1075,416,57,916,863,47,184,73,805,237,59,1211,447,234,14,519,239,618,956,232,202,116,915,91,158,25,18,84,45,292,295,238,408,30,105,318,1482,303,113,263,411,209,199,223,568,587,878,63,146,147,151,277,376,95,25,689,192,229,307,174,649,238,317,249,422,283,108,79,73,101,156,131,438,502,60,115,277,694,140,205,427,434,433,157,662,181,523,77,302,465,460,180,228,915,367,1182,255,133,138,562,311,36,46,80,273,87,103,378,628,293,260,741,177,306,1280,152,2007,177,142,1086,792,50,269,1719,649,716,49,97,254,146,1188,254,649,304,374,138,66,334,617,80,63,1167,555,459,54,85,74,283,211,494,286,345,160,430,939,271,1608,107,970,60,1521,41,84,70,100,207,795,1078,355,234,199,229,636,357,474,679,180,290,826,499,92,309,192,56,848,2003,484,253,296,589,25,263,155,261,154,72,195,84,395,563,266,46,25,487,90,1997,771,105,131,37,597,457,315,105,239,355,370,64,651,65,163,1005,360,267,282,40,191,981,28,602,95,454,663,1219,181,107,59,314,954,1330,1755,105,70,65,430,903,221,301,1079,783,668,17,1632,150,347,208,89,812,567,102,304,482,467,495,1826,281,97,296,106,187,312,644,440,129,183,610,78,69,587,513,149,189,320,371,102,50,851,108,278,42,154,2010,101,247,950,311,444,1354,149,387,522,386,1057,456,128,132,158,949,102,49,297,426,341,24,174,295,563,84,107,39,249,66,188,225,89,92,285,186,78,1361,76,718,366,143,249,19,67,675,204,753,366,439,454,353,706,205,136,98,387,184,240,891,268,93,37,61,99,1159,105,58,189,228,546,235,272,172,120,149,407,282,353,253,44,626,367,92,309,915,333,200,108,339,791,123,28,511,199,1628,154,80,405,76,391,592,375,285,510,294,36,314,780,56,941,327,81,226,358,288,120,152,1547,223,74,127,722,184,82,692,104,986,138,206,218,288,1240,70,479,69,189,119,137,104,144,55,291,20,395,79,476,45,101,696,133,157,57,223,687,719,252,196,81,195,1197,358,62,1740,367,374,80,1327,323,547,82,148,414,1366,276,289,885,15,299,208,314,736,129,437,170,144,202,66,1295,354,593,134,979,1245,110,53,121,315,775,638,36,178,359,145,477,296,1018,141,1297,409,306,153,210,280,519,232,253,226,235,1585,65,232,124,78,421,70,277,86,139,71,96,622,69,134,403,134,37,525,182,178,271,202,495,177,719,567,339,130,42,110,405,106,601,218,239,374,348,249,391,67,380,1177,329,40,20,30,224,300,185,36,338,518,84,297,1198,69,529,572,134,1018,104,125,83,1091,472,1239,460,183,739,1804,59,108,70,236,293,571,453,34,545,164,27,747,77,966,210,31,48,22,24,52,824,321,98,45,371,206,401,191,689,339,355,113,170,61,156,551,44,282,44,507,227,93,42,63,84,272,185,136,141,54,494,61,344,390,703,636,294,1366,49,231,435,169,456,32,34,181,689,411,266,255,74,12,260,448,201,1859,126,472,453,166,66,144,56,255,58,632,506,278,735,67,1028,451,159,282,58,1309,868,903,129,72,853,92,50,73,319,83,91,36,186,52,29,248,438,25,74,2002,99,52,1210,41,1007,39,964,109,148,225,37,67,256,230,204,46,1581,640,323,172,113,206,272,46,1050,88,814,418,89,51,410,1118,509,220,559,1258,1069,101,760,141,560,118,292,73,1103,559,791,598,756,111,550,380,1169,124,510,227,123,277,235,419,234,135,625,218,83,424,759,25,58,573,39,95,313,193,15,44,666,47,279,383,93,88,737,187,120,1289,65,75,105,97,875,19,1008,395,279,109,592,79,36,79,945,980,15,343,105,268,1067,154,139,157,220,111,721,1123,380,665,671,284,274,528,238,244,33,866,72,140,937,516,58,297,65,134,182,101,265,136,334,62,989,259,13,143,599,849,107,484,54,223,48,111,313,128,59,593,193,357,780,40,54,276,596,2004,1216,332,86,621,371,354,1092,737,678,423,503,369,140,373,1287,1228,585,458,1883,1090,193,644,136,23,55,256,139,582,86,44,378,511,498,85,567,62,2005,1719,253,593,2002,781,585,130,120,92,717,310,87,159,1038,93,322,549,525,188,417,431,620,47,267,609,272,108,456,526,212,940,142,574,1303,136,750,1132,1240,32,798,126,190,240,315,301,310,297,397,56,39,85,130,217,715,1473,435,64,225,35,461,50,318,536,169,292,229,236,2004,545,2007,305,173,48,122,240,663,169,467,997,81,95,287,1341,98,157,633,692,725,46,109,318,1672,46,56,631,41,585,201,183,321,457,755,19,1249,172,294,512,32,313,2003,289,337,589,38,380,714,85,124,129,154,355,1495,465,428,1627,183,244,494,60,304,113,564,430,882,207,1372,203,591,31,710,104,17,353,853,427,648,381,87,246,500,338,658,1284,62,45,81,676,26,13,134,295,206,82,68,351,146,103,163,135,358,30,320,271,658,902,92,324,639,64,485,57,277,93,319,192,168,174,83,528,928,725,38,219,139,1036,427,298,652,290,429,247,242,165,439,268,447,410,268,319,260,206,90,131,1418,65,82,65,22,479,1082,115,224,1067,451,253,77,111,468,369,648,102,183,310,191,129,74,282,36,302,1333,94,247,1070,70,382,150,54,341,316,37,104,66,53,502,33,1263,2008,579,900,1101,43,659,119,663,106,159,102,264,77,368,777,176,411,167,128,116,139,90,162,1225,252,45,118,344,19,489,734,204,444,58,436,921,350,559,88,30,58,90,1922,377,82,419,868,201,373,398,148,826,258,672,1145,20,82,185,638,146,1255,12,89,152,777,133,73,153,1610,153,1415,43,952,126,75,42,243,96,801,118,100,143,127,329,1828,1615,41,151,152,395,544,91,193,930,221,61,198,96,87,86,237,326,1503,359,1650,347,130,322,73,463,895,102,522,123,1332,761,213,62,902,60,662,846,61,827,223,286,232,577,1302,578,91,308,157,780,192,526,50,65,61,319,414,68,38,251,342,291,410,367,207,769,883,345,666,384,119,30,150,46,933,291,921,86,168,215,98,411,115,201,119,409,708,809,119,480,190,312,172,1227,235,383,100,174,923,315,103,48,197,765,700,1190,37,497,1191,119,572,50,816,362,81,1064,94,76,266,857,205,1180,342,47,914,93,443,483,274,832,89,115,304,680,112,104,863,34,185,163,230,655,21,144,679,399,181,360,1767,48,616,73,685,257,131,518,570,327,568,307,1351,191,291,2006,2007,758,31,56,649,545,1316,786,58,120,53,929,713,41,521,374,120,844,373,42,445,945,864,987,74,173,130,241,597,258,114,72,122,185,90,24,142,1860,267,107,134,1994,226,74,82,214,70,281,2009,536,58,111,54,129,642,311,272,376,33,212,351,37,229,596,231,131,47,89,110,293,118,235,89,75,160,30,626,42,132,203,247,62,693,202,46,102,286,388,188,244,256,345,23,447,132,838,1107,72,219,330,224,557,933,661,75,84,52,417,52,32,86,54,939,782,299,747,398,189,36,704,157,107,313,360,119,52,1240,984,162,17,67,134,400,94,383,63,571,771,43,94,82,431,48,412,41,380,198,83,38,2005,89,692,73,524,336,280,300,175,119,103,151,222,805,139,356,354,504,310,602,123,96,939,542,1771,28,356,247,480,58,245,677,63,884,768,192,400,1322,141,32,521,285,479,536,310,127,726,65,51,681,280,716,214,17,229,923,183,307,233,222,148,45,989,185,233,35,171,62,194,279,1104,121,185,450,666,18,75,257,13,545,583,651,239,551,568,88,1727,697,274,114,178,1578,782,124,49,109,658,330,939,925,681,72,85,86,285,181,538,106,923,159,41,177,32,298,378,986,242,311,200,359,93,720,20,190,614,76,51,46,726,937,325,52,121,120,171,126,1171,435,586,123,104,423,806,412,363,280,1548,190,136,136,450,353,76,713,1123,145,993,389,247,460,221,552,55,57,1952,337,467,309,64,107,327,419,206,134,1610,266,117,120,130,1369,56,255,237,85,100,89,511,85,20,67,116,72,102,794,50,578,214,97,64,95,627,38,431,361,234,226,137,401,119,135,175,115,264,274,560,286,257,407,56,82,161,798,278,535,271,631,291,54,148,117,131,218,70,1121,334,95,356,113,768,98,505,159,546,76,320,1175,188,20,1576,177,52,77,150,1803,1948,161,357,131,415,614,229,73,51,551,1276,248,105,326,812,650,70,201,134,111,136,83,402,93,23,241,164,346,305,64,109,76,693,234,56,114,83,168,232,91,343,448,23,144,553,20,106,312,560,46,282,170,83,129,427,179,680,266,129,69,1069,207,222,1534,272,123,909,118,407,535,84,1076,82,30,225,269,76,382,76,782,384,125,1145,66,512,175,154,245,123,944,217,159,365,514,48,298,50,400,80,1036,107,251,134,154,1474,105,82,267,542,121,130,30,68,192,136,66,110,255,1443,129,126,204,522,83,218,250,635,100,45,55,156,719,789,894,1651,23,212,65,1171,161,97,420,166,76,47,213,93,152,275,929,562,36,1269,267,69,358,531,89,60,333,102,439,244,246,298,165,206,2007,898,162,50,1381,126,54,90,85,740,2006,534,234,261,86,1043,81,78,215,399,24,136,434,1140,301,1403,323,421,558,113,349,324,952,281,150,314,237,111,241,342,247,857,379,131,107,371,89,1391,118,150,283,247,332,125,38,34,590,583,15,417,1869,338,39,936,78,799,155,463,64,364,1759,54,649,38,67,302,223,937,160,57,1012,48,331,59,14,127,93,1649,408,172,159,607,223,99,350,640,649,351,34,629,59,61,75,727,402,29,673,1140,681,210,464,623,72,684,86,168,117,137,333,1085,284,737,899,169,190,71,363,162,175,38,757,185,81,401,671,111,122,1148,296,28,22,557,1302,241,694,357,383,320,314,203,124,779,324,604,679,171,367,26,295,444,86,241,273,120,396,195,85,87,609,579,16,324,280,1169,1122,155,484,399,29,187,284,69,265,175,448,35,1061,240,370,1446,206,73,274,718,710,466,283,138,647,128,178,216,840,95,385,57,173,83,194,1321,73,537,106,394,160,163,132,128,249,610,208,302,252,711,136,189,198,127,774,758,2009,214,197,36,231,254,559,42,320,1207,449,74,176,74,191,25,307,359,329,583,86,969,1789,123,197,230,64,65,151,139,288,87,354,215,72,59,184,61,316,437,621,357,176,60,26,124,1572,408,29,62,1024,381,177,628,163,79,237,247,561,64,274,329,267,343,96,997,253,151,56,668,50,596,112,1414,172,75,153,123,47,391,305,116,1079,1153,272,301,63,162,509,474,78,87,438,38,579,171,73,311,411,835,225,227,453,209,451,116,276,79,1026,623,443,201,439,606,108,371,289,301,657,166,99,218,758,92,298,395,35,224,54,588,777,32,240,343,87,284,230,427,1545,326,1054,495,766,413,115,215,194,1110,211,378,813,1293,237,285,197,45,44,34,1192,569,57,1226,79,44,672,201,41,295,423,1850,44,176,259,398,303,170,25,1187,811,87,126,187,933,92,179,459,209,34,41,1319,163,99,69,456,511,23,60,19,1439,702,521,254,212,152,234,403,150,682,569,446,798,553,273,1002,31,380,206,104,559,246,343,40,45,156,394,133,248,89,284,145,184,89,86,155,171,113,504,379,214,531,797,116,397,91,253,117,20,697,225,850,667,1283,406,369,101,113,124,266,114,569,2007,68,37,129,338,76,362,247,447,350,147,2009,34,295,65,365,129,370,499,121,116,373,352,58,264,98,81,209,174,187,64,284,582,236,93,199,168,116,77,90,95,78,557,744,259,733,50,2008,83,204,811,98,189,307,233,12,608,93,256,428,457,940,668,1385,350,361,371,87,140,114,995,25,149,222,421,194,549,112,416,168,532,110,187,331,891,248,329,54,109,1138,13,71,222,156,445,32,430,110,16,49,114,235,392,64,69,354,163,818,167,45,88,107,149,236,83,162,1340,584,27,494,65,116,2007,182,1059,423,156,42,57,577,169,376,110,146,68,54,71,288,208,983,72,98,958,1087,18,81,120,842,546,756,136,241,1837,58,372,1672,109,204,696,311,565,586,507,136,713,1285,151,1229,61,68,120,134,556,538,134,230,151,304,115,250,2003,679,77,80,448,147,34,40,75,169,50,1751,485,656,47,264,832,33,74,235,33,365,282,35,145,508,158,77,116,317,137,185,116,1043,502,296,1028,26,314,109,678,196,202,235,606,833,441,299,53,1336,179,2002,309,61,70,259,63,1016,227,530,315,111,105,502,77,357,420,122,125,46,80,27,290,8,160,84,29,406,71,433,216,589,1998,56,1030,898,632,40,63,664,116,1758,162,97,389,630,312,93,118,49,228,49,39,635,278,72,82,108,358,104,54,310,106,172,57,1611,356,64,270,33,170,121,55,381,81,128,727,285,614,201,180,258,192,30,557,139,101,14,664,255,103,2094,197,212,1181,388,210,172,187,500,250,82,159,44,203,505,355,384,186,104,180,123,162,843,111,216,262,288,101,2004,361,468,863,457,71,304,47,1402,135,181,68,176,107,84,394,18,87,763,274,33,235,71,189,224,851,563,114,185,186,1873,256,656,112,654,231,327,555,197,94,204,57,184,216,446,109,642,91,147,2005,133,169,239,261,221,80,486,728,1732,864,987,82,30,431,1726,228,394,435,353,124,214,645,43,815,556,268,175,929,760,67,59,672,537,1015,116,679,247,90,69,467,357,36,848,1039,706,659,143,149,101,1250,127,639,63,142,427,36,372,87,361,126,413,594,171,1175,681,103,19,86,394,44,77,108,391,283,187,140,312,273,67,532,117,112,505,424,56,403,114,600,393,86,432,374,250,332,477,612,118,2003,55,1084,2587,166,778,118,398,300,137,230,137,137,358,603,57,92,53,67,82,344,621,66,72,39,232,131,213,1935,160,229,495,361,630,33,96,615,186,193,472,457,38,434,433,246,189,947,131,629,599,55,132,65,999,69,459,309,19,166,466,301,84,94,209,501,116,549,1012,87,115,36,313,308,526,970,348,83,2006,63,719,1372,33,224,404,103,180,136,425,439,422,1319,217,1271,556,91,377,421,737,217,101,392,104,255,144,260,218,96,91,179,113,1059,328,188,400,52,9,780,300,287,145,184,442,81,60,275,259,162,173,75,186,105,55,58,127,36,336,668,1252,103,490,1288,70,162,294,275,205,45,322,206,99,457,1439,487,38,304,515,285,169,591,368,145,32,486,285,254,2004,53,207,184,89,68,257,484,141,238,122,737,38,41,108,332,394,290,245,263,74,92,55,350,88,40,459,325,626,111,183,77,2003,568,74,789,119,171,1000,171,324,646,279,508,334,164,44,514,674,142,229,489,698,532,90,152,224,33,67,251,92,58,1006,71,23,464,327,1011,32,813,383,465,307,109,856,103,215,52,136,778,231,28,39,278,113,105,381,233,232,88,658,62,232,474,1464,249,279,208,354,21,104,46,73,151,94,46,13,104,136,2008,410,24,2006,328,151,375,738,262,28,388,26,230,178,978,127,220,185,194,1950,48,223,355,387,74,216,72,408,574,80,12,147,195,54,86,207,108,165,138,21,120,94,423,237,1111,227,434,185,78,101,473,108,990,1124,436,206,133,440,89,298,57,1274,245,47,81,251,796,171,138,206,85,472,329,255,137,319,445,104,56,63,160,61,130,410,135,144,822,246,904,142,1680,96,1255,797,212,247,269,167,2000,864,61,680,1054,317,359,181,287,362,110,179,608,88,244,183,295,50,380,182,65,139,179,331,648,527,17,637,424,288,302,173,290,863,217,294,171,63,87,1250,758,200,1369,158,169,108,751,53,217,1996,53,653,386,243,182,73,476,11,1204,962,154,195,444,224,296,47,78,1063,483,193,352,29,853,313,93,142,284,38,60,346,629,133,145,67,892,241,751,342,70,1131,47,773,336,412,70,286,67,48,296,102,41,871,908,290,155,293,54,202,275,427,210,316,87,302,456,87,356,150,66,205,1045,1031,142,487,64,323,93,94,999,1196,71,115,155,63,102,28,103,44,1245,401,245,556,345,2001,15,885,116,36,279,423,609,431,178,331,356,554,432,386,1317,556,319,789,77,1025,84,774,387,57,124,371,294,251,111,809,399,110,182,922,905,217,87,692,92,161,49,411,408,140,70,14,493,117,593,64,511,69,823,1227,398,515,385,883,79,96,249,207,167,577,361,155,297,549,74,84,111,831,748,415,299,149,1159,691,106,62,192,649,90,410,321,24,68,112,136,85,110,175,197,108,29,503,759,62,505,499,435,39,111,43,223,59,67,622,152,51,79,111,166,595,876,636,116,107,70,108,1179,183,100,85,152,95,21,99,44,185,394,116,25,692,213,221,1282,2001,903,445,477,74,1270,57,216,75,183,24,128,457,67,225,92,79,130,220,194,723,58,95,32,22,361,110,207,449,233,360,472,47,323,90,564,227,51,202,525,286,239,454,313,280,342,91,474,170,446,96,160,236,1166,69,869,12,1050,209,189,476,185,316,218,84,681,22,413,1573,30,608,509,145,178,316,542,153,771,112,151,561,61,174,271,293,264,235,26,267,429,362,2007,349,666,918,402,201,160,304,101,968,331,54,852,100,114,17,1075,168,81,279,61,516,188,63,37,1770,126,272,549,492,357,471,545,185,153,18,135,243,18,340,33,1247,178,197,646,131,1420,87,584,108,302,709,480,639,59,409,32,1713,75,523,66,204,344,108,254,66,82,47,243,85,1295,209,1156,73,310,46,128,178,74,435,659,290,213,272,340,538,809,817,2009,119,82,116,543,555,532,435,55,219,1384,1245,886,104,257,276,620,171,129,210,459,144,1165,31,162,157,903,195,115,82,413,235,153,175,104,194,55,89,51,478,549,179,6,95,125,855,1582,298,379,102,742,463,56,599,845,132,63,162,209,330,467,23,74,233,145,329,113,102,59,852,236,194,467,90,341,152,117,38,137,123,492,417,43,25,554,163,57,263,832,43,218,486,307,35,48,434,289,352,174,241,175,169,662,1749,1185,136,874,425,286,67,21,71,148,245,206,160,620,91,364,800,39,51,109,712,482,103,71,999,219,120,287,90,175,152,1149,371,267,105,53,218,236,175,281,508,83,1352,105,419,333,155,160,255,261,320,694,292,67,188,410,175,373,139,1233,650,49,349,91,220,526,56,253,152,229,21,29,239,178,813,202,146,250,553,616,1441,349,533,244,42,278,913,256,900,612,24,432,88,508,320,174,2005,178,420,760,607,373,332,55,151,85,208,221,314,1202,32,454,963,312,540,396,1583,103,78,28,171,1083,245,1145,83,108,197,38,89,574,155,1998,807,574,189,159,47,105,241,556,1093,260,276,382,102,34,309,1170,19,437,248,132,275,646,655,101,549,419,166,540,124,53,59,104,218,204,157,320,86,167,395,967,271,50,675,321,31,76,289,1083,30,329,130,703,567,157,622,1336,208,151,76,75,787,558,1354,466,499,477,86,162,151,151,241,239,271,1785,887,530,939,60,623,467,2003,388,23,205,130,66,452,245,311,196,199,352,107,34,49,228,449,190,196,145,161,1620,318,211,566,538,880,47,107,1024,385,68,36,129,115,311,581,698,867,378,1085,174,2008,393,454,137,137,29,26,300,191,642,143,1061,2488,473,119,233,897,282,182,102,288,306,503,310,213,99,42,727,855,346,258,148,564,189,332,495,82,129,1727,43,657,31,48,250,229,141,124,525,915,266,79,370,105,701,987,141,474,490,33,668,254,87,795,115,358,160,417,98,138,679,487,831,629,87,296,1117,106,396,86,33,495,93,64,356,382,333,142,517,114,91,1335,239,134,2001,117,271,106,329,282,56,65,34,201,372,107,311,415,82,661,182,955,122,52,27,1999,181,178,116,150,76,286,275,372,125,470,708,302,52,193,317,293,561,196,72,234,629,130,24,316,36,1360,120,456,266,68,193,142,69,64,437,102,1328,119,164,475,178,54,159,185,789,1674,195,107,481,176,688,305,263,1417,23,221,288,69,234,1085,857,827,264,505,95,98,519,1237,152,43,103,137,27,337,56,530,100,70,257,33,365,175,121,322,1332,146,161,507,64,137,495,244,238,433,13,234,29,136,965,48,814,272,1099,428,237,450,156,93,44,205,88,1918,1817,394,221,371,578,378,687,180,199,708,45,171,95,600,80,153,75,521,66,122,46,212,885,315,64,321,128,496,157,115,75,99,42,39,428,243,1393,1399,157,92,392,69,53,123,129,688,164,190,52,67,100,298,399,1788,46,757,59,205,840,138,160,26,138,732,37,297,389,234,126,447,17,90,53,172,564,392,105,32,178,182,480,88,708,374,402,120,49,67,379,112,165,409,108,1614,74,55,437,148,277,209,22,511,206,175,377,445,612,336,46,195,130,58,284,130,200,685,30,68,283,135,87,351,77,38,129,409,133,145,1278,84,64,1415,21,135,82,529,1228,57,95,359,122,316,535,76,79,424,527,496,134,180,229,307,388,247,259,522,267,25,403,202,57,44,74,927,824,433,70,71,1242,853,45,283,133,246,73,191,703,293,227,349,132,325,322,1218,245,411,264,844,117,697,235,84,94,270,50,12,11,92,1106,339,202,324,397,74,40,54,53,187,619,25,83,52,381,117,53,190,159,105,72,59,518,650,34,637,161,14,165,178,166,32,759,297,306,714,209,459,344,589,90,352,569,333,383,55,33,102,1292,811,54,335,154,547,84,535,135,82,707,109,280,320,394,103,21,752,690,179,526,16,684,350,45,302,87,371,94,205,2000,330,50,99,300,218,151,134,544,656,152,152,247,34,40,95,364,847,637,58,55,618,501,192,764,1600,568,273,76,199,228,90,118,320,40,34,282,408,496,352,491,241,86,1121,307,64,497,548,608,171,143,182,513,197,42,64,143,791,68,20,51,39,297,108,157,430,17,89,98,230,928,33,580,142,1563,96,241,950,417,558,489,190,150,1318,992,236,93,950,939,190,174,455,339,24,296,284,358,361,243,790,272,1006,114,690,126,26,73,694,211,560,274,64,359,104,96,284,142,220,707,179,613,508,120,78,428,283,203,2006,532,85,377,158,140,195,382,25,32,70,436,1180,232,51,10,48,104,54,10,475,228,390,131,561,52,1253,206,80,261,355,299,207,106,1084,272,332,364,542,120,237,78,537,480,188,403,327,824,56,316,190,267,75,162,26,107,113,85,814,848,140,1092,18,110,57,341,40,89,1662,44,88,513,1085,138,222,104,343,187,197,123,112,77,449,251,94,85,301,496,372,714,190,187,1002,1615,47,2008,525,472,253,1668,272,224,318,363,347,32,133,438,225,454,864,522,42,388,26,315,547,720,463,100,262,423,147,1390,40,480,82,155,22,75,2008,101,1207,1418,131,897,332,399,181,1086,771,533,54,469,154,1351,300,274,434,585,57,173,110,170,1261,74,150,296,788,328,211,835,155,1816,952,357,220,150,608,116,412,64,250,128,415,625,71,266,279,138,40,104,29,460,32,344,47,277,668,146,254,471,121,84,390,488,907,284,161,66,66,958,629,134,245,216,1036,523,166,1043,87,97,40,519,642,50,70,131,254,429,702,680,287,221,1502,16,242,31,1438,68,18,342,713,705,1318,1267,176,101,1239,522,108,361,407,450,511,55,183,108,590,354,105,947,263,149,783,185,686,935,343,1196,715,543,16,124,40,894,326,984,74,43,47,94,175,102,586,215,108,538,708,378,308,210,350,134,81,216,266,346,109,238,573,502,771,487,52,115,371,86,419,112,361,32,144,366,374,135,170,525,45,1360,715,134,151,45,110,45,56,211,107,557,76,96,141,71,659,1919,462,235,304,112,2000,441,133,242,812,772,2004,73,64,71,199,200,225,56,380,154,271,295,444,108,364,290,64,375,286,80,363,175,75,64,1142,605,61,244,221,695,349,151,318,145,529,190,123,199,216,384,24,434,144,243,46,979,1176,503,661,274,433,137,117,291,110,145,46,110,462,2004,821,230,1109,165,810,591,38,114,1167,378,38,145,193,146,1386,130,1408,865,171,503,126,112,183,250,2000,81,452,254,756,59,117,309,31,96,242,155,298,393,986,205,94,366,161,490,72,34,173,62,473,99,121,195,130,231,177,1115,745,1611,53,1611,98,561,16,45,903,676,585,281,126,87,60,154,889,68,297,228,84,42,161,116,765,294,625,100,242,515,372,313,379,322,280,549,213,141,372,565,31,674,161,721,394,107,422,719,413,66,678,111,19,137,319,299,864,477,2001,85,137,151,331,24,1454,219,223,463,236,84,1420,297,140,139,65,323,282,61,209,91,126,668,187,259,1751,127,197,408,136,184,270,844,301,404,234,474,500,346,716,528,671,171,164,353,588,808,32,66,2001,268,1000,118,876,529,1076,428,279,151,326,1105,376,281,197,409,1089,419,1491,196,36,75,625,26,233,354,45,64,46,167,16,156,43,257,858,550,1572,12,60,694,95,72,111,119,67,441,52,505,220,247,611,512,39,405,370,131,392,25,2006,1284,292,213,627,40,295,229,1058,628,157,150,207,74,57,163,485,105,1181,33,218,1934,449,123,92,145,134,1228,201,82,255,1037,1147,281,811,680,386,223,603,93,165,264,81,551,236,326,65,626,31,27,82,320,379,143,97,1997,192,55,291,224,291,57,1168,359,167,141,316,438,437,260,959,553,123,49,283,111,140,258,69,963,143,1173,356,733,61,1021,51,115,520,388,77,141,31,167,130,699,384,347,1110,240,1401,884,373,114,1142,447,131,851,150,97,134,153,329,713,1061,68,123,231,947,68,258,320,112,481,58,488,764,416,279,88,216,1162,269,155,769,775,200,64,247,422,572,1809,939,640,374,72,1998,105,661,223,32,554,232,180,438,75,164,272,21,19,188,31,73,276,54,119,33,263,364,885,34,31,83,310,190,1129,15,240,1394,199,378,865,55,468,958,707,314,376,458,89,239,163,225,182,1759,119,189,35,1045,371,192,59,314,192,106,1644,371,41,155,514,14,166,180,228,69,947,178,259,522,203,136,320,196,491,168,189,348,119,524,174,410,103,108,189,120,123,111,249,77,622,156,71,308,162,30,202,82,545,971,665,599,823,521,236,114,264,131,63,365,289,366,349,1671,60,55,346,811,1494,225,127,94,446,1061,787,167,1129,2003,424,816,231,38,419,92,278,864,505,226,176,123,352,65,352,64,115,155,173,163,676,87,1664,980,406,215,117,408,33,33,559,171,65,2010,192,360,58,111,444,1016,40,63,134,295,134,316,107,108,354,82,169,111,62,350,222,368,54,649,981,194,437,262,49,135,39,1738,279,130,289,556,616,664,91,231,198,498,1274,45,98,90,138,127,296,170,257,172,303,53,285,528,97,80,113,455,628,784,370,98,86,502,121,1510,356,64,635,927,115,380,169,71,240,661,48,1547,33,625,173,222,477,96,217,486,153,103,269,227,773,467,32,1272,891,478,950,141,407,97,70,276,176,12,367,274,907,251,173,637,149,63,1171,1410,16,56,1690,64,37,84,186,386,204,533,61,925,894,581,42,22,179,543,128,1378,179,586,245,176,312,300,114,88,179,1057,68,942,272,841,183,99,117,216,46,311,54,452,123,268,288,126,340,1199,296,38,524,1100,84,290,133,41,75,71,61,162,15,2007,1707,838,466,161,214,81,74,369,115,373,319,430,505,2000,2007,204,94,209,592,258,295,876,225,206,564,354,287,88,189,64,171,2001,73,1044,455,476,1351,182,1194,19,207,351,556,65,775,343,78,219,218,808,92,195,82,1054,366,32,454,500,394,253,188,94,61,147,299,652,975,58,133,213,205,440,56,111,1270,54,58,1841,62,233,378,105,73,298,431,1426,223,604,424,413,263,87,244,38,285,173,111,342,279,30,1154,103,65,194,23,119,19,109,205,1224,280,54,117,270,373,117,320,223,208,634,455,185,319,1476,386,187,180,397,726,222,235,55,336,131,734,86,1582,27,1112,161,567,943,530,1614,310,434,475,2005,226,65,404,573,271,1310,669,75,100,295,1371,162,294,318,57,1790,1018,227,208,738,751,200,292,331,45,68,96,1309,40,942,26,72,73,225,136,849,48,661,94,65,63,149,218,725,1301,224,524,625,350,167,165,646,73,13,192,669,621,238,102,185,187,134,1174,141,529,568,192,323,70,121,512,182,119,95,58,128,135,900,1234,389,123,266,378,582,143,2007,179,406,148,63,367,207,112,190,147,42,50,100,96,646,401,575,141,731,262,282,1040,90,69,780,374,162,26,33,137,200,118,221,48,982,259,58,479,1076,336,67,124,19,27,318,102,436,331,111,428,1156,276,751,53,334,950,45,212,63,267,394,565,48,195,403,363,60,1493,417,471,60,48,199,1354,278,88,1808,151,402,791,900,2003,582,956,1719,221,555,160,85,1536,184,834,680,314,772,263,120,354,700,65,75,165,372,63,486,63,42,145,22,296,142,320,588,1051,732,643,138,36,648,166,1708,324,424,56,1996,803,1344,284,50,249,331,121,62,86,196,431,216,319,465,76,41,211,113,63,181,144,157,52,197,631,192,143,238,669,260,38,28,223,971,508,178,171,305,1112,301,356,43,209,227,275,546,622,290,521,146,508,112,579,836,333,589,126,383,215,313,298,133,349,746,837,258,569,136,758,978,351,423,609,148,879,625,185,184,121,87,87,157,592,131,196,329,98,535,630,170,295,164,40,24,278,1077,194,129,36,138,290,735,112,261,81,315,279,392,48,393,135,31,183,777,110,159,248,1490,557,425,1320,311,138,274,71,611,318,411,377,1887,101,575,81,192,57,159,42,294,95,818,222,642,359,566,672,479,630,263,196,110,918,1027,1473,460,104,397,207,79,923,238,593,223,213,1800,462,270,20,98,13,1134,570,69,643,265,221,120,96,545,34,1614,762,32,153,513,162,41,145,133,280,38,448,657,1558,310,1527,339,626,133,602,118,525,86,23,140,234,374,88,142,24,798,71,138,460,101,153,1627,106,749,375,707,128,167,160,96,2005,626,898,364,120,873,332,149,57,266,125,118,601,202,410,180,1296,876,55,246,991,197,122,703,214,1709,362,26,167,48,139,366,71,768,490,90,48,180,51,281,771,211,1411,413,98,30,542,57,234,318,206,463,92,236,689,86,143,67,536,998,167,1330,360,194,812,169,131,161,170,125,311,192,286,1354,402,67,186,118,197,127,224,88,918,28,372,137,526,361,77,290,74,295,325,321,129,363,408,807,929,43,101,285,107,220,283,268,153,251,314,878,376,202,41,76,415,106,92,2003,440,384,121,1793,127,349,968,1549,144,86,1234,73,141,192,46,974,528,154,1258,96,217,53,39,28,53,393,314,375,20,340,1086,180,302,456,252,244,185,54,93,553,236,289,1074,234,1872,926,179,188,135,98,199,278,280,134,252,24,39,344,727,48,237,187,1134,95,158,94,65,362,999,111,264,97,218,1810,79,25,1312,160,528,1930,1820,47,145,163,334,201,171,72,270,257,93,118,148,217,57,681,162,182,126,303,178,37,64,1046,206,192,113,64,127,209,177,281,530,237,185,473,493,29,488,254,622,24,69,161,45,188,151,337,399,346,566,351,55,223,168,347,340,75,719,1177,333,693,245,127,2006,158,46,53,145,400,146,132,162,1569,31,139,562,170,319,1362,1300,67,255,84,273,1568,98,176,532,328,192,22,79,261,155,113,851,195,444,91,76,102,408,1001,321,138,480,61,1187,158,342,812,400,34,677,273,483,68,534,979,436,290,402,405,396,58,233,669,1022,345,299,224,318,161,200,352,160,40,78,655,55,281,234,86,345,29,319,333,60,143,1172,146,34,80,113,73,27,127,1218,359,140,253,144,103,646,185,851,123,96,176,101,215,744,164,337,48,179,76,721,332,238,395,210,128,23,2004,35,589,183,1547,380,630,38,108,382,416,360,720,123,302,118,25,36,355,594,218,452,288,125,16,819,224,463,727,319,307,274,425,201,109,628,187,72,225,131,167,1038,244,311,592,61,139,89,145,177,28,375,577,212,549,162,62,258,97,47,323,1520,34,501,224,241,44,44,50,152,997,317,404,70,98,572,59,40,980,369,297,185,1342,368,42,10,73,503,2011,97,104,276,254,209,74,141,209,1589,20,283,688,168,773,59,79,945,57,60,323,455,245,118,318,2004,97,246,63,146,1173,197,58,472,540,1998,221,730,506,434,328,34,108,325,58,45,19,1252,72,69,205,454,76,226,101,704,150,276,52,340,629,132,833,337,1945,556,123,449,76,382,81,71,162,193,1298,192,42,709,94,686,416,279,243,123,87,60,468,279,975,63,94,689,107,223,74,118,1054,729,1376,72,899,287,76,1562,158,98,295,319,45,120,377,787,190,419,46,76,52,206,164,266,267,124,1115,290,1119,33,380,32,10,508,993,19,876,286,511,270,46,219,69,552,319,84,448,74,418,351,579,594,236,117,662,498,250,93,1134,1756,118,274,34,146,374,269,220,115,1269,408,536,491,477,308,14,156,31,48,71,84,339,56,66,95,472,95,366,900,116,63,1470,228,72,651,537,180,97,2010,323,1896,198,111,264,350,175,382,1697,15,263,148,204,75,406,254,71,964,102,336,58,288,300,52,75,89,589,279,602,468,75,185,1404,336,982,264,428,525,140,252,72,534,72,57,435,564,346,99,744,528,168,402,1062,270,68,567,192,193,63,261,150,234,128,1875,117,143,56,47,402,500,843,585,89,573,163,102,778,57,416,973,266,13,156,375,239,342,509,119,442,136,87,2001,197,770,178,56,208,94,47,503,376,80,349,124,489,80,887,132,304,49,45,1021,1049,196,448,459,90,1245,595,857,1557,108,297,202,169,486,904,1043,569,658,284,287,565,786,1532,198,171,1683,716,308,37,56,559,74,229,174,173,108,149,243,116,483,321,266,427,653,31,28,77,402,358,239,214,159,377,142,376,792,754,291,479,39,216,493,255,930,772,1092,227,64,737,59,323,139,289,461,501,1939,354,179,122,791,69,113,571,219,61,278,903,530,147,481,124,187,519,88,21,626,428,109,1272,889,377,232,542,60,613,21,297,722,142,247,517,90,259,17,412,510,322,488,896,263,202,121,678,134,155,1479,589,1342,73,682,49,125,99,290,176,411,133,202,51,1151,670,110,131,167,277,596,169,38,137,1043,46,709,288,159,543,1453,187,341,319,134,276,132,164,1048,772,239,873,230,23,1299,945,56,869,190,42,817,829,463,597,211,682,104,203,51,1254,725,138,139,92,240,145,238,841,113,458,89,120,78,456,188,59,880,589,114,426,866,47,993,392,252,289,126,188,361,228,82,455,385,245,236,149,117,237,328,1518,217,402,64,127,189,307,91,148,70,636,211,155,783,278,588,345,908,1368,1050,1620,154,19,182,308,83,32,1276,195,580,683,178,130,1362,343,387,80,325,222,473,177,522,676,359,444,856,539,345,1756,82,260,45,1047,42,267,42,264,1998,18,57,1387,72,1363,307,691,687,59,99,515,327,105,168,450,472,49,571,980,53,398,440,397,397,63,123,140,120,46,560,77,489,108,208,294,112,110,458,196,164,24,383,50,645,260,147,405,609,355,763,1643,731,1160,45,158,124,949,98,150,53,597,178,96,95,569,116,55,1455,168,482,681,115,450,534,463,128,67,124,272,151,67,199,76,386,154,267,13,94,418,52,66,281,90,1034,505,669,129,139,592,88,52,318,93,95,1297,80,905,155,82,33,253,2001,317,245,64,103,144,245,648,376,301,395,77,135,376,281,71,737,698,168,757,1278,163,357,71,371,1011,67,111,1245,176,1027,1429,110,178,413,1787,450,391,386,476,356,49,344,660,443,349,279,42,181,271,1500,135,133,82,104,70,254,326,146,228,62,48,194,990,627,692,89,31,83,124,385,146,1035,310,927,137,458,373,42,333,84,389,129,376,271,336,235,130,50,120,517,33,232,247,169,496,467,26,161,161,150,481,67,23,910,394,336,48,82,288,170,34,163,21,103,451,807,29,126,46,304,365,84,37,247,480,661,704,148,181,219,213,212,155,1473,1062,523,202,213,89,10,116,192,180,425,496,50,491,359,472,314,118,93,298,592,431,439,63,1113,299,1484,248,59,78,95,844,73,241,224,207,553,80,168,14,154,54,37,29,67,210,104,33,26,375,215,237,464,187,673,547,53,343,717,653,239,320,80,210,173,343,317,100,104,29,173,107,216,192,384,183,286,319,186,300,352,373,90,422,151,75,112,245,44,158,116,173,872,400,253,142,20,232,324,652,317,2010,1742,555,363,57,333,137,87,172,1022,158,712,424,282,89,66,100,97,13,78,319,67,893,560,351,439,13,708,58,377,339,182,73,1308,190,843,122,1849,404,437,943,225,141,109,352,148,298,577,745,297,97,33,697,275,317,1021,189,410,321,715,19,253,340,28,201,77,135,14,444,175,175,71,49,93,452,170,511,75,102,113,275,68,42,55,40,121,135,345,455,93,47,662,733,314,153,101,143,56,306,144,901,433,1009,453,110,237,620,82,141,135,2009,658,322,163,528,933,46,242,67,282,671,248,1307,155,750,109,566,254,290,202,1963,17,317,254,110,84,484,87,342,591,220,383,268,28,729,123,1039,79,417,816,1818,527,226,318,499,244,247,172,177,842,28,198,36,376,370,115,1024,83,568,603,74,56,620,131,809,104,188,143,87,43,87,468,204,736,336,23,1172,70,50,196,1196,101,18,640,185,272,77,34,514,751,178,111,216,107,1673,627,63,134,59,581,96,1364,25,101,247,566,97,52,423,381,131,241,36,374,30,119,326,692,423,267,100,719,189,577,121,43,143,198,721,234,530,2006,30,319,16,297,118,809,63,128,112,52,63,398,177,270,294,100,216,448,724,146,2006,61,95,124,1438,27,201,179,37,18,339,137,203,149,461,113,284,65,403,130,84,131,52,210,115,449,85,93,110,978,41,180,513,67,74,520,11,133,206,246,103,80,740,238,185,390,22,189,313,547,345,1900,267,135,366,146,576,144,815,808,805,104,861,892,267,111,197,233,34,450,56,369,422,115,426,294,780,58,10,100,304,492,57,188,248,186,517,118,360,100,93,96,515,166,412,176,150,159,969,310,87,415,508,620,294,259,19,94,113,245,52,768,196,664,65,282,394,35,97,258,344,209,316,95,70,95,72,465,666,299,114,116,91,167,65,192,330,1522,28,140,1265,53,2005,259,528,141,585,128,35,871,301,411,95,59,216,131,465,150,84,140,375,945,48,114,297,504,694,63,445,196,1261,52,55,1057,1543,38,1079,94,91,815,490,79,41,405,816,2001,1315,276,351,421,2000,86,209,39,693,308,284,291,122,298,279,32,991,620,208,193,175,262,270,67,149,347,306,59,690,638,144,77,379,41,279,227,652,703,245,348,109,110,767,1246,113,159,710,11,229,473,637,363,213,868,36,668,631,217,61,93,1077,381,13,362,63,541,122,1273,36,365,688,573,525,751,803,374,117,57,1182,1061,428,65,766,329,1094,47,97,1242,188,22,71,905,317,96,140,1999,39,57,787,48,215,446,186,353,97,653,187,157,283,432,760,99,170,291,299,196,316,227,1635,363,216,127,209,289,123,129,127,110,628,44,85,340,592,447,426,129,49,56,149,1076,1181,316,275,201,364,758,78,698,165,155,99,1264,28,638,180,1107,211,54,922,291,415,63,129,112,320,197,17,133,447,56,164,513,795,748,299,37,81,276,137,753,184,237,89,177,36,315,260,154,69,105,253,478,521,184,44,417,1349,282,356,1308,2008,181,152,476,553,1328,1130,399,129,91,186,241,49,437,1132,121,109,396,161,675,338,446,176,41,1173,757,2001,617,339,611,481,51,692,76,66,712,113,18,453,177,406,927,248,116,81,167,584,440,847,366,2009,177,313,254,166,378,95,245,36,92,190,646,208,237,41,83,948,581,148,13,92,1540,60,323,47,229,160,237,176,909,57,313,207,38,2005,1999,266,163,430,810,282,898,293,198,126,311,289,116,171,565,219,159,129,2007,213,280,989,135,364,72,72,468,1719,84,137,79,152,1519,312,414,81,64,144,378,944,53,1239,139,139,262,196,324,442,232,64,828,621,68,417,207,141,60,61,760,279,104,530,131,619,48,297,160,300,34,1099,24,731,166,822,75,818,24,63,126,286,356,71,93,71,681,233,2005,741,107,50,11,52,206,87,248,294,136,522,385,190,120,62,204,1286,60,503,443,33,253,153,233,431,42,313,2005,495,359,559,647,312,507,15,502,186,55,378,707,73,249,355,26,136,106,67,1187,411,1047,486,163,236,955,123,191,41,502,302,936,925,551,140,1280,717,707,27,122,325,46,237,105,34,104,678,24,92,775,85,274,237,28,572,226,720,446,69,144,527,184,169,242,119,2003,503,90,233,120,98,1135,812,698,119,304,128,97,55,48,268,181,493,160,2008,762,153,2004,105,270,217,254,243,97,169,233,229,409,203,426,158,40,445,832,156,49,102,515,1776,230,426,60,514,106,23,141,243,1027,2007,185,284,761,448,1268,61,707,1054,120,160,474,920,117,379,61,181,491,116,26,363,421,42,397,373,464,664,515,696,675,64,131,422,121,335,94,618,19,50,307,181,784,847,422,878,202,583,124,93,344,134,1201,98,1779,47,277,358,274,105,174,134,2001,36,268,501,193,77,286,225,186,811,333,345,222,285,67,171,285,1684,317,112,760,440,712,155,1799,259,708,261,1379,208,30,715,98,431,477,575,180,149,217,92,69,270,121,250,768,469,815,116,464,356,57,102,128,348,86,685,273,543,48,10,39,203,179,119,173,94,72,56,956,43,548,419,238,199,129,575,322,170,638,141,310,1070,368,530,372,179,361,55,473,440,866,341,752,99,366,150,669,751,26,681,869,523,60,285,175,106,928,99,1134,446,199,261,631,97,616,834,922,301,53,2007,198,22,211,94,80,138,172,499,68,301,796,118,70,388,160,227,286,952,72,67,222,395,249,193,443,519,595,172,55,48,126,1432,30,113,199,23,71,337,251,1049,2008,200,1605,72,108,432,237,523,198,2002,88,104,1097,350,406,433,641,425,687,340,344,84,106,46,133,51,77,357,852,246,225,314,46,282,163,464,245,1334,28,99,299,108,216,784,130,245,506,994,405,91,1364,862,716,129,247,90,646,103,95,280,47,105,263,1554,72,806,356,234,488,125,41,350,341,671,1555,207,654,81,212,34,31,49,64,285,47,60,82,365,175,17,93,763,185,159,163,104,325,958,143,383,28,119,104,971,492,56,212,128,75,129,584,347,30,215,136,61,111,182,84,186,215,576,351,421,106,334,142,271,183,65,182,90,520,240,57,477,183,343,198,421,983,157,146,225,703,172,728,45,52,164,779,316,331,91,256,102,297,94,389,130,234,60,630,235,863,18,538,1989,164,118,140,864,180,264,494,89,203,377,406,323,196,383,93,160,41,371,211,199,1470,67,218,258,264,130,187,58,173,165,437,154,62,1037,144,167,636,213,78,135,256,184,218,1354,671,163,45,90,646,257,626,1126,109,154,224,381,212,257,1813,150,156,168,1056,156,193,1096,210,86,641,188,252,90,86,459,139,1096,816,254,94,96,471,227,25,205,439,37,529,323,47,102,340,600,281,202,651,1130,1385,1402,485,328,416,593,93,152,553,284,55,238,565,140,167,129,61,720,827,859,99,217,206,285,106,133,778,1121,56,214,279,861,593,44,120,72,262,44,218,386,118,491,420,30,90,125,34,48,45,289,72,128,335,427,150,43,101,396,214,21,356,224,378,199,155,1995,371,684,111,498,433,39,225,30,107,45,222,283,18,364,140,495,699,87,115,168,48,181,404,310,240,32,59,188,255,83,546,66,70,234,13,1029,462,384,129,108,526,669,171,187,871,429,632,267,289,741,92,136,207,214,278,466,46,287,82,219,219,1651,224,758,308,519,234,295,166,153,43,99,63,182,25,186,878,1604,47,70,50,718,41,475,81,24,712,444,51,52,397,168,862,32,310,563,347,279,173,855,476,98,292,208,141,311,1031,158,118,326,253,507,78,784,172,293,338,363,298,254,147,593,601,416,66,483,276,1487,2008,696,1225,437,36,296,129,631,190,264,681,116,466,794,47,32,72,276,24,689,1999,713,68,26,1486,168,49,149,153,582,165,69,77,1170,33,440,447,96,496,143,58,110,50,21,236,90,678,674,93,258,434,140,1089,1116,872,668,312,399,360,51,391,113,209,102,81,23,39,72,248,20,13,140,144,188,599,417,277,363,510,1335,662,1516,828,32,1100,853,100,40,330,243,107,111,251,993,96,475,1674,616,1252,613,231,1323,204,911,61,77,180,73,851,496,146,332,1096,141,130,175,221,207,173,1053,327,1675,73,95,108,420,630,106,165,536,178,109,610,642,1075,226,235,60,206,187,118,201,471,955,334,82,157,92,68,37,777,511,513,52,120,63,691,63,42,697,120,90,317,240,303,783,103,122,204,259,124,160,66,125,69,110,111,114,171,199,84,47,345,553,796,29,132,425,2913,426,398,64,593,99,111,363,34,379,405,72,131,796,869,111,231,96,410,836,552,50,57,208,504,1140,504,572,132,1084,115,299,55,937,332,243,42,246,378,118,13,168,422,1100,1048,1478,110,788,92,1512,419,1442,115,88,141,674,144,147,136,201,269,129,826,64,2052,434,649,585,87,605,221,237,303,22,481,264,607,65,442,163,526,601,283,207,42,77,217,110,597,207,71,331,29,964,155,82,354,455,141,56,43,159,389,113,2003,850,608,122,909,53,945,172,153,141,182,349,614,288,139,70,13,235,526,732,244,18,233,604,139,379,91,424,787,173,510,32,50,415,737,685,48,1123,1412,189,16,139,610,699,103,135,659,1371,214,1441,72,259,93,80,251,77,663,663,64,167,122,216,175,113,78,276,75,305,239,621,1336,26,320,253,197,61,142,113,54,23,434,140,362,156,268,218,205,401,26,1102,26,517,290,39,319,1453,259,63,75,178,33,265,462,260,34,183,44,272,1088,483,472,205,138,243,69,171,230,63,32,1854,103,141,178,341,874,96,451,279,133,323,12,602,1168,103,83,197,1719,33,109,2004,46,878,75,139,172,206,251,591,542,57,132,428,1009,238,236,118,591,686,284,266,26,279,112,235,386,74,620,233,38,72,542,1523,1783,894,150,319,110,68,82,193,153,22,15,65,852,563,363,54,92,327,656,41,114,56,852,172,740,323,77,240,23,1927,585,412,243,696,310,146,1206,35,143,438,79,233,1067,47,37,158,272,552,64,2001,58,38,168,92,139,173,207,334,72,165,291,216,141,160,120,493,647,950,601,515,1511,224,141,142,377,59,546,392,1095,526,200,611,111,779,55,201,24,184,206,62,240,536,175,303,336,105,237,82,221,84,285,124,311,111,564,285,57,248,613,540,432,73,269,145,346,661,499,37,434,210,507,150,41,336,38,1540,133,1509,713,94,246,50,666,430,352,1045,2003,44,788,497,28,245,84,953,2001,43,32,81,163,92,66,875,687,293,118,357,147,265,138,2006,123,45,272,65,270,643,53,90,77,70,429,82,503,791,516,46,320,1547,253,436,47,765,172,211,171,86,90,46,33,189,114,513,172,1530,242,347,548,109,2004,750,324,1003,306,1783,156,152,2009,311,138,177,261,254,259,754,148,231,107,109,354,1003,464,397,714,220,137,90,98,665,38,338,22,228,127,23,261,292,390,522,788,667,450,303,78,705,49,539,178,513,107,398,193,30,250,110,787,97,130,59,167,433,366,326,1488,263,182,90,78,462,33,72,1741,184,532,97,435,42,481,201,125,290,532,149,86,102,51,88,17,463,449,1217,227,412,244,23,312,80,66,519,2002,282,40,1132,37,76,560,376,65,670,48,687,152,2002,51,1346,78,164,325,49,211,426,1366,2004,567,236,202,300,1026,670,75,628,515,172,94,452,242,150,121,36,315,124,459,16,437,493,275,196,174,187,85,181,393,370,167,457,618,106,208,178,292,166,798,307,1052,486,626,2446,458,31,165,400,52,595,181,75,201,64,259,152,43,51,463,387,535,76,328,49,587,110,118,485,96,16,45,111,439,270,194,46,234,189,351,71,1429,899,1556,49,349,122,87,1022,867,180,146,1104,526,69,381,1251,90,71,209,91,245,657,666,494,101,890,216,1230,401,127,1999,475,59,14,128,10,64,165,229,799,163,108,216,481,511,936,690,93,917,434,109,76,157,200,131,94,100,415,36,314,156,373,232,61,11,560,125,2004,152,792,328,222,110,728,131,263,238,38,198,234,349,101,734,2009,383,340,274,92,243,111,268,94,48,22,655,727,185,9,216,317,603,127,18,39,317,795,67,1207,133,815,252,229,67,219,120,1308,340,441,635,311,209,63,912,696,2007,245,382,139,64,319,587,636,55,85,345,52,173,100,233,476,181,574,252,326,259,945,126,255,79,46,41,284,84,303,99,249,52,542,222,1016,764,298,446,757,25,153,136,427,507,30,222,500,92,539,745,557,257,165,298,697,359,204,76,73,438,1108,78,55,159,92,347,77,2003,41,116,440,1275,193,181,118,450,127,235,33,258,821,131,366,247,808,1177,810,1723,121,78,42,334,205,384,85,1050,464,221,305,116,1144,54,441,423,139,324,182,195,1182,1541,123,809,48,633,178,278,191,54,57,380,462,111,277,470,1771,77,120,554,187,17,557,372,344,125,276,707,480,591,331,232,141,659,111,384,323,77,80,769,144,253,82,251,43,661,782,10,240,61,158,111,382,1144,1997,269,376,358,157,83,123,268,300,336,101,52,648,173,620,319,155,158,677,331,157,404,821,110,207,391,193,453,41,305,153,83,161,733,119,26,653,257,152,1447,98,757,32,155,276,281,46,323,44,97,129,149,1892,1725,306,279,111,97,82,41,1689,2011,278,56,372,69,49,314,73,765,39,834,233,1034,215,327,1141,650,239,215,1653,375,27,419,39,169,1995,1113,373,63,44,168,270,420,1931,985,1362,265,173,60,979,60,1019,167,147,1963,226,279,38,176,47,1608,515,38,754,382,164,101,654,572,93,537,153,167,65,201,363,145,344,791,616,311,26,65,129,592,106,431,430,199,296,1999,284,470,914,316,455,119,233,58,110,82,142,105,1106,112,189,34,209,88,404,172,249,14,108,357,105,1076,1004,736,354,213,215,237,16,146,103,43,125,321,78,631,537,163,50,213,675,409,400,678,395,568,231,135,1189,1363,407,132,291,358,20,156,90,237,60,175,152,119,1036,980,101,579,461,52,478,171,293,17,62,93,66,943,59,51,20,85,219,39,251,88,122,231,424,154,176,98,33,52,1405,1176,122,275,222,476,73,619,2009,572,25,101,1090,791,419,86,32,367,226,260,240,519,199,540,292,71,168,139,238,51,26,483,16,41,143,372,332,96,29,324,133,114,189,459,74,70,421,112,923,678,67,328,750,1171,166,572,416,271,172,56,212,76,714,1330,471,53,130,412,977,229,60,361,222,561,682,816,313,105,289,379,273,112,974,83,213,213,352,874,156,534,251,357,346,125,184,107,859,2004,111,1050,56,101,429,110,205,211,445,748,301,284,926,40,267,117,116,133,185,267,43,273,158,295,41,835,94,208,691,230,719,190,53,256,135,1713,69,161,726,184,272,106,140,11,260,78,391,63,533,913,772,49,39,246,208,284,290,253,145,117,157,181,27,184,1293,50,313,152,654,340,582,641,233,208,219,144,98,164,1682,1247,69,142,115,463,97,242,1638,33,559,905,205,189,108,60,149,556,183,1959,705,503,153,532,925,271,70,95,248,419,354,258,60,189,1246,105,196,54,291,69,12,37,339,182,1307,63,119,114,75,1299,59,130,106,99,191,237,985,114,575,226,32,228,343,42,358,200,54,534,136,788,114,342,1201,991,66,424,603,83,183,39,182,15,204,207,249,49,354,344,65,669,390,576,565,217,570,456,222,122,315,269,82,69,134,155,62,533,1063,369,131,252,296,209,438,115,438,293,49,490,116,1295,116,462,77,622,2005,38,25,150,389,1068,55,100,238,98,509,50,230,1114,102,351,453,389,461,417,629,56,145,518,94,116,142,258,167,359,292,96,788,765,581,914,295,521,106,99,267,131,61,382,133,181,430,174,62,8,237,49,961,205,24,41,675,40,731,932,1063,23,67,1836,328,81,162,201,454,162,245,89,235,1225,109,200,85,61,420,520,493,329,138,90,153,741,556,560,107,285,301,296,131,95,62,294,141,84,44,326,1299,128,117,1557,154,650,103,455,142,352,534,178,187,288,176,66,374,136,282,506,246,431,195,22,69,1689,508,347,197,79,373,763,188,211,669,49,581,78,1420,27,1496,80,156,72,1282,251,26,221,183,62,104,409,496,307,571,107,538,1211,564,83,50,155,78,68,455,126,491,2001,19,194,818,115,216,354,56,970,137,477,705,102,130,58,50,69,955,587,615,154,310,44,224,208,48,132,270,92,69,278,297,2005,152,385,253,257,17,106,63,413,122,122,723,185,183,237,37,359,934,49,75,152,240,184,362,111,880,420,38,762,465,651,88,127,149,1781,442,67,91,42,232,44,608,304,194,488,748,1042,111,323,117,233,74,229,79,46,976,277,203,1708,437,259,140,399,156,92,90,597,731,1403,40,502,401,22,367,194,98,21,35,146,71,830,88,108,37,405,218,105,32,874,433,2007,891,72,342,44,285,24,35,95,513,106,807,35,77,122,84,326,305,115,184,137,98,41,163,37,313,403,290,564,238,918,254,332,84,155,709,137,197,87,25,899,74,765,1374,65,735,1120,209,18,761,372,798,36,68,486,214,56,342,246,146,148,224,32,444,849,195,391,295,349,178,75,425,432,190,49,194,34,29,218,223,96,137,450,20,310,76,462,287,60,500,1996,867,947,76,131,145,50,139,70,398,1833,356,228,186,87,476,226,1202,458,51,99,1848,274,117,192,153,32,74,93,311,82,191,133,167,23,1935,66,90,161,196,659,231,88,399,585,279,90,270,833,110,634,909,804,30,183,376,347,43,66,254,248,314,279,89,331,551,76,14,49,39,112,96,66,123,475,1305,445,560,595,125,463,207,56,176,81,395,611,42,581,349,33,31,42,1461,132,123,376,212,905,46,642,120,432,214,214,155,169,145,391,1060,144,1106,27,271,229,87,843,51,665,167,258,1115,71,209,54,80,225,659,131,567,123,218,823,1424,264,688,462,289,309,86,724,893,471,227,184,519,337,33,107,405,41,105,140,190,1664,556,183,101,363,394,530,125,286,135,565,649,399,77,852,1440,134,1948,110,47,200,25,473,319,40,60,619,284,723,155,195,132,1188,362,1277,471,21,133,254,144,128,144,29,1201,96,594,75,170,46,42,142,213,524,595,105,86,115,393,233,48,757,878,438,286,372,1999,177,45,131,236,16,247,1668,1002,485,615,149,729,253,115,112,67,356,258,189,67,248,265,452,533,98,225,347,30,1309,599,100,1031,96,441,408,155,194,521,444,596,61,270,205,43,216,356,353,1278,48,30,1185,51,226,649,1056,203,20,54,554,137,160,1050,161,300,149,186,412,107,44,448,171,269,277,14,96,617,136,641,1070,355,209,47,51,270,897,362,154,576,74,326,1151,84,31,646,291,157,891,2000,374,812,1303,59,536,188,358,280,564,333,279,75,554,22,280,218,298,173,1686,157,106,1431,231,317,108,350,185,362,136,422,752,106,517,88,88,46,62,186,269,74,261,947,1071,880,506,43,2004,2000,386,16,165,177,305,1998,39,149,196,235,592,96,361,820,262,312,256,476,506,366,276,125,54,38,57,94,143,2002,712,50,374,399,497,1254,211,208,615,301,241,404,1417,256,73,216,10,560,305,151,245,89,195,64,202,80,28,134,182,2010,1124,211,2006,301,140,824,375,312,1430,120,511,514,94,616,36,613,232,54,356,296,340,455,218,179,244,356,149,155,112,549,526,82,110,322,101,602,71,47,58,41,71,27,155,205,653,210,251,116,1808,83,215,176,280,596,282,341,415,250,197,157,43,294,1372,28,485,123,35,528,640,687,233,1671,243,970,107,384,820,207,209,66,1632,284,212,661,704,458,119,132,76,221,1409,541,74,87,40,551,494,386,224,21,39,199,589,43,22,131,412,104,99,164,191,620,148,272,59,173,467,47,49,88,88,302,521,537,336,38,32,65,116,697,201,978,416,56,46,857,247,228,264,1468,251,1055,147,78,343,194,174,29,233,106,458,284,470,184,388,1998,514,472,126,601,193,79,955,1199,450,194,1450,70,393,97,344,140,1069,596,406,713,337,158,361,336,196,46,16,179,206,288,78,106,28,607,198,521,209,1776,115,35,377,120,350,97,24,128,88,141,47,125,502,384,542,279,650,1656,429,73,1539,721,88,66,299,83,199,205,143,252,859,277,412,138,92,206,87,172,977,345,1549,1068,219,435,31,1197,172,769,66,51,33,116,97,302,88,224,1099,159,413,351,274,14,106,12456,160,320,70,601,377,755,79,172,276,366,113,268,382,76,738,149,118,98,73,186,495,1104,203,372,1952,195,64,410,356,81,150,106,205,75,343,491,1171,248,414,532,485,232,115,106,541,263,240,56,147,490,82,95,976,299,237,802,227,805,74,447,359,270,123,85,390,169,73,1164,132,140,216,756,2007,441,522,127,112,106,91,887,201,251,773,322,341,708,186,26,825,54,395,325,69,334,331,141,410,110,14,1552,137,615,583,193,72,42,833,619,672,189,251,821,263,118,572,26,154,122,188,1005,110,607,1141,1369,196,782,723,41,779,1381,650,135,169,399,1119,45,199,60,880,128,1244,529,166,579,383,507,148,54,78,32,524,53,352,180,415,296,29,129,13,132,68,438,507,223,48,131,50,26,943,103,302,567,272,1027,777,209,334,313,255,348,163,216,361,469,174,379,954,33,116,90,286,181,158,274,668,138,52,193,931,414,394,11,535,40,172,25,24,108,397,848,65,64,347,32,139,493,132,181,17,181,121,56,336,996,332,412,106,322,27,131,264,95,195,1832,129,39,573,118,93,41,1533,203,329,442,129,78,41,110,48,45,330,1474,130,208,111,1008,756,127,202,41,92,848,121,299,174,543,29,324,70,107,38,1514,89,487,338,58,276,241,134,1124,222,199,274,89,225,125,182,132,123,778,813,266,217,822,354,134,681,524,181,264,112,662,721,290,328,286,774,145,349,94,99,393,52,10,40,803,989,154,161,744,187,237,193,403,235,481,192,439,33,304,1335,58,533,184,46,388,380,93,181,483,868,439,63,125,238,988,244,1750,91,1791,1001,1190,509,243,100,109,113,64,229,55,520,118,360,1344,97,891,312,324,540,26,152,1026,987,593,147,86,217,402,157,105,214,453,96,489,355,555,164,550,43,347,153,32,664,991,222,15,663,150,227,51,529,295,232,286,133,101,59,496,1158,538,433,137,577,60,265,29,1333,125,370,113,74,75,272,125,258,16,1024,640,548,88,235,111,783,224,1153,526,57,488,26,148,292,177,630,583,42,518,662,879,88,751,718,172,58,159,93,63,62,67,1322,28,34,649,1249,320,98,65,779,349,194,95,382,29,299,802,139,447,552,1093,164,269,314,590,96,214,122,36,146,281,45,84,125,69,76,82,125,242,72,70,576,494,330,904,780,695,453,2011,203,72,117,149,100,280,138,608,371,190,342,343,84,211,547,386,223,44,571,335,477,248,80,108,2004,266,40,377,1093,898,331,229,561,265,453,194,58,223,40,1292,1310,87,302,209,248,1013,200,129,270,526,135,30,817,458,673,124,73,105,116,206,94,85,127,65,175,665,643,559,304,573,140,96,219,1040,700,484,477,97,66,62,289,1901,84,426,207,584,604,155,607,104,472,373,662,119,2003,79,47,47,180,104,127,181,38,50,146,1744,1649,766,266,705,216,233,85,52,362,306,60,69,262,63,63,239,92,736,126,936,34,66,640,854,273,360,670,326,710,524,51,472,72,96,221,376,1239,373,150,250,84,1493,20,218,710,881,130,384,75,591,115,350,39,210,93,2004,383,81,70,1640,154,455,122,257,944,278,457,143,260,90,1831,298,629,159,178,184,831,122,89,209,164,127,334,107,204,959,191,132,130,404,1612,73,149,399,193,204,1596,83,282,199,35,607,422,144,84,72,1804,315,15,452,191,1445,297,101,603,144,170,575,765,180,449,130,61,250,36,283,45,505,765,1455,602,214,197,755,673,278,411,2006,128,269,453,50,45,205,196,2005,67,340,52,253,195,1244,131,50,638,13,132,1590,395,158,326,110,44,101,327,183,435,24,86,64,68,255,113,910,271,577,679,122,944,21,350,251,36,219,151,92,183,485,168,31,452,44,48,794,86,2004,96,1077,24,175,219,682,264,1486,107,2004,225,193,126,427,338,1345,856,147,13,23,148,165,925,38,327,580,744,310,265,620,14,187,352,57,960,285,117,418,253,293,122,87,347,251,171,343,28,117,45,96,455,52,79,142,161,44,53,472,71,170,633,994,567,1348,119,545,257,270,1122,317,397,166,118,131,416,485,421,400,141,96,86,161,55,309,327,248,260,322,598,220,154,149,43,38,346,231,432,341,143,40,65,99,410,406,98,702,191,743,38,168,247,1455,418,2007,292,404,449,72,144,551,49,934,77,111,202,388,464,505,356,119,639,143,909,271,35,369,446,219,974,269,114,375,1315,458,122,85,168,425,1110,151,390,137,53,107,45,353,178,46,374,270,198,453,171,165,194,213,297,784,1294,321,377,187,371,219,656,362,998,91,242,671,74,344,74,184,165,280,250,51,55,900,207,405,116,236,513,117,713,46,60,256,22,152,46,83,123,558,246,51,271,189,1235,242,67,361,29,111,1579,615,37,77,148,370,113,1085,303,96,89,1814,895,115,96,299,453,470,459,385,262,685,762,101,148,203,168,192,60,69,60,497,130,99,269,90,520,173,351,127,241,238,456,299,263,27,183,143,314,219,37,391,108,532,289,1854,1054,467,117,56,70,39,394,85,246,365,512,201,92,218,43,96,171,1967,128,131,55,263,297,399,177,651,96,174,132,366,265,78,1104,16,42,52,274,2102,211,130,78,42,498,172,77,162,185,128,310,113,33,17,347,658,41,181,33,186,674,501,235,1733,166,404,724,49,784,110,331,358,249,347,126,99,249,107,110,112,224,891,104,412,69,177,114,715,764,1388,1933,239,327,44,590,794,124,400,124,2005,2006,74,468,267,484,63,938,151,428,387,220,260,852,434,214,611,147,328,360,224,42,98,556,201,923,223,350,109,768,467,66,233,711,1252,560,118,82,166,358,1635,441,106,154,418,32,786,50,163,495,192,188,253,90,677,646,66,653,100,203,68,1768,198,279,1262,21,375,604,38,99,239,72,631,110,169,127,130,374,332,2004,80,583,399,245,62,310,89,70,629,415,113,134,104,958,109,381,152,329,117,196,1014,1424,445,196,99,126,2002,584,336,134,40,1097,635,86,162,202,229,189,111,117,319,414,1087,65,93,486,112,1181,38,60,167,301,1315,75,39,515,270,510,474,900,428,666,996,26,37,340,169,177,348,625,442,63,269,64,956,251,87,445,72,1574,1888,1687,614,123,432,131,62,117,553,713,54,246,492,994,371,164,2005,805,246,103,290,167,127,355,31,308,151,124,450,112,302,264,266,1366,114,113,525,299,1090,259,118,523,174,1222,329,196,255,313,167,883,367,80,159,746,319,204,1400,78,142,158,1147,1215,86,417,46,38,1560,945,14,41,148,312,28,356,1054,443,145,426,532,228,1818,72,264,480,1351,351,1885,142,175,1061,174,1110,155,1017,68,255,1166,140,48,180,455,164,173,45,1541,35,183,286,237,851,1085,221,38,71,114,77,64,102,100,289,143,236,64,187,86,626,709,90,66,204,665,369,69,573,630,105,137,570,123,25,574,22,186,78,713,33,507,421,2009,461,56,379,620,429,361,731,363,207,991,68,128,22,68,346,126,324,138,153,99,391,112,239,160,355,910,94,473,68,339,542,146,209,116,53,95,136,559,21,972,162,234,929,1097,141,405,519,2003,556,383,2005,83,339,479,295,64,133,1000,334,596,177,274,81,611,155,311,396,211,349,264,86,63,268,470,429,436,833,993,776,155,606,772,49,173,22,242,453,161,74,187,489,102,613,2003,732,386,683,163,133,134,628,262,157,139,504,285,68,130,383,190,345,73,142,1045,101,52,72,162,662,1054,1202,86,200,660,271,859,91,452,2005,628,112,416,490,550,168,54,309,27,390,138,121,131,677,1876,71,23,2006,89,369,122,109,1490,163,152,193,71,406,292,1204,325,459,515,565,317,125,89,958,302,44,14,373,45,237,109,109,1250,173,153,88,958,92,52,869,355,811,250,199,1377,125,866,23,69,591,170,340,39,321,859,312,188,245,37,113,1664,125,102,231,151,61,769,26,192,1342,1030,126,82,105,103,608,188,272,83,107,163,484,591,904,149,48,312,815,1636,787,1399,107,63,944,90,318,59,1515,91,246,418,34,181,96,619,241,55,219,363,281,265,857,147,806,1117,155,274,92,311,566,175,83,151,770,391,617,167,260,133,692,100,951,268,35,66,222,1041,146,34,65,56,800,141,792,206,384,565,130,570,396,435,55,64,238,581,633,700,66,117,330,110,284,499,352,937,119,419,15,16,169,248,1212,305,395,182,686,80,1564,136,697,711,314,78,1140,220,92,220,352,191,343,50,89,113,1063,2003,574,405,37,259,680,499,495,378,271,1476,83,218,61,23,52,296,2002,154,358,53,882,883,1068,141,420,77,662,78,371,120,96,198,220,173,587,255,25,292,94,684,846,332,662,945,317,63,652,20,888,244,229,441,452,314,205,444,63,591,685,611,1218,300,316,62,83,23,1038,312,53,450,233,684,101,95,460,97,54,69,499,196,196,1413,606,164,1289,1066,118,168,179,909,461,24,129,237,52,1132,306,780,73,81,93,82,89,172,70,354,645,42,71,179,43,936,235,136,29,371,917,60,127,616,316,585,269,1273,76,606,577,1320,152,214,568,518,488,215,400,904,118,113,57,54,693,278,183,416,723,106,33,1674,721,137,14,157,62,48,93,257,527,45,43,135,34,314,240,1106,163,937,295,192,10,67,222,241,314,57,526,143,1325,918,436,41,952,94,189,274,156,21,164,190,230,585,506,544,552,127,366,453,342,73,133,440,171,357,268,178,481,80,414,41,474,12,55,175,358,973,560,524,262,144,413,248,604,298,40,290,110,135,776,263,203,538,576,759,66,290,387,259,532,240,161,1085,174,381,1471,970,889,866,747,256,72,136,163,41,51,137,601,223,570,700,37,34,78,158,225,185,61,1061,286,101,158,299,21,138,184,976,957,560,1024,416,275,183,49,110,1375,234,520,65,37,534,42,1216,44,550,138,175,192,106,1133,483,125,605,520,311,282,63,333,799,95,1610,260,270,783,317,49,41,530,255,39,569,523,67,100,285,1999,100,68,124,127,824,690,1005,98,138,322,323,316,586,56,577,69,43,510,110,59,1610,1904,176,89,380,187,1425,113,1777,1678,1727,142,77,790,129,28,300,363,363,75,266,671,630,516,417,29,49,396,267,104,174,848,387,103,24,1126,1474,45,72,82,193,64,182,2006,75,93,181,148,412,160,1400,242,1752,116,158,234,303,123,153,573,422,1168,53,240,135,423,327,446,321,1036,394,214,161,43,128,94,855,287,78,123,115,149,299,132,123,164,418,34,491,307,61,63,11,142,35,261,385,43,304,724,241,15,654,163,264,491,394,418,318,247,232,953,327,16,74,2006,286,159,511,1497,1516,2001,850,284,633,125,192,146,60,29,79,267,59,292,299,283,235,362,475,568,43,694,98,357,148,36,1345,593,281,144,39,54,609,57,194,466,21,1292,1283,1376,560,19,511,1301,93,193,1051,334,163,31,205,2005,184,398,137,258,219,51,360,754,37,120,163,1333,584,441,402,24,381,62,272,260,78,369,258,453,916,310,168,26,584,122,2005,151,107,217,71,80,157,351,18,738,286,1724,64,623,436,52,484,103,120,395,57,444,99,264,52,443,405,486,34,336,464,193,170,85,263,401,22,419,576,126,678,34,97,245,968,175,79,217,170,187,177,138,179,97,256,171,337,668,405,926,217,62,79,317,136,93,435,318,255,162,142,357,225,220,183,87,332,438,213,100,56,298,71,481,320,175,48,18,33,41,282,137,1094,580,397,375,424,78,316,621,137,43,865,60,582,46,173,36,78,208,1434,1280,339,309,411,79,84,484,115,983,552,76,303,144,322,280,163,262,742,783,1566,379,765,117,503,431,1101,66,1065,1085,111,192,168,475,250,413,147,53,101,491,333,515,1308,453,218,159,94,940,182,1569,118,257,227,50,126,540,79,798,138,57,98,1496,489,477,174,65,289,246,204,832,602,47,169,1546,95,75,288,1065,96,114,197,2002,1998,91,280,557,688,271,803,380,318,569,544,160,60,777,217,305,761,88,381,184,305,769,282,460,1193,122,230,659,1153,74,414,555,48,873,408,109,134,61,86,148,18,145,173,312,103,498,315,78,136,185,254,276,747,172,61,50,76,192,59,26,653,46,994,70,693,330,967,532,682,342,88,65,629,300,174,73,399,110,361,101,298,256,305,140,30,153,38,134,1204,799,25,94,539,305,130,112,279,178,1332,44,147,316,642,645,88,64,488,222,277,888,149,183,1341,78,349,566,246,319,406,164,831,735,217,195,92,252,245,2004,120,187,416,171,1432,532,161,135,2004,577,805,111,601,319,65,258,868,386,203,206,381,97,807,211,86,410,58,215,47,1048,180,1082,32,157,167,208,681,285,34,147,16,74,34,38,716,764,1393,175,86,445,162,295,2003,361,470,46,125,622,165,1759,166,213,185,399,1260,15,1047,469,100,110,1251,77,617,637,617,556,58,242,1178,125,461,51,1287,252,1285,34,32,589,670,316,274,427,157,129,1996,89,266,36,133,153,163,110,518,573,154,345,483,621,214,235,111,516,86,130,44,179,150,49,122,710,323,316,227,129,262,109,669,504,1195,291,131,827,535,99,473,820,207,577,157,372,220,176,55,559,109,502,371,142,387,933,801,70,185,468,234,1431,60,294,117,63,545,173,239,118,487,135,120,350,404,539,422,347,50,632,24,141,33,355,86,539,81,1480,597,85,326,260,168,254,130,357,44,435,39,532,173,1160,199,622,408,53,99,107,189,366,137,178,54,149,53,302,127,451,329,32,624,162,90,119,737,408,90,484,97,13,362,1277,372,119,31,609,70,341,110,782,686,109,818,558,1063,59,261,1852,88,10,210,399,84,618,109,353,454,86,522,641,40,33,96,289,424,121,137,318,286,332,405,21,644,271,533,406,221,127,155,92,102,22,111,365,589,69,428,309,95,122,598,960,203,80,738,262,2008,469,179,324,550,650,682,126,348,455,132,235,91,68,327,875,1367,98,132,264,184,150,231,841,346,118,810,106,12,647,97,89,46,713,317,1194,389,93,17,39,622,87,47,84,110,162,360,117,779,242,679,497,168,988,95,402,808,622,367,193,118,90,101,899,681,70,66,837,127,662,75,32,127,292,629,332,363,36,270,69,121,1154,35,88,1266,532,74,709,95,222,551,120,224,105,745,83,208,240,213,267,165,452,2004,716,760,201,71,62,382,1039,976,244,473,179,467,124,522,200,1307,64,22,240,55,254,270,436,206,279,97,133,148,50,59,805,144,80,25,625,68,79,340,414,231,33,210,1237,113,170,171,293,219,293,539,274,1160,580,705,981,281,183,467,288,1018,388,444,1164,280,281,1007,774,220,187,561,194,172,214,33,362,1208,930,299,230,2431,249,216,45,136,295,416,254,79,389,101,118,450,25,318,52,2007,133,528,579,93,88,349,191,1112,408,1084,226,274,1689,372,56,661,236,156,291,164,48,435,279,1001,75,382,254,224,84,279,52,765,1064,105,140,693,447,252,281,124,224,81,148,84,1055,792,2007,330,565,401,755,743,504,75,449,93,149,327,1111,168,70,325,668,283,588,65,2006,533,351,62,46,107,213,276,272,170,489,255,143,76,1037,251,314,112,330,288,675,131,91,419,1048,1739,469,117,556,468,244,2008,233,157,49,512,173,82,227,71,44,125,238,347,382,382,347,50,366,68,174,212,159,189,550,398,652,409,498,487,56,418,77,551,804,368,26,112,369,890,19,44,538,267,55,84,716,1189,930,167,889,127,496,1671,673,394,211,1175,790,389,130,265,84,174,24,43,171,154,760,155,277,108,294,49,86,69,69,1509,411,280,174,93,73,140,169,66,3085,263,219,116,136,530,115,442,14,1119,147,2025,65,176,122,73,277,389,248,133,51,157,134,177,1180,1075,689,234,419,361,51,428,598,229,427,102,1083,263,568,116,170,195,135,332,420,685,521,265,195,308,493,357,66,104,24,35,560,211,157,803,167,304,949,211,97,161,169,1332,190,555,243,504,356,492,142,258,201,2006,699,172,86,112,120,532,531,125,316,188,573,280,487,74,119,407,13,589,765,1088,747,156,191,139,73,184,204,109,872,153,326,17,827,213,281,312,392,897,195,403,628,421,206,144,31,269,141,1172,48,214,236,246,120,480,460,428,1138,209,153,283,636,246,537,1127,467,43,267,360,631,403,269,683,382,166,1193,143,776,362,478,292,174,304,146,342,367,62,585,149,201,113,521,169,343,400,53,382,56,1375,256,292,65,289,137,518,126,294,408,101,529,276,416,73,110,62,378,50,154,117,706,858,138,562,546,236,76,531,626,169,215,1177,356,648,473,94,417,41,62,184,206,127,223,1029,847,143,177,14,312,246,410,158,334,54,474,607,65,21,1310,544,213,21,689,56,343,210,406,94,342,245,1080,57,473,155,103,608,97,371,229,46,280,69,612,276,572,110,437,200,247,133,555,134,137,182,588,67,363,43,56,422,542,74,397,884,424,837,105,766,1150,69,103,512,935,61,206,504,101,245,19,112,156,34,305,489,595,218,648,170,93,158,488,477,314,1747,213,136,69,645,589,92,508,165,582,213,720,23,58,48,55,91,197,55,893,101,410,779,533,333,172,341,1129,90,149,373,574,626,589,315,151,185,43,320,621,185,49,226,171,292,651,685,187,53,119,1086,461,38,134,415,148,36,62,353,1326,649,131,663,137,651,93,208,458,851,194,158,225,309,593,231,289,50,82,89,107,691,243,63,394,338,846,1267,870,124,586,621,450,177,53,815,540,648,183,891,143,159,493,623,181,367,99,223,152,116,611,1001,45,122,227,247,315,239,1075,62,515,402,434,753,300,257,80,115,1638,228,1034,753,488,395,59,154,971,90,92,120,490,156,79,308,128,70,179,337,709,63,355,617,223,254,178,81,28,622,510,175,578,185,925,493,158,193,939,1477,136,209,315,634,60,969,1037,749,151,232,492,45,724,399,150,598,64,35,1030,71,603,312,26,1382,222,593,1087,502,278,618,280,580,223,70,289,83,199,1204,426,60,2004,76,13,134,57,538,180,203,118,23,138,181,427,54,111,143,183,217,531,90,447,100,448,12,21,135,46,747,773,1092,610,859,696,601,1075,411,236,91,35,66,129,162,102,71,245,402,164,707,237,450,891,119,835,388,352,55,60,751,832,336,589,171,259,252,402,624,93,32,146,27,45,101,260,728,34,224,983,837,101,501,86,127,95,82,30,37,96,579,108,141,1170,371,137,83,85,511,114,969,270,344,641,488,152,292,93,178,50,1106,159,363,321,32,303,321,583,755,49,197,210,68,281,217,69,183,241,515,320,432,24,114,53,51,76,43,170,988,113,194,226,709,2008,625,43,114,293,460,459,1397,589,323,34,354,70,77,169,892,223,44,792,335,214,119,44,88,49,1003,210,260,30,665,676,334,100,60,30,50,22,466,630,604,187,126,407,451,54,151,323,1152,285,801,260,683,378,94,67,434,164,376,68,214,1050,424,104,159,329,420,119,928,659,28,35,291,308,335,1672,128,105,413,593,1498,477,118,1414,2006,149,167,535,105,76,231,156,1406,107,115,778,235,356,627,24,487,102,361,480,56,390,73,2001,347,692,969,92,70,267,915,231,110,259,12,1269,73,209,161,45,365,65,37,272,178,640,1004,808,667,454,774,379,131,30,133,406,1129,608,114,39,116,270,275,63,31,77,93,230,465,64,59,18,458,275,77,73,44,490,159,1906,592,681,380,1285,231,1971,75,476,213,26,37,641,116,630,59,494,296,515,38,834,656,1754,60,228,1265,314,647,1999,473,1140,379,1443,131,170,46,83,826,34,298,68,816,239,240,64,149,133,267,589,575,1404,892,76,97,102,36,170,261,510,60,853,56,22,173,239,1117,1083,173,243,171,93,193,614,1726,725,58,291,175,37,579,768,69,183,741,552,354,604,864,125,196,498,102,399,60,579,379,44,226,619,81,91,85,397,258,606,985,1629,81,115,745,88,27,102,105,438,280,151,642,73,501,37,31,891,221,741,232,2004,290,206,64,51,583,435,400,202,149,81,1025,725,245,104,483,749,298,542,241,263,63,179,146,476,104,926,201,190,89,78,77,135,429,182,26,832,1281,62,238,123,419,28,338,10,231,34,287,163,61,161,729,150,334,524,605,141,71,96,28,226,1192,629,1276,73,256,284,78,537,129,138,108,361,39,224,333,829,617,118,373,132,712,357,350,88,576,2010,320,234,83,18,106,369,130,87,352,124,61,188,1511,102,1100,238,158,336,343,30,73,894,208,321,518,443,159,507,353,106,39,808,145,90,215,66,1542,769,354,223,209,144,211,535,1065,216,34,161,109,146,44,148,415,90,714,181,405,101,1740,280,489,47,78,328,37,248,253,165,32,36,686,494,19,410,339,939,211,56,1646,164,516,75,604,864,1999,108,496,238,36,127,201,103,183,566,698,80,883,1193,375,1146,110,340,216,2003,143,102,640,1021,273,105,302,26,201,349,190,170,274,2002,73,286,146,121,61,241,375,459,19,2510,522,46,452,428,512,59,81,232,1389,1030,687,987,532,143,56,313,269,209,129,333,343,201,328,308,153,132,961,118,504,59,464,1136,509,1103,1437,138,270,44,215,234,690,37,56,214,259,319,1164,2005,274,425,635,97,244,47,56,445,408,93,180,51,308,990,32,1481,445,590,114,945,549,1316,65,525,214,1941,97,501,703,68,198,61,251,223,304,629,289,91,513,334,58,380,236,135,76,2005,116,718,210,64,178,196,502,1182,229,181,54,530,937,737,311,36,55,340,369,367,249,56,150,770,586,375,75,210,479,78,217,235,137,605,740,124,125,493,102,179,363,541,301,157,77,55,162,210,404,27,46,49,212,90,119,115,346,303,432,129,59,194,121,131,354,199,188,846,44,510,132,246,537,304,741,71,214,386,554,368,479,217,143,828,595,386,229,314,260,48,434,237,78,79,323,465,61,64,56,196,78,116,316,883,86,318,1013,2008,114,1001,597,429,313,201,960,478,479,645,493,270,186,288,216,1104,174,589,372,71,133,69,202,315,92,520,1712,193,1603,56,115,1711,234,117,90,34,121,51,74,389,426,158,39,234,361,508,58,141,139,446,57,146,23,168,640,23,46,149,376,358,911,101,399,223,700,355,194,766,24,169,153,25,145,125,141,147,1001,296,32,195,283,229,423,765,70,231,144,111,83,249,85,13,38,613,762,326,1106,38,722,379,180,73,149,252,510,1017,239,343,116,55,240,318,673,257,483,367,409,119,1622,971,155,381,429,81,26,549,49,254,324,664,83,712,125,23,152,807,176,105,123,15,307,423,317,145,71,649,691,436,1158,190,167,241,166,904,409,1759,956,783,492,378,98,65,229,82,559,682,271,1606,487,115,79,560,13,152,681,167,113,105,273,745,88,159,110,171,37,383,1275,31,255,59,179,121,9,54,438,60,305,219,370,122,348,347,472,354,688,165,75,304,48,103,44,164,274,673,61,331,49,220,347,532,103,21,125,465,1240,88,113,201,358,20,31,446,99,184,129,333,109,669,57,279,291,158,127,63,717,251,94,58,46,54,106,74,25,757,66,252,55,305,49,93,88,81,470,198,90,175,226,700,580,487,1247,488,445,381,623,155,292,924,152,271,334,124,288,1490,812,335,11,576,773,334,359,318,368,362,1444,244,148,245,45,725,631,68,64,216,269,1223,393,292,858,663,1496,273,82,761,210,365,567,152,728,182,179,664,145,390,118,684,1106,613,292,828,269,56,233,849,805,72,118,945,35,58,351,109,70,90,70,263,249,96,135,274,49,15,461,385,196,816,54,219,582,46,380,179,79,248,157,39,61,104,2007,1580,416,318,168,136,1133,1684,755,756,230,215,1068,1109,205,138,86,191,259,346,478,73,934,1485,93,50,183,839,174,73,802,263,162,147,303,831,677,51,130,1197,79,75,281,28,78,219,297,132,384,405,430,194,36,194,2363,603,529,343,44,79,399,98,317,25,125,216,136,873,251,187,605,261,493,859,302,328,91,308,798,234,46,187,771,27,1723,1358,674,199,68,12,459,311,797,38,257,283,290,164,86,758,162,151,261,107,545,51,895,41,221,64,823,66,400,66,178,499,2004,43,35,34,908,573,1885,140,344,199,80,267,212,319,1099,36,252,614,198,157,141,425,60,286,64,57,2011,34,31,420,109,650,186,212,119,148,35,603,1455,78,437,74,261,1433,202,837,455,177,79,125,2006,96,30,25,893,589,351,140,1122,294,261,168,50,245,128,335,190,323,104,414,109,145,542,35,325,358,245,98,218,125,30,53,148,618,32,147,698,435,102,70,29,156,558,45,604,210,377,464,871,102,97,541,546,292,1105,88,241,145,674,423,265,691,1013,16,328,151,52,94,74,1174,229,223,74,241,139,229,90,238,376,414,90,793,176,59,55,2008,107,229,327,125,98,31,391,327,113,893,186,66,126,138,158,343,148,326,43,94,468,137,170,473,221,269,15,365,197,135,123,193,1035,1998,811,571,126,91,375,527,331,438,248,99,64,1646,26,418,123,918,510,449,78,1446,444,137,229,1593,88,268,170,172,547,906,153,139,79,87,903,66,185,788,300,269,75,800,375,786,418,593,60,97,23,111,591,160,48,80,495,1178,16,396,131,249,170,262,156,403,175,348,78,596,269,488,308,205,66,407,87,481,107,77,401,141,119,125,173,98,116,103,102,66,102,23,66,324,203,1264,371,123,287,1122,279,286,351,191,85,450,52,308,456,58,246,100,430,519,344,1215,507,377,141,472,125,537,418,240,60,145,530,118,1635,278,1782,207,378,210,291,242,212,140,157,421,57,208,89,248,1069,85,48,44,105,523,37,21,170,150,336,272,168,1656,120,548,597,310,11,212,147,327,40,54,166,397,169,1516,232,97,519,728,115,526,225,882,75,22,460,219,549,118,527,264,165,1223,70,125,324,97,86,407,553,407,109,817,165,102,203,1278,302,1014,493,25,140,143,662,495,1104,99,52,1928,357,63,15,246,88,166,1383,34,196,102,301,122,241,52,93,246,184,46,102,159,1260,257,507,2709,458,28,555,414,277,108,166,741,38,181,326,230,82,1976,95,1241,299,205,95,295,133,157,287,1237,224,907,404,127,51,47,162,20,103,299,883,134,161,142,113,98,71,362,149,473,193,760,407,123,299,867,536,213,1391,534,469,815,230,379,1933,490,319,246,336,94,449,300,55,154,400,100,304,1366,172,77,1805,171,370,272,266,780,169,872,141,693,341,443,118,453,136,274,86,407,851,1377,48,109,23,422,235,111,341,60,258,1996,156,281,188,173,760,48,721,130,88,178,197,903,145,304,77,219,18,1539,31,362,624,116,280,1511,62,624,225,328,252,163,44,200,689,435,81,925,12,103,36,97,34,121,1040,557,124,311,405,802,161,53,546,1299,804,245,310,62,186,862,1055,47,322,313,347,1202,897,303,132,45,266,444,1428,158,615,92,88,204,522,61,433,13,1263,1065,27,325,44,88,92,51,284,2000,115,297,102,70,121,299,53,57,77,348,1037,905,1263,187,242,40,69,45,183,756,93,736,27,100,219,82,397,200,248,76,72,690,84,1225,53,538,290,587,513,1768,1042,1701,101,231,458,466,40,636,389,885,1288,200,127,84,730,383,697,91,697,801,410,12,60,120,461,404,483,177,75,617,683,157,379,31,394,61,53,1032,43,656,988,559,976,162,910,55,129,41,142,704,239,279,182,210,225,624,51,390,662,144,98,600,835,329,451,18,568,60,551,42,69,119,45,69,1802,49,1519,539,1115,73,578,409,104,66,61,368,651,360,460,74,198,843,445,897,75,44,336,108,809,2001,1092,81,43,161,127,123,316,275,621,161,125,1674,200,104,77,649,188,57,127,271,482,463,273,349,96,697,127,249,51,151,226,306,534,97,199,61,618,113,124,70,2003,254,519,747,322,201,48,469,70,76,253,47,349,52,673,540,180,634,468,1528,370,392,42,137,339,216,243,71,164,121,93,1141,150,352,142,338,106,169,154,292,190,81,508,81,97,106,289,1207,553,433,209,373,622,457,119,579,376,904,613,1019,153,51,139,996,206,63,209,440,53,111,1047,232,782,107,21,1374,104,267,50,165,84,131,462,453,428,976,1492,561,42,187,167,762,32,476,171,253,14,415,1632,830,105,1094,43,497,64,782,44,166,93,896,95,107,1706,183,115,291,220,589,382,160,700,104,514,517,87,159,131,139,175,278,169,54,528,428,1104,62,316,1054,642,105,200,548,238,420,733,183,57,47,175,583,327,968,310,416,236,1106,295,171,75,289,631,665,299,190,285,54,25,206,200,559,296,66,920,174,260,360,73,71,299,324,825,371,55,469,695,271,200,41,81,917,68,400,587,336,945,267,140,192,407,142,539,1710,194,374,293,129,157,252,403,32,43,101,330,48,89,253,34,161,257,66,55,321,463,383,26,98,81,437,169,228,108,548,120,650,601,257,121,963,547,106,1035,958,118,60,93,372,633,576,194,344,57,273,1680,1110,204,26,134,689,315,318,173,780,37,193,147,295,414,163,249,127,64,234,757,846,106,114,309,330,729,86,171,145,133,972,95,586,176,484,794,149,380,931,81,609,261,1996,338,58,314,229,259,109,308,449,282,287,2004,479,49,505,463,106,214,507,176,1332,87,184,30,289,56,135,431,237,580,363,97,272,517,177,212,69,148,124,138,199,201,97,326,769,479,46,308,396,46,291,233,83,12,12,278,132,450,317,38,174,274,25,616,740,150,47,587,905,292,1389,151,558,167,73,177,378,262,1034,52,286,188,185,836,430,35,259,78,211,342,757,517,47,146,101,63,942,25,1210,184,153,701,93,138,410,213,137,619,70,715,117,156,1825,168,280,166,328,357,376,135,72,217,698,54,233,229,382,96,90,204,78,264,120,386,217,230,1316,456,198,47,324,630,1136,615,107,239,740,1195,88,118,83,30,1390,92,158,88,117,44,220,213,166,387,48,124,195,1575,90,639,300,1593,1293,342,490,655,487,52,226,89,1501,823,275,118,464,196,359,307,314,579,476,524,80,330,40,188,72,492,164,207,492,172,394,386,356,142,63,140,354,33,370,17,961,554,73,138,1568,62,242,94,65,219,78,909,124,594,74,293,353,560,23,1854,385,254,881,323,20,728,73,171,149,162,75,61,694,54,116,50,82,319,387,65,1065,81,186,195,902,40,67,64,361,369,295,258,290,50,344,406,726,170,234,279,420,526,734,81,1098,135,87,145,442,895,475,42,24,215,301,181,170,83,116,293,118,324,33,279,205,65,593,632,1183,202,254,226,202,210,1005,19,262,855,210,156,289,274,607,208,120,1371,32,205,1002,375,8,127,284,103,259,212,82,30,19,1078,158,470,277,1296,106,162,930,468,267,142,550,544,60,911,196,112,505,326,73,191,198,70,440,307,123,94,618,53,481,155,478,343,251,182,450,27,137,255,272,270,211,139,349,744,77,106,587,403,25,161,1155,70,293,86,604,1017,54,232,66,196,54,185,77,75,310,467,279,18,74,338,24,199,202,516,282,90,98,356,169,133,776,242,427,958,249,91,118,37,763,2002,1631,930,153,97,221,572,56,386,324,188,666,124,349,151,107,355,592,380,125,146,209,428,173,109,740,307,125,204,634,196,230,211,1832,216,47,612,641,528,71,876,107,115,140,443,137,998,893,241,708,84,379,582,108,923,507,126,967,51,159,312,38,397,471,159,428,457,1841,466,131,192,125,24,74,135,661,151,55,117,957,227,104,101,217,258,416,287,404,290,153,246,510,130,803,461,241,460,268,350,206,45,295,426,169,435,164,166,1740,546,251,123,44,126,426,131,201,162,211,44,347,870,167,265,1400,622,41,251,113,476,214,421,266,394,856,121,256,252,294,712,560,238,274,115,373,178,287,544,147,68,224,396,548,90,629,599,136,268,1022,225,796,191,61,71,54,151,416,82,98,232,566,74,307,577,201,22,336,1633,373,483,603,266,94,36,1057,66,44,547,1216,129,118,20,909,164,237,293,387,153,407,131,909,695,83,158,312,26,1648,95,142,1997,121,215,1127,1104,431,341,80,91,135,284,27,353,61,136,272,249,225,170,930,233,92,137,68,202,152,110,596,741,192,67,367,507,278,923,658,247,486,130,333,705,776,467,579,652,166,139,313,90,569,387,257,304,130,596,41,122,316,1136,572,365,240,125,1104,119,658,526,264,497,216,100,194,152,27,1515,314,183,424,619,502,556,190,250,858,206,1447,76,1940,252,29,291,83,201,86,152,385,78,643,436,822,176,372,46,92,109,197,454,590,629,432,563,462,225,43,244,71,11,355,89,491,361,1132,151,420,324,120,341,570,277,24,99,35,1131,628,60,483,76,101,513,224,2001,112,641,93,288,22,44,225,258,332,274,603,47,430,175,141,105,330,104,28,724,112,153,367,42,150,138,148,126,95,38,203,28,99,142,39,119,171,77,72,590,2008,47,640,70,90,333,602,1135,137,164,137,1129,142,491,99,245,40,278,489,54,374,203,1388,211,76,459,51,660,591,33,144,156,433,326,477,920,366,124,132,282,159,280,104,288,684,575,142,879,424,782,240,22,446,1020,89,105,348,305,1513,157,1096,39,22,948,284,141,368,399,144,99,648,319,1852,180,369,921,94,235,445,85,177,135,1264,172,23,144,1856,119,292,131,781,1157,831,87,171,720,132,441,34,399,75,33,615,659,62,137,398,88,412,395,337,828,498,391,347,1573,249,232,532,605,514,152,150,394,506,917,162,218,1052,42,179,486,91,550,57,343,71,54,154,173,163,220,183,157,358,216,87,116,774,555,559,985,558,221,1127,330,853,25,37,802,182,180,505,68,217,126,26,157,2090,481,47,34,107,511,800,200,757,117,116,41,138,1654,116,334,188,384,184,99,22,138,66,116,316,215,683,47,426,1259,96,648,223,551,349,96,180,389,1001,309,51,111,122,212,44,311,187,102,18,136,574,93,44,590,116,19,287,319,154,182,194,286,399,185,793,451,456,154,193,431,153,770,151,181,1393,2001,169,212,221,282,125,251,125,48,245,1019,22,349,129,45,529,677,309,317,294,300,55,162,22,178,587,230,160,322,88,218,305,61,159,519,325,451,257,459,631,341,226,250,124,164,461,426,277,565,452,396,340,800,170,584,204,253,241,174,91,148,41,2004,340,114,297,73,132,278,418,668,56,145,104,639,64,383,178,214,80,85,225,82,121,180,126,495,269,68,87,84,80,336,130,256,1759,273,88,180,517,1092,250,70,120,331,361,248,326,408,463,95,500,136,345,99,586,65,569,214,750,120,305,261,133,126,207,105,423,779,396,68,632,792,395,91,455,1311,62,221,1261,39,131,117,185,66,160,684,127,535,162,81,106,690,317,62,227,420,290,394,776,375,778,202,95,92,347,1562,105,614,112,172,309,794,371,206,112,536,70,146,288,902,87,87,532,1581,141,34,735,97,32,110,248,176,63,488,360,1918,116,1432,620,140,246,884,751,789,182,117,681,44,43,689,71,357,437,849,120,44,2001,628,78,222,1727,162,1805,322,652,32,505,437,232,1667,551,72,120,98,503,1044,441,341,132,118,91,688,621,71,71,212,596,49,376,287,61,913,347,270,448,518,132,1033,341,242,418,94,314,40,310,191,962,90,116,105,207,450,38,2010,40,95,54,177,352,10,136,154,120,343,112,176,227,120,382,662,39,276,938,66,58,28,138,232,323,1127,424,226,645,213,173,112,216,970,177,939,22,157,59,516,669,173,1787,222,368,257,176,944,97,1237,184,75,852,52,465,162,105,83,193,64,251,708,144,69,429,77,292,1720,459,138,552,326,788,181,453,376,137,73,181,1486,160,466,251,900,26,539,213,95,582,125,75,147,841,43,790,973,49,59,46,766,34,551,144,255,1766,249,55,152,606,237,45,248,290,139,192,84,87,431,171,43,22,233,231,256,138,151,142,306,1010,10,115,117,450,1156,54,266,79,613,2003,356,64,1302,199,35,105,40,530,661,136,101,790,67,63,263,174,80,314,83,68,273,415,165,136,87,170,91,49,230,473,379,9,283,13,443,135,24,62,35,138,218,18,317,152,227,84,83,186,55,139,69,27,415,942,414,329,474,108,400,56,113,750,181,485,300,120,435,783,158,1999,2005,132,65,202,323,883,159,78,214,2004,63,126,23,357,571,706,578,445,279,1804,67,491,75,177,1087,87,347,849,309,124,248,21,195,182,51,122,382,340,178,541,644,210,2004,254,687,409,635,563,80,488,340,43,600,600,228,700,252,177,270,114,544,225,141,207,187,170,109,201,127,139,1024,510,274,243,129,211,2000,177,682,1467,985,65,60,232,333,168,332,191,68,581,86,528,125,362,1314,336,152,214,61,284,415,210,44,5936,454,53,746,371,145,20,591,100,340,96,1046,108,270,95,71,607,272,205,471,107,1842,196,508,624,702,76,85,126,81,262,546,2002,126,657,130,386,241,30,517,334,21,139,156,111,39,323,1010,303,316,23,418,604,71,204,465,631,178,1488,272,73,228,432,406,323,596,92,154,155,164,1071,1320,327,605,304,26,451,366,70,491,126,204,267,82,12,401,103,227,461,217,51,310,160,42,213,64,156,132,164,150,265,260,676,279,359,459,168,1289,298,691,263,2002,466,973,1013,164,1010,164,177,167,113,66,1331,264,597,231,201,1257,42,126,223,388,1092,486,1016,29,384,1273,536,57,64,445,1201,354,1330,30,33,788,101,136,320,975,105,976,150,307,169,123,683,181,34,62,131,72,857,87,412,75,390,2632,47,355,106,102,167,478,146,94,198,1397,1031,562,559,61,183,43,59,180,1100,652,344,217,1097,100,100,1443,145,280,339,1430,904,170,832,551,88,476,44,32,165,17,207,190,482,347,1084,152,48,187,52,31,96,168,779,630,482,49,168,252,1535,624,186,1031,108,1196,41,255,64,837,676,97,20,227,132,425,620,162,796,365,88,758,231,100,277,138,196,611,418,180,511,143,169,90,604,888,173,1029,136,452,216,169,200,48,186,88,209,1335,113,182,769,289,71,895,510,932,107,84,176,353,54,65,1957,11,606,102,36,1224,533,454,409,35,1023,25,113,796,185,185,90,226,60,105,779,370,275,145,905,655,14,546,134,382,645,345,710,31,568,19,711,1116,1571,175,125,85,2003,476,185,119,401,94,83,42,285,283,538,187,1323,64,663,313,21,115,207,86,214,22,82,219,498,237,262,1278,146,86,651,783,700,1079,513,150,45,544,1266,331,1115,678,287,199,844,33,296,241,171,991,655,311,802,360,567,124,188,111,53,229,1785,165,171,133,219,612,256,166,187,652,1604,891,154,400,81,820,181,249,320,753,590,2005,222,374,219,826,355,129,221,87,25,134,121,100,433,310,148,783,1001,67,130,172,83,174,52,145,163,90,443,172,328,536,32,109,53,26,291,156,288,26,897,393,78,328,245,182,50,273,80,394,196,69,211,427,873,32,1995,35,266,304,279,221,216,1511,87,718,787,167,97,187,1192,379,204,177,366,176,412,411,282,488,191,587,27,134,343,61,1303,183,478,586,75,231,848,166,235,384,218,88,72,1212,776,102,32,57,255,29,38,91,555,218,221,242,145,92,690,255,337,311,154,1085,48,334,334,243,296,61,288,72,284,96,127,314,433,104,143,221,199,495,225,82,295,711,522,142,580,426,380,357,1404,136,76,207,923,335,18,313,2005,48,90,780,1770,137,34,350,258,191,57,101,117,387,28,94,103,210,199,126,64,73,284,1890,290,47,234,112,1125,79,178,338,203,137,303,660,352,487,96,554,369,117,459,260,1098,859,74,23,107,64,458,356,713,323,65,121,80,497,261,113,349,169,1524,704,73,360,163,346,829,56,284,87,32,78,154,281,135,137,295,277,195,182,152,1420,387,170,1072,142,553,326,121,105,429,151,92,932,72,15,424,663,319,704,262,1085,645,88,473,28,102,947,559,1435,156,36,339,310,306,707,24,210,61,88,426,381,155,86,40,1802,447,292,66,330,180,393,126,81,276,868,58,166,45,202,97,578,34,50,267,32,519,963,294,215,631,55,35,558,386,1019,766,43,131,161,69,202,101,339,100,91,994,198,346,61,295,373,454,146,197,80,67,136,178,261,168,135,284,25,118,202,241,288,116,315,115,567,304,131,262,444,165,101,96,74,75,1037,182,1216,101,68,91,30,49,14,591,35,723,105,552,1350,130,88,209,46,1684,222,210,1978,429,167,930,1436,85,223,1106,948,97,617,75,29,243,197,116,582,310,932,139,87,68,361,137,279,512,380,47,286,1033,921,26,282,174,671,2009,2005,1749,386,88,769,160,501,140,48,206,238,105,65,85,48,65,423,652,104,906,70,55,1294,76,123,369,212,173,86,185,933,382,92,482,47,25,280,99,199,112,551,184,61,138,980,128,437,431,204,20,78,266,330,141,66,413,415,285,103,230,981,794,101,38,300,942,50,19,457,266,300,272,60,70,206,65,363,99,17,186,523,77,84,32,47,411,715,450,359,377,283,591,592,763,396,129,1294,157,51,1445,491,217,59,131,239,251,767,176,823,593,1288,259,55,663,561,199,110,1332,671,240,441,320,156,969,44,1230,33,1229,186,456,127,305,495,1165,78,719,157,192,78,155,725,134,350,180,158,43,228,621,62,29,173,69,409,966,343,25,125,115,119,178,545,97,362,182,664,198,202,476,573,470,62,108,222,69,746,51,69,330,1098,107,240,347,258,307,1359,51,208,66,159,280,702,350,389,916,272,85,367,871,182,68,598,2004,891,34,93,110,96,279,2002,70,72,59,429,304,53,269,86,67,112,23,1285,301,389,739,126,409,62,198,1556,260,537,390,1295,401,175,79,83,205,209,70,212,108,157,196,103,315,1718,225,603,52,135,222,365,240,454,553,71,1018,36,414,259,184,592,1066,264,1081,29,155,276,108,161,88,247,467,1118,320,951,427,68,855,312,324,632,90,88,343,498,220,244,87,584,746,524,534,99,474,1014,654,1189,555,102,660,1841,240,740,1023,412,292,116,604,122,438,185,182,468,771,175,97,531,139,38,1161,313,545,695,51,833,135,383,1214,228,116,471,198,86,32,181,1189,341,730,225,554,206,84,105,345,705,275,55,268,164,1428,226,130,90,115,602,188,183,181,81,41,595,95,25,635,443,75,59,179,72,322,389,1109,365,249,65,240,34,33,120,151,64,33,570,460,112,283,308,203,146,440,256,347,86,273,157,850,91,15,736,53,51,599,66,803,91,138,224,44,492,630,509,1400,1510,344,1189,667,501,68,244,130,181,69,195,70,250,177,202,397,515,126,149,444,766,1459,1794,119,201,789,304,111,776,349,231,596,145,51,56,450,369,59,26,67,482,67,765,537,771,191,764,955,232,1056,901,751,1491,209,334,181,338,57,428,46,457,376,72,2007,48,301,375,141,164,240,710,95,1149,559,141,1769,373,124,29,175,231,70,740,36,2002,171,398,805,412,102,732,260,70,261,444,96,746,196,372,684,272,456,145,80,330,280,217,75,107,232,321,545,1158,337,765,269,659,1635,862,370,1081,583,210,114,335,144,482,104,1095,241,265,425,146,43,1000,1052,330,674,301,329,83,120,52,254,89,73,898,118,386,65,494,368,141,634,884,470,158,181,104,170,136,35,200,224,203,218,356,1152,290,33,23,73,479,570,344,63,81,99,843,231,57,1001,531,1268,55,32,204,245,242,2004,218,503,466,280,1999,308,1417,124,89,291,227,656,238,266,300,363,256,52,869,1351,30,727,105,128,191,262,135,1355,176,75,371,101,583,597,289,610,2001,255,325,196,107,493,176,619,213,184,455,268,754,241,114,902,387,873,220,233,136,283,61,442,308,184,256,24,255,42,121,33,52,73,751,163,624,474,60,300,259,288,120,728,1229,107,524,111,631,926,152,746,92,234,175,1543,589,129,1208,654,1511,847,12,259,823,297,312,524,108,231,547,59,175,182,644,649,80,146,402,2011,134,826,238,78,452,174,1005,2006,79,328,97,112,73,276,54,466,81,714,650,358,130,82,25,731,298,326,653,407,128,1534,212,314,64,492,1929,108,66,35,611,480,28,531,192,298,142,226,870,151,271,132,374,1868,226,1497,581,51,495,998,62,797,1117,40,196,192,182,69,434,290,570,1409,759,299,360,118,230,80,972,123,75,522,237,191,119,170,107,662,230,393,58,110,177,130,692,452,1451,89,577,419,83,17,2294,1502,70,127,35,164,86,238,334,159,771,444,161,890,2001,176,95,145,47,173,187,47,387,90,456,91,135,216,515,99,120,59,762,86,387,133,110,269,153,2005,29,286,819,99,158,201,35,85,799,140,385,59,684,331,399,266,87,583,178,181,188,86,270,80,175,447,235,463,343,29,207,694,385,31,382,263,166,228,516,460,882,442,421,282,505,48,132,120,396,26,108,129,285,510,1307,319,103,44,315,340,268,524,331,337,324,354,294,680,435,591,662,43,412,1296,180,1288,82,131,225,185,260,299,43,610,183,367,66,477,52,234,3806,494,113,266,1712,162,90,184,735,111,68,293,999,77,329,212,291,106,1095,1051,492,90,536,34,46,96,366,268,65,147,569,135,74,440,273,110,12,2004,130,382,101,515,105,714,15,1144,323,54,63,861,774,116,333,752,103,55,311,109,349,491,431,271,200,108,35,100,335,175,152,230,23,345,109,416,48,97,119,69,186,265,29,503,230,28,1000,97,800,90,279,265,1093,218,370,300,244,82,123,108,142,216,110,144,130,67,214,269,131,82,719,83,211,51,48,92,66,91,220,74,740,26,424,221,306,53,216,406,80,54,354,44,312,143,414,318,1009,382,601,243,717,218,155,154,558,473,292,977,226,336,583,126,35,1256,74,144,65,162,43,516,1326,150,24,62,1004,72,80,239,152,133,152,85,535,434,294,1032,147,126,95,1486,352,321,330,49,64,313,151,198,1151,281,512,67,430,32,72,251,309,44,243,17,60,1163,167,506,21,174,2008,82,106,529,208,693,514,56,97,862,1352,180,1237,135,218,729,230,44,171,391,702,495,151,278,351,47,126,273,1950,285,50,99,203,134,271,160,47,209,338,486,173,509,721,498,100,341,54,32,1310,1144,813,44,741,268,499,160,246,963,286,256,1815,104,174,38,498,109,24,156,392,327,28,86,811,471,34,351,108,208,496,59,122,154,359,185,34,298,239,139,1144,94,874,213,18,272,183,200,541,74,806,736,36,43,122,18,38,19,109,188,397,707,151,66,523,60,385,492,702,331,664,1522,551,258,83,90,275,105,527,149,418,224,944,162,1496,71,654,439,149,460,206,119,80,423,924,578,94,123,62,367,50,104,431,506,77,83,1102,959,150,84,755,557,201,52,245,95,750,443,88,197,69,124,207,112,146,477,341,563,280,131,1281,108,1647,200,115,271,17,705,28,42,712,16,74,309,55,946,1529,591,576,27,215,437,72,293,111,493,180,418,99,564,76,281,53,537,747,130,155,837,125,194,374,722,723,170,262,140,398,160,1008,56,239,81,210,429,984,123,1461,126,317,275,43,352,2008,308,420,295,766,208,430,86,205,1973,555,118,29,55,101,221,168,407,144,107,196,1353,94,134,52,314,530,155,262,322,266,66,472,215,337,92,154,52,453,751,47,88,338,591,198,1645,1334,456,41,213,109,421,759,75,537,189,122,445,35,251,378,92,367,36,232,1316,2002,974,142,892,1013,1162,502,2001,86,769,378,412,628,395,316,403,513,1160,178,188,63,476,268,130,165,1343,158,82,468,549,658,385,104,98,449,210,259,143,917,833,71,30,70,238,133,677,32,1012,875,555,81,394,191,1225,632,60,1341,863,300,276,109,236,806,130,235,87,43,259,117,64,195,114,109,282,127,662,304,378,28,137,728,26,289,243,147,410,1342,46,147,420,161,139,175,868,933,137,76,767,216,105,372,371,936,926,680,121,249,227,508,203,134,52,86,78,780,670,284,147,699,806,646,209,915,221,107,152,93,397,74,155,496,368,78,125,167,212,451,174,559,57,78,1018,576,261,614,123,917,177,112,114,67,1318,115,545,45,909,301,219,31,332,1024,783,307,265,360,319,200,29,111,2004,239,70,83,324,65,162,120,841,52,542,477,610,80,912,120,232,72,33,316,280,535,1117,684,13,67,530,329,106,216,174,576,103,116,626,125,152,42,193,148,170,163,317,20,545,307,150,163,295,51,135,328,1139,295,290,373,536,50,175,322,23,80,534,1164,527,166,331,81,384,79,111,89,1583,78,672,55,28,298,531,623,139,87,157,487,96,248,52,163,463,899,71,133,595,313,718,118,63,497,127,60,383,344,476,911,65,252,421,222,183,607,791,131,93,560,702,122,738,502,188,31,361,467,465,471,275,497,1109,323,69,89,383,845,231,110,178,267,164,145,195,403,164,105,421,99,73,561,163,187,363,97,535,129,175,146,45,524,543,358,500,271,124,58,159,151,100,1617,57,14,691,122,68,188,134,105,219,1176,167,391,435,80,1298,31,45,1442,2008,182,355,117,294,89,209,422,21,180,63,72,397,270,10,792,1429,595,448,84,101,774,170,249,60,329,30,333,254,134,48,447,52,92,153,198,57,217,168,134,1056,534,305,26,536,1224,444,148,193,18,85,14,184,31,1726,80,875,336,106,179,41,469,280,135,57,59,785,176,336,78,96,102,130,47,102,28,154,945,139,479,52,711,47,79,278,247,131,261,136,70,102,290,363,211,17,217,661,77,791,133,143,964,287,66,119,32,105,472,349,72,131,1388,67,32,134,288,237,146,472,104,64,467,1134,525,451,100,127,483,204,365,136,1877,376,59,2006,440,175,271,144,271,248,1504,139,443,326,112,25,39,115,692,208,448,431,23,158,504,472,980,13,605,134,234,397,746,182,69,163,304,355,30,1235,1998,99,147,62,76,122,872,365,134,1396,48,254,110,70,1584,30,819,281,76,23,104,770,570,325,890,115,196,619,268,462,60,195,555,96,120,2002,46,1084,178,81,53,123,94,44,928,15,539,593,1640,515,18,533,214,154,181,1489,412,60,162,234,105,663,2005,86,889,86,970,287,131,85,1007,28,273,403,109,82,1172,1463,580,158,906,818,35,27,77,34,47,107,25,157,286,444,368,79,249,698,120,244,357,94,123,48,451,41,81,189,221,83,952,185,1581,103,84,453,256,41,909,745,240,124,58,136,29,116,541,131,96,161,24,263,623,81,167,201,255,56,647,261,78,1037,294,782,177,392,465,353,16,251,121,189,155,159,394,131,147,642,564,1168,198,71,412,674,28,1234,311,137,202,593,551,265,171,471,68,52,1391,223,235,1488,609,87,1438,347,42,216,227,302,52,214,717,404,88,55,130,397,206,57,63,804,380,204,545,752,157,590,933,117,126,696,137,912,845,391,415,1220,82,180,653,807,646,87,624,53,1287,819,367,67,217,247,594,898,255,177,654,208,605,343,455,521,73,1089,1463,857,83,234,15,227,2004,967,233,1486,118,197,66,171,303,76,95,452,383,114,41,15,394,99,72,74,190,490,714,72,380,1079,1123,281,340,308,454,253,83,23,244,359,151,122,38,148,90,228,485,80,1226,452,147,81,574,114,353,186,674,107,419,920,319,320,601,176,270,1557,1031,228,685,532,94,1002,821,171,84,435,77,238,165,189,26,157,279,317,625,808,2004,471,470,624,86,771,138,195,915,68,30,276,209,29,194,243,232,176,959,700,984,187,440,1226,155,557,36,170,320,463,239,1163,45,401,115,220,214,636,439,612,95,77,360,199,229,72,361,41,242,991,202,364,173,24,30,534,587,103,212,386,547,52,52,385,902,160,305,90,353,109,228,210,871,186,357,69,25,375,127,329,27,92,360,343,513,302,160,1999,68,827,546,24,313,16,1251,785,65,941,75,558,211,362,293,67,587,128,405,98,44,85,745,363,741,126,469,280,24,263,310,534,381,115,147,137,638,73,37,66,819,211,211,77,872,792,467,41,149,287,410,262,338,547,151,104,358,354,278,483,89,203,246,447,123,688,402,147,188,64,262,1021,990,94,44,65,1346,185,57,503,956,294,125,271,537,712,366,499,310,319,419,658,37,149,97,163,413,2225,180,155,20,394,34,38,148,105,295,71,532,854,54,49,289,314,46,781,788,113,292,1042,481,191,69,142,67,38,551,18,344,652,323,264,53,169,622,95,632,114,813,73,94,489,21,32,136,226,60,25,1181,144,61,36,60,129,342,83,345,271,506,204,32,89,71,464,253,1167,82,139,215,157,43,190,228,718,116,1619,222,489,135,91,387,368,476,268,108,348,289,151,220,77,73,86,145,124,339,464,215,27,282,325,35,371,450,74,446,167,206,50,457,223,123,1912,60,337,128,135,304,103,189,40,350,949,148,1687,905,556,368,594,417,466,419,428,195,81,44,638,149,737,233,140,149,299,188,442,514,1410,97,24,474,180,1381,404,162,144,47,181,604,97,287,432,171,60,501,374,622,95,683,675,110,247,152,149,164,902,170,669,720,57,307,239,271,1538,80,69,145,850,66,652,673,19,166,293,473,445,25,323,437,466,118,97,568,49,430,275,421,1076,154,464,264,1139,138,272,156,401,759,494,123,246,1069,519,174,63,63,879,456,184,1732,52,326,414,347,932,178,432,758,1474,67,66,455,560,478,230,48,138,309,332,23,263,530,167,218,133,165,305,93,786,301,40,725,219,147,14,239,357,1886,153,438,565,76,1414,161,321,944,1370,71,2718,297,148,114,164,352,116,1020,1090,76,337,147,193,307,63,400,35,126,66,698,264,447,272,984,580,111,37,10,673,189,1321,82,61,52,229,60,133,110,250,479,471,301,1022,688,229,64,225,706,402,2009,130,951,621,479,551,40,362,419,78,246,433,108,629,139,211,277,47,44,236,269,56,131,159,99,117,375,94,65,280,628,357,311,568,505,62,846,208,597,523,144,339,164,347,77,2003,128,769,51,426,379,27,27,111,111,376,275,30,756,122,662,71,207,307,452,175,135,380,431,100,148,687,455,62,296,220,1305,104,341,185,56,47,906,224,237,476,416,1064,660,227,894,156,206,1130,230,2000,61,128,468,161,33,480,59,1081,65,65,760,80,54,122,407,157,392,885,50,192,48,209,208,252,220,111,72,170,88,283,276,1860,148,33,69,89,86,145,39,86,309,633,23,45,971,351,104,1350,94,122,110,476,139,1553,220,284,158,1162,224,409,77,211,421,958,53,127,279,16,410,582,149,350,861,295,1688,84,485,237,497,408,87,225,534,192,175,617,76,66,243,409,637,490,279,80,90,12,636,1122,146,218,182,729,702,146,154,50,192,135,265,727,352,425,48,224,188,700,1260,1164,144,597,95,866,203,116,138,379,30,171,65,77,55,246,1187,95,1119,865,56,445,322,273,45,450,319,1014,600,1319,386,1087,701,53,105,449,2005,165,231,157,408,988,30,177,1616,977,155,135,141,299,104,188,64,2004,186,167,90,283,698,1100,937,565,91,118,873,114,131,59,117,1800,97,407,314,233,216,318,75,138,898,122,96,309,646,51,178,278,114,1899,198,474,157,277,912,671,359,1039,471,522,373,59,295,1223,90,888,125,71,202,153,304,25,516,14,345,222,406,351,510,140,782,218,566,478,2008,45,307,215,82,213,592,21,57,1330,86,232,66,871,215,182,50,39,271,55,104,88,125,1508,814,74,120,123,68,400,139,831,1384,167,169,196,29,249,795,395,739,250,556,612,2009,86,140,186,86,185,234,324,933,782,1029,434,180,418,156,121,254,1248,431,505,217,1162,678,1133,539,275,161,331,281,99,1432,197,383,106,56,544,926,458,58,359,123,1171,477,535,245,437,416,323,1187,75,306,138,154,143,581,386,1406,59,96,743,104,452,39,185,343,108,21,72,77,175,279,618,441,33,1367,162,99,459,249,11,342,57,108,689,567,178,21,176,181,1173,149,220,46,238,153,89,164,363,59,205,163,1631,21,91,331,348,110,144,35,259,519,90,42,60,119,24,39,353,346,101,90,282,1242,263,182,608,690,129,293,549,345,984,257,92,802,199,1673,65,274,242,218,334,487,543,202,489,477,76,78,416,75,63,288,207,121,726,622,132,655,992,896,15,26,1185,695,198,229,109,2006,303,368,150,41,165,142,76,232,252,170,79,285,406,468,325,276,193,368,83,367,300,306,265,480,219,256,780,254,556,369,373,979,51,163,247,324,117,165,512,48,325,112,23,104,308,457,48,1088,727,803,1743,92,412,220,397,371,213,312,158,263,280,213,181,68,68,233,182,512,140,114,415,18,558,963,1393,953,133,721,287,47,125,183,111,1623,333,225,125,445,215,288,998,428,257,825,24,309,519,467,194,89,334,438,683,536,532,641,501,119,610,361,52,33,704,275,267,504,232,652,34,764,353,124,482,430,276,1529,824,458,337,70,195,49,510,143,17,19,794,440,166,511,24,206,1963,170,76,102,106,153,49,56,1170,414,32,250,211,133,115,33,354,168,241,119,1016,334,582,134,112,176,572,133,271,237,70,703,49,231,303,258,102,771,79,985,49,112,233,76,437,774,394,51,201,45,104,839,83,290,395,235,816,146,1024,152,166,101,86,1003,219,121,1042,752,41,364,279,234,146,361,241,64,1251,37,409,59,338,387,296,657,154,1444,39,142,262,87,215,170,979,1597,85,384,26,1134,710,290,40,627,39,175,27,370,440,576,441,83,116,46,79,134,641,244,330,776,92,160,1591,460,174,468,124,575,560,414,369,244,2005,1445,147,355,352,22,227,98,45,249,427,1995,111,970,271,159,87,341,135,489,431,283,127,2005,248,356,75,274,46,103,116,43,811,74,46,409,867,272,13,320,38,25,55,512,200,633,213,180,348,151,634,537,204,261,177,43,11,136,113,126,81,161,59,195,53,66,340,815,144,224,689,308,121,101,21,393,92,14,29,339,52,60,55,524,472,296,649,27,300,1113,79,94,76,425,320,63,26,363,512,1024,1126,46,398,1437,178,131,345,407,1849,94,248,248,98,46,44,73,80,520,549,451,65,57,95,53,983,298,34,157,75,116,80,46,123,104,46,298,864,167,424,590,248,210,1567,159,51,568,645,117,113,152,128,798,993,298,79,72,347,381,335,477,1260,89,577,50,51,205,737,393,288,280,44,249,231,599,405,249,1068,1626,278,1075,48,819,1030,145,320,50,296,49,135,101,32,1600,823,75,419,114,292,25,351,162,310,434,92,146,278,78,87,181,75,2318,980,89,77,279,901,202,107,215,574,942,255,306,304,87,41,62,145,460,164,669,395,187,271,44,300,35,776,47,810,145,118,633,135,1230,446,139,911,110,1994,93,546,330,64,522,387,508,486,667,997,444,222,879,524,428,918,516,322,334,219,532,432,179,186,24,2005,434,231,207,562,67,513,43,110,92,185,213,613,171,147,76,317,188,31,976,167,283,363,53,248,288,422,39,23,129,198,507,492,382,572,123,340,265,784,294,15,170,342,282,186,123,332,916,144,32,113,678,41,308,382,472,171,35,489,109,455,786,193,194,402,134,67,172,235,729,744,31,2006,166,1065,671,312,47,611,101,157,101,185,652,166,91,293,105,19,186,227,92,320,301,40,547,348,31,57,603,430,465,524,67,129,299,411,588,172,186,1225,789,143,87,1463,169,59,63,170,91,93,723,1605,969,393,104,1511,251,92,188,341,39,59,185,207,269,42,436,17,134,127,222,362,535,415,166,2004,838,677,104,471,365,475,54,103,38,139,589,236,370,2006,55,1182,167,76,414,46,91,191,112,59,188,88,345,697,1751,639,953,365,40,193,877,188,1143,642,186,252,1502,96,332,748,678,790,49,69,260,146,1394,78,888,62,809,163,426,83,547,174,1054,275,89,249,139,827,232,274,63,703,104,529,497,2007,90,150,1117,638,140,55,624,68,207,33,136,411,180,270,22,119,162,174,263,556,650,192,209,902,777,113,283,362,65,760,856,82,300,189,1835,417,123,189,74,831,500,153,1295,59,171,987,68,549,657,68,192,555,217,642,53,126,678,259,140,1079,403,57,260,338,11,40,142,712,99,903,198,539,153,1251,474,490,87,72,42,187,51,394,1648,170,2005,91,137,493,1110,1211,28,1045,75,418,233,42,141,96,230,77,538,170,338,21,584,32,154,378,90,1185,74,53,109,123,177,16,114,67,366,270,60,87,113,116,86,225,98,147,419,175,120,89,245,311,324,498,445,451,43,24,685,87,146,155,643,288,893,73,29,123,93,110,65,75,1172,393,112,408,96,944,123,236,86,1036,302,38,628,29,63,416,343,257,192,321,232,239,384,288,135,163,537,525,1100,682,3780,41,571,70,171,302,1949,492,415,697,340,475,228,115,70,101,248,82,95,60,532,11,445,1419,87,117,181,266,533,583,22,151,207,39,734,69,58,155,87,421,25,43,36,104,80,745,130,621,212,465,266,728,124,350,23,41,766,115,365,152,16,96,278,78,71,418,54,1509,140,219,1022,621,603,94,755,204,599,107,454,104,92,105,228,410,266,70,39,152,307,83,731,78,140,308,117,253,153,277,154,854,131,339,75,39,503,152,417,245,74,750,110,411,219,1229,233,55,22,154,386,632,952,84,2002,487,80,1376,44,50,238,729,167,60,102,101,59,122,60,281,737,91,409,25,131,377,121,729,387,293,1618,1406,98,213,80,284,36,857,169,215,1023,62,885,46,197,126,526,20,122,1011,380,88,263,142,354,100,533,570,1008,21,638,374,264,241,76,288,177,370,212,453,58,311,135,15,490,95,823,423,147,90,786,637,98,120,220,225,63,1137,76,125,195,385,456,897,44,65,99,220,69,534,273,334,333,335,471,258,75,559,64,282,581,97,163,52,28,421,182,13,574,226,935,137,287,531,1479,1196,86,495,11,61,349,551,336,95,155,226,75,50,650,673,176,118,257,78,379,607,749,41,165,279,303,297,2002,298,26,212,138,183,475,832,196,303,485,267,169,650,212,17,1163,699,324,777,319,541,323,940,153,487,155,71,39,187,56,248,700,1235,288,167,352,104,126,260,413,1792,402,57,321,630,130,700,99,84,761,620,305,240,2000,1008,983,56,72,68,34,732,56,100,59,238,101,390,193,191,829,240,202,912,1607,1259,92,56,714,386,159,272,144,90,42,2009,356,420,54,558,298,1394,197,28,137,164,273,826,292,421,593,40,410,182,49,478,475,273,60,58,757,208,908,534,11,87,400,364,58,1176,2007,79,1578,299,368,290,728,777,226,59,362,328,44,561,561,292,2007,1001,821,125,936,193,567,139,106,25,149,484,250,68,237,341,1198,181,57,425,219,367,310,940,845,22,65,80,255,666,217,96,983,25,1037,311,99,47,868,117,156,254,46,79,1267,141,127,1013,132,1195,408,351,315,245,383,56,26,795,101,481,165,171,179,220,340,150,48,443,2005,1250,156,519,203,1406,330,86,474,65,66,304,617,437,143,1145,167,392,78,1186,190,253,115,748,62,947,870,34,26,172,478,220,137,1047,157,339,540,641,923,243,86,183,473,184,168,865,657,1086,301,272,1272,610,769,110,552,460,765,503,224,160,1484,122,846,93,69,167,139,214,325,140,509,107,129,369,538,395,98,642,740,140,350,600,456,299,243,196,1153,570,49,65,265,216,118,167,516,434,101,617,812,56,413,598,41,740,108,40,42,278,53,521,482,645,114,456,707,53,657,88,106,398,249,276,64,20,373,79,328,74,76,663,41,282,262,205,325,74,39,282,189,18,937,151,324,301,152,407,283,28,13,223,202,95,58,306,59,501,657,1732,284,1267,210,113,84,451,259,923,49,490,145,26,72,81,254,45,69,813,73,52,162,136,458,915,166,199,580,408,171,1699,307,79,52,55,103,511,784,310,292,189,269,41,433,234,839,1492,646,213,186,2009,522,417,667,53,139,2008,132,282,289,49,4991,160,343,95,1627,1094,600,134,309,859,445,106,194,204,133,1559,449,56,442,215,416,45,2005,489,408,149,687,1045,163,105,455,105,62,581,1278,85,552,304,494,301,116,94,1078,355,115,1183,26,382,352,364,1086,234,357,235,601,850,83,308,648,55,558,169,236,377,83,564,150,127,124,86,109,129,455,114,1540,598,157,328,527,110,193,469,33,883,410,634,166,198,38,425,408,783,223,11,302,134,405,71,37,257,128,149,333,73,77,561,20,27,143,178,332,1267,457,682,46,824,230,564,341,469,43,80,83,571,68,754,624,149,160,54,111,74,504,478,309,558,223,169,44,752,696,1218,1121,114,134,1052,93,639,295,494,125,420,349,682,828,713,104,485,750,36,24,141,33,1282,201,97,1116,94,128,607,234,340,203,310,121,39,280,876,226,144,120,98,231,767,563,150,102,1290,1824,198,817,1208,72,598,97,240,164,264,106,614,579,341,654,371,153,205,134,410,259,214,38,279,46,784,360,465,35,210,27,2003,151,445,1192,597,132,1460,750,1295,94,311,171,668,734,63,347,116,94,166,126,63,497,372,174,921,101,575,1038,590,656,927,144,248,1233,59,735,78,776,350,83,1748,301,875,185,90,362,87,277,357,1845,607,44,87,356,81,436,108,89,718,107,196,1939,733,373,73,1005,575,258,40,357,573,220,307,190,267,196,686,84,132,48,526,390,13,595,101,248,296,525,229,52,93,244,269,220,229,707,207,185,949,699,429,365,879,52,638,365,88,24,137,767,73,162,267,209,276,345,488,82,91,637,230,123,386,914,1375,271,333,85,68,274,571,727,89,265,132,370,295,49,683,91,1544,382,288,79,176,213,93,1587,262,1258,56,339,92,137,867,388,1336,161,1370,87,39,244,1062,566,1007,581,122,171,116,565,282,34,276,531,109,558,918,1075,29,190,463,159,94,53,870,265,459,988,199,401,84,62,64,159,96,147,153,212,75,844,12,958,94,262,239,331,456,1433,563,234,136,40,739,425,36,28,473,65,1063,313,272,84,69,69,146,179,519,856,1755,97,916,11,196,319,64,19,469,419,160,242,64,83,594,161,349,622,416,339,174,423,219,515,137,143,1551,384,306,657,372,873,257,58,65,40,411,114,473,303,62,120,121,249,138,75,164,338,88,232,357,432,219,215,59,723,496,318,272,107,876,44,1055,74,443,187,151,153,409,207,117,181,60,92,1669,304,342,243,53,605,1120,417,152,211,223,191,104,168,279,399,170,104,501,439,703,319,312,178,401,313,86,271,174,485,67,194,659,295,810,197,736,339,204,126,745,108,467,427,35,198,663,304,564,671,232,232,392,275,53,33,129,458,97,541,579,40,91,608,76,233,414,45,671,322,1138,20,204,1389,132,713,542,357,1488,46,50,222,859,131,90,519,406,318,80,54,207,45,95,554,25,46,1292,654,223,1724,71,142,637,206,2002,775,582,596,103,1057,222,116,177,105,229,523,51,396,424,164,78,397,720,704,289,76,274,102,518,97,128,242,107,607,257,348,240,81,1044,112,529,197,980,150,133,66,96,380,188,36,24,92,39,13,387,405,474,571,531,1905,13,22,140,297,46,757,371,50,78,188,81,88,22,50,614,166,585,156,88,88,352,373,1481,34,761,895,462,849,87,1385,124,75,375,519,219,315,303,306,764,80,231,959,334,213,201,246,923,1618,262,103,83,217,515,254,276,127,457,72,417,159,30,67,250,150,132,500,201,935,98,1013,317,1455,386,1071,51,608,205,52,15,150,236,108,697,47,165,150,455,270,210,1791,61,99,1106,47,781,259,196,40,171,428,585,476,175,169,40,30,175,355,167,235,231,917,99,728,43,452,1691,2003,32,342,430,86,757,160,106,854,192,321,175,175,270,51,394,205,90,196,1579,705,290,257,37,65,100,38,129,144,2008,1262,538,1152,148,106,382,80,1689,396,663,100,856,830,343,194,291,83,108,102,701,883,39,51,286,247,60,579,1580,240,45,344,713,505,163,47,325,168,216,301,174,646,290,172,317,1494,429,777,673,57,308,68,1551,396,133,987,87,140,195,139,60,1996,51,216,223,200,31,182,221,91,430,774,1404,86,134,218,459,116,668,379,29,385,416,63,387,1628,567,755,82,167,354,270,207,172,394,294,242,688,22,72,84,21,89,557,189,790,125,836,60,265,83,67,830,1685,149,141,229,122,275,140,50,316,384,982,42,166,180,46,195,289,280,1235,177,414,903,128,505,222,66,485,59,618,244,364,303,522,170,502,1203,141,119,283,414,511,36,103,131,544,261,290,75,70,175,55,289,417,679,56,236,1648,1505,528,35,83,914,416,226,148,181,201,526,218,81,72,185,319,112,1092,137,800,525,1022,464,526,96,541,32,173,42,31,535,1090,268,25,126,655,235,106,137,721,43,199,278,1006,220,92,91,97,293,156,251,105,306,18,29,80,344,395,810,67,200,360,92,432,59,380,16,115,186,380,167,235,326,103,451,111,184,526,635,234,90,140,66,1430,289,623,150,67,221,805,269,140,299,364,636,90,119,92,69,80,502,161,408,1950,253,70,741,28,237,312,749,320,543,141,317,423,314,37,941,399,133,170,231,233,109,134,166,1872,158,291,429,191,224,590,79,186,288,586,192,211,63,2008,735,1202,73,57,173,512,149,670,128,697,420,498,524,268,318,332,220,402,89,2001,525,110,16,905,138,63,176,74,515,439,606,2002,484,178,128,48,102,541,88,402,369,47,51,53,320,171,621,51,141,285,248,239,35,331,54,991,172,192,2059,108,565,50,51,1099,30,82,153,2010,204,598,564,384,1246,252,152,772,94,192,237,545,87,72,48,1521,236,91,270,53,359,81,172,40,601,70,347,169,243,676,250,24,169,165,578,1292,413,41,59,374,130,143,26,176,858,1088,498,154,52,599,322,90,69,368,162,603,287,1310,10,858,547,242,423,517,147,376,255,155,157,187,440,276,80,799,354,159,25,177,206,343,94,438,254,112,190,446,25,852,1085,198,81,97,720,69,27,293,179,424,604,82,72,176,60,90,416,66,102,607,32,273,163,174,257,1446,96,28,41,559,129,69,632,552,1642,773,174,186,116,1229,181,845,1110,724,107,669,99,441,405,496,135,1177,123,363,568,80,1862,68,525,80,16,316,350,153,49,291,304,408,152,305,22,268,1216,882,99,332,309,313,132,1574,276,111,279,669,15,44,52,115,203,100,285,195,327,85,345,217,510,56,88,824,337,125,82,188,305,581,147,81,666,184,73,148,2003,28,229,623,32,363,57,285,333,1266,71,584,159,71,157,132,352,2005,247,356,414,225,37,755,1212,483,465,333,105,633,181,175,61,337,25,14,53,70,343,273,231,465,23,233,110,16,212,117,821,26,143,111,264,84,630,1058,133,381,55,198,229,97,358,657,226,136,221,101,294,214,57,56,300,1408,272,54,185,381,1852,117,89,183,236,422,128,230,1527,2009,187,50,752,735,417,20,347,49,243,140,193,120,60,64,66,50,61,597,40,474,141,663,330,527,64,593,508,69,90,83,140,298,885,132,1247,65,19,886,1024,172,315,311,357,192,53,231,257,179,371,152,142,1487,377,499,470,29,255,555,1372,460,200,154,32,84,12,116,45,523,139,546,183,228,33,706,41,947,532,166,68,405,29,29,234,820,42,1389,978,45,883,568,650,411,57,646,77,408,98,100,452,494,73,146,242,136,385,266,201,1727,108,189,112,78,200,45,532,448,92,1477,173,131,159,226,541,673,310,447,415,56,49,520,161,299,572,182,306,163,67,642,159,302,1224,507,297,711,330,715,473,53,175,249,273,92,466,169,525,162,409,73,215,34,113,43,98,701,33,582,635,789,448,283,1220,1046,86,115,202,170,493,1455,729,252,125,611,482,185,199,270,235,134,762,58,65,976,449,1695,236,159,76,178,186,1052,1997,340,125,283,274,312,675,274,413,35,454,80,100,508,694,63,68,26,288,91,243,135,806,83,186,36,279,260,607,550,1585,231,373,663,305,128,144,89,111,93,243,282,87,298,1295,84,268,343,82,44,96,1219,220,1116,112,564,369,62,244,188,72,165,487,388,227,57,169,135,273,928,383,46,218,457,51,67,89,631,819,474,169,434,233,828,96,937,243,527,227,1143,159,109,66,273,63,164,97,62,216,17,435,102,757,354],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Text Length\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"title\":{\"text\":\"is_arabic\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Distribution of Text Lengths\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c22297e0-a625-4ff2-a763-5caf1b84f840');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_arabic_text(text):\n",
        "    \"\"\"Reshape Arabic text for proper display\"\"\"\n",
        "    # Remove punctuation and special characters\n",
        "    punctuation = '،.:;!؟()[]{}«»\\'\"'\n",
        "    text = ''.join(ch for ch in text if ch not in punctuation)\n",
        "\n",
        "    # Reshape and handle RTL\n",
        "    reshaped_text = arabic_reshaper.reshape(text)\n",
        "    bidi_text = get_display(reshaped_text)\n",
        "    return bidi_text\n",
        "\n",
        "def analyze_arabic_text(df):\n",
        "    \"\"\"Analyze Arabic text content\"\"\"\n",
        "    # Filter Arabic text\n",
        "    arabic_texts = df[df['is_arabic']]['text'].tolist()\n",
        "\n",
        "    # Tokenize and count words, excluding punctuation and single characters\n",
        "    all_words = []\n",
        "    for text in arabic_texts:\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            # Check if word contains Arabic characters and is longer than 1 character\n",
        "            if any('\\u0600' <= c <= '\\u06FF' for c in word) and len(word) > 1:\n",
        "                # Remove punctuation\n",
        "                word = ''.join(ch for ch in word if '\\u0600' <= ch <= '\\u06FF' or ch == ' ')\n",
        "                if word.strip():  # Only add non-empty words\n",
        "                    all_words.append(word)\n",
        "\n",
        "    word_freq = Counter(all_words).most_common(10)\n",
        "\n",
        "\n",
        "    words, counts = zip(*word_freq)\n",
        "    words = [preprocess_arabic_text(w) for w in words]\n",
        "\n",
        "    fig = go.Figure(data=[go.Bar(\n",
        "        x=words[::-1],\n",
        "        y=counts[::-1],\n",
        "        text=counts[::-1],\n",
        "        textposition='auto',\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Most Common Arabic Words',\n",
        "        xaxis_tickangle=-45,\n",
        "        height=500,\n",
        "        font=dict(size=14),\n",
        "        xaxis=dict(\n",
        "            autorange='reversed',\n",
        "            tickfont=dict(size=12)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "analyze_arabic_text(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "l4k63jdeLuRv",
        "outputId": "1900f9c5-3e8c-478a-d492-34988ed15f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"695a1c75-2cc4-4b8e-b4b9-fcc630ec8c7a\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"695a1c75-2cc4-4b8e-b4b9-fcc630ec8c7a\")) {                    Plotly.newPlot(                        \"695a1c75-2cc4-4b8e-b4b9-fcc630ec8c7a\",                        [{\"text\":[\"17210\",\"17376\",\"17677\",\"18704\",\"20398\",\"24466\",\"26538\",\"31967\",\"37978\",\"46079\"],\"textposition\":\"auto\",\"x\":[\"ﻢﻟ\",\"ﺎﻣ\",\"ﻪﻴﻠﻋ\",\"ﻱﺃ\",\"ﻪﻟﻮﻗ\",\"ﻻ\",\"ﻰﻠﻋ\",\"ﻭﺃ\",\"ﻦﻣ\",\"ﻲﻓ\"],\"y\":[17210,17376,17677,18704,20398,24466,26538,31967,37978,46079],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"tickfont\":{\"size\":12},\"tickangle\":-45,\"autorange\":\"reversed\"},\"font\":{\"size\":14},\"title\":{\"text\":\"Most Common Arabic Words\"},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('695a1c75-2cc4-4b8e-b4b9-fcc630ec8c7a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing **(For Pure HMM and CBHG + HMM)**"
      ],
      "metadata": {
        "id": "-3TFVs4cMxzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"[-()\\\"#/@;:_<>{}`﴾﴿ے+=~&|.!?,a-zA-Z0-9٠-٩]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def remove_diacritics(data_raw):\n",
        "  return data_raw.translate(str.maketrans('', '', ''.join(DIACRITICS_LIST)))"
      ],
      "metadata": {
        "id": "ou6UDCZSL98a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicPreprocessor:\n",
        "    \"\"\"Preprocessor for Arabic diacritization data\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \"arabic_data\"):\n",
        "        # Define main directories\n",
        "        self.DIACRITICS_LIST = re.compile(f\"[{''.join(DIACRITICS_LIST)}]\")\n",
        "        self.base_dir = base_dir\n",
        "        self.train_dir = os.path.join(base_dir, \"train\")\n",
        "        self.test_dir = os.path.join(base_dir, \"test\")\n",
        "        self.val_dir = os.path.join(base_dir, \"val\")\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for directory in [self.base_dir, self.train_dir, self.test_dir, self.val_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean the text by removing unwanted characters\"\"\"\n",
        "        text = re.sub(r\"[-()\\\"#/@;:_<>{}`﴾﴿ے+=~&|.!?,a-zA-Z0-9٠-٩]\", \"\", text)\n",
        "        return text\n",
        "\n",
        "    def split_into_words(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into words and filter empty ones\"\"\"\n",
        "        return [word for word in text.split() if word.strip()]\n",
        "\n",
        "    def is_diacritized(self, word: str) -> bool:\n",
        "        \"\"\"Check if a word contains diacritics\"\"\"\n",
        "        return bool(self.DIACRITICS_LIST.search(word))\n",
        "\n",
        "    def process_file(self, file_path: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"Process a single file and return list of word pairs\"\"\"\n",
        "        processed_words = []\n",
        "\n",
        "        try:\n",
        "            # Try different encodings\n",
        "            encodings = ['utf-8', 'utf-8-sig', 'cp1256', 'arabic']\n",
        "            text = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding=encoding) as f:\n",
        "                        text = f.read()\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if text is None:\n",
        "                # If all encodings fail, try binary mode\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    text = f.read().decode('utf-8', errors='ignore')\n",
        "\n",
        "            # Clean text before processing\n",
        "            cleaned_text = self.clean_text(text)\n",
        "\n",
        "            words = self.split_into_words(cleaned_text)\n",
        "\n",
        "            for word in words:\n",
        "                if self.is_diacritized(word):\n",
        "                    # Create clean version (without diacritics)\n",
        "                    clean_word = self.DIACRITICS_LIST.sub('', word)\n",
        "                    processed_words.append({\n",
        "                        'diacritized': word,\n",
        "                        'clean': clean_word\n",
        "                    })\n",
        "\n",
        "            print(f\"Processed {len(processed_words)} words from {file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "\n",
        "        return processed_words\n",
        "\n",
        "    def save_words(self, words: List[Dict[str, str]], output_file: str):\n",
        "        \"\"\"Save processed words to file\"\"\"\n",
        "        try:\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                for word in words:\n",
        "                    f.write(f\"{word['diacritized']}\\n\")\n",
        "            print(f\"Saved {len(words)} words to {output_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {output_file}: {str(e)}\")\n",
        "\n",
        "    def process_data(self,\n",
        "                     input_dir: str,\n",
        "                     train_ratio: float = 0.8,\n",
        "                     val_ratio: float = 0.1,\n",
        "                     test_ratio: float = 0.1):\n",
        "        \"\"\"Process all files and split into train/val/test sets\"\"\"\n",
        "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9, \"Ratios must sum to 1\"\n",
        "\n",
        "        # Get all text files\n",
        "        files = []\n",
        "        for ext in ['.txt']:  # Add more extensions if needed\n",
        "            files.extend(glob(os.path.join(input_dir, f\"**/*{ext}\"), recursive=True))\n",
        "\n",
        "        if not files:\n",
        "            raise ValueError(f\"No text files found in {input_dir}\")\n",
        "\n",
        "        print(f\"Found {len(files)} files to process\")\n",
        "\n",
        "        # Process files\n",
        "        all_words = []\n",
        "        for file_path in files:\n",
        "            words = self.process_file(file_path)\n",
        "            if words:  # Only add if we got some words\n",
        "                all_words.extend(words)\n",
        "\n",
        "        if not all_words:\n",
        "            raise ValueError(\"No diacritized words found in input files\")\n",
        "\n",
        "        print(f\"Processed {len(all_words)} words total\")\n",
        "\n",
        "        # Split data\n",
        "        train_end = int(len(all_words) * train_ratio)\n",
        "        val_end = int(len(all_words) * (train_ratio + val_ratio))\n",
        "\n",
        "        train_words = all_words[:train_end]\n",
        "        val_words = all_words[train_end:val_end]\n",
        "        test_words = all_words[val_end:]\n",
        "\n",
        "        # Save splits\n",
        "        self.save_words(train_words, os.path.join(self.train_dir, \"train.txt\"))\n",
        "        self.save_words(val_words, os.path.join(self.val_dir, \"val.txt\"))\n",
        "        self.save_words(test_words, os.path.join(self.test_dir, \"test.txt\"))\n",
        "\n",
        "        # Save clean versions for test set\n",
        "        with open(os.path.join(self.test_dir, \"test_clean.txt\"), 'w', encoding='utf-8') as f:\n",
        "            for word in test_words:\n",
        "                f.write(f\"{word['clean']}\\n\")\n",
        "\n",
        "        print(f\"Saved {len(train_words)} training words\")\n",
        "        print(f\"Saved {len(val_words)} validation words\")\n",
        "        print(f\"Saved {len(test_words)} test words\")\n",
        "\n",
        "        # Verify files were created and have content\n",
        "        for file_path in [\n",
        "            os.path.join(self.train_dir, \"train.txt\"),\n",
        "            os.path.join(self.val_dir, \"val.txt\"),\n",
        "            os.path.join(self.test_dir, \"test.txt\"),\n",
        "            os.path.join(self.test_dir, \"test_clean.txt\")\n",
        "        ]:\n",
        "            if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
        "                raise ValueError(f\"Failed to create or populate {file_path}\")\n",
        "\n",
        "def preprocess_arabic_data(\n",
        "    input_dir: str = \"raw_data\",\n",
        "    output_dir: str = \"arabic_data\",\n",
        "    train_ratio: float = 0.8,\n",
        "    val_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1\n",
        "):\n",
        "    \"\"\"Convenience function to preprocess data\"\"\"\n",
        "    preprocessor = ArabicPreprocessor(output_dir)\n",
        "    preprocessor.process_data(\n",
        "        input_dir=input_dir,\n",
        "        train_ratio=train_ratio,\n",
        "        val_ratio=val_ratio,\n",
        "        test_ratio=test_ratio\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    try:\n",
        "        preprocessor = preprocess_arabic_data(\n",
        "            input_dir=\"raw_data\",\n",
        "            output_dir=\"arabic_data\"\n",
        "        )\n",
        "        print(\"Preprocessing completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Preprocessing failed: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBLOwx86M_oL",
        "outputId": "a92803ea-4d86-4cde-f477-7076f0b43b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 files to process\n",
            "Processed 2093147 words from raw_data/train.txt\n",
            "Processed 2093147 words total\n",
            "Saved 1674517 words to arabic_data/train/train.txt\n",
            "Saved 209315 words to arabic_data/val/val.txt\n",
            "Saved 209315 words to arabic_data/test/test.txt\n",
            "Saved 1674517 training words\n",
            "Saved 209315 validation words\n",
            "Saved 209315 test words\n",
            "Preprocessing completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    entries = [e for e in text.split('\\n') if e.strip()]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'text': entries,\n",
        "        'length': [len(e) for e in entries],\n",
        "        'is_arabic': [any('\\u0600' <= c <= '\\u06FF' for c in e) for e in entries],\n",
        "        'word_count': [len(e.split()) for e in entries]\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "df = load_and_process_text(\"/content/arabic_data/train/train.txt\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "aStkezbJVKK5",
        "outputId": "6c1b9dd1-f636-4cbb-fb69-ff2a7e0da24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     text  length  is_arabic  word_count\n",
              "0  وَلَوْ       6       True           1\n",
              "1  جَمَعَ       6       True           1\n",
              "2   ثُمَّ       5       True           1\n",
              "3  عَلِمَ       6       True           1\n",
              "4  تَرْكَ       6       True           1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e304b2f-398b-4a24-82bc-6e68a69f2d23\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "      <th>is_arabic</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>وَلَوْ</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>جَمَعَ</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ثُمَّ</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>عَلِمَ</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تَرْكَ</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e304b2f-398b-4a24-82bc-6e68a69f2d23')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e304b2f-398b-4a24-82bc-6e68a69f2d23 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e304b2f-398b-4a24-82bc-6e68a69f2d23');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46e91520-d419-4c93-b9ff-7006caa1bfd9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46e91520-d419-4c93-b9ff-7006caa1bfd9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46e91520-d419-4c93-b9ff-7006caa1bfd9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_patterns(df):\n",
        "    \"\"\"Create pattern visualizations using Plotly\"\"\"\n",
        "    # Calculate various text patterns\n",
        "    pattern_data = {\n",
        "        'Has numbers': df['text'].str.contains('\\d').sum(),\n",
        "        'Has parentheses': df['text'].str.contains('[()]').sum(),\n",
        "        'Has punctuation': df['text'].str.contains('[،.:؛]').sum(),\n",
        "        'Pure Arabic': df['is_arabic'].sum() # this can be removed\n",
        "    }\n",
        "\n",
        "    # Create sunburst chart\n",
        "    fig = px.sunburst(\n",
        "        names=list(pattern_data.keys()),\n",
        "        parents=[''] * len(pattern_data),\n",
        "        values=list(pattern_data.values()),\n",
        "        title='Text Pattern Distribution'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "visualize_patterns(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "r-yIGaNaSfz8",
        "outputId": "e971bd6e-9626-46d9-fb85-25939c6b476e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"21f14438-cf43-407c-8b40-d82ca57e0c32\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"21f14438-cf43-407c-8b40-d82ca57e0c32\")) {                    Plotly.newPlot(                        \"21f14438-cf43-407c-8b40-d82ca57e0c32\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}\\u003cbr\\u003evalue=%{value}\\u003cbr\\u003eparent=%{parent}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"Has numbers\",\"Has parentheses\",\"Has punctuation\",\"Pure Arabic\"],\"name\":\"\",\"parents\":[\"\",\"\",\"\",\"\"],\"values\":[0,0,3739,1674517],\"type\":\"sunburst\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Text Pattern Distribution\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('21f14438-cf43-407c-8b40-d82ca57e0c32');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure HMM **(5-gram)**"
      ],
      "metadata": {
        "id": "gdmbb6BZE-bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMM:\n",
        "    \"\"\"Hidden Markov Model for Arabic diacritization\"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 5, model_dir: str = \"models\"):\n",
        "        \"\"\"\n",
        "        Initialize HMM model\n",
        "        Args:\n",
        "            n: n-gram size (default: 3)\n",
        "            model_dir: directory to save/load model files\n",
        "        \"\"\"\n",
        "        assert n >= 2, \"n-gram size must be >= 2\"\n",
        "\n",
        "        self.n = n\n",
        "        self.model_dir = model_dir\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # Model constants\n",
        "        self.START = '*'\n",
        "        self.NULL_TAG = 'O'  # Tag for non-diacritized characters\n",
        "\n",
        "        # Define possible diacritic states\n",
        "                # Define possible diacritic states\n",
        "        self.STATES = {\n",
        "            # Basic diacritics\n",
        "            'ُ', 'َ', 'ِ',      # Damma, Fatha, Kasra\n",
        "            'ْ', 'ّ',          # Sukoon, Shadda\n",
        "            'ٌ', 'ً', 'ٍ',      # Tanween (Double) Damma, Fatha, Kasra\n",
        "\n",
        "            # Shadda combinations\n",
        "            'َّ', 'ُّ', 'ِّ',    # Shadda + Basic\n",
        "            'ّْ',              # Shadda + Sukoon\n",
        "            'ٌّ', 'ًّ', 'ٍّ',    # Shadda + Tanween\n",
        "\n",
        "            # Additional combinations\n",
        "            'ْٰ', 'ٰ',         # Superscript Alef\n",
        "            'ٖ', 'ٗ',         # Other diacritics\n",
        "\n",
        "            'O'               # No diacritic\n",
        "        }\n",
        "\n",
        "        # Initialize or load n-gram model\n",
        "        self.model_path = os.path.join(model_dir, f\"{n}gram_model.pkl\")\n",
        "        self.character_ngram = self._load_model()\n",
        "\n",
        "    def _load_model(self) -> Dict:\n",
        "        \"\"\"Load existing model or create new one\"\"\"\n",
        "        try:\n",
        "            with open(self.model_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "                print(f\"Loaded existing {self.n}-gram model\")\n",
        "                return model\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Creating new {self.n}-gram model\")\n",
        "            return defaultdict(int)\n",
        "\n",
        "    def _save_model(self):\n",
        "        \"\"\"Save model to disk\"\"\"\n",
        "        with open(self.model_path, 'wb') as f:\n",
        "            pickle.dump(self.character_ngram, f)\n",
        "        print(f\"Saved model to {self.model_path}\")\n",
        "\n",
        "    def load_trained_model(self, model_path: str):\n",
        "        \"\"\"Load a pre-trained model from a .pkl file\"\"\"\n",
        "        with open(model_path, 'rb') as f:\n",
        "            self.character_ngram = pickle.load(f)\n",
        "        print(f\"Loaded pre-trained {self.n}-gram model from {model_path}\")\n",
        "        return self\n",
        "\n",
        "    def test_on_sentences(self, sentences: list):\n",
        "        \"\"\"\n",
        "        Test the model on a list of sentences\n",
        "        Args:\n",
        "            sentences: list of sentences (words) to diacritize\n",
        "        Returns:\n",
        "            List of diacritized sentences\n",
        "        \"\"\"\n",
        "        diacritized_sentences = []\n",
        "        for sentence in sentences:\n",
        "            diacritized_sentence = self.diacritize_word(sentence)\n",
        "            diacritized_sentences.append(diacritized_sentence)\n",
        "        return diacritized_sentences\n",
        "\n",
        "    def _extract_character_diacritic_pairs(self, word: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Extract character-diacritic pairs from a word\"\"\"\n",
        "        pairs = []\n",
        "        current_char = \"\"\n",
        "        current_diacritics = \"\"\n",
        "\n",
        "        for char in word:\n",
        "            if char in self.STATES:\n",
        "                current_diacritics += char\n",
        "            else:\n",
        "                if current_char:\n",
        "                    pairs.append((current_char,\n",
        "                                current_diacritics if current_diacritics else self.NULL_TAG))\n",
        "                current_char = char\n",
        "                current_diacritics = \"\"\n",
        "\n",
        "        # Handle last character\n",
        "        if current_char:\n",
        "            pairs.append((current_char,\n",
        "                         current_diacritics if current_diacritics else self.NULL_TAG))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def train(self, train_file: str):\n",
        "        \"\"\"\n",
        "        Train the HMM model on diacritized text\n",
        "        Args:\n",
        "            train_file: path to training file with diacritized words\n",
        "        \"\"\"\n",
        "        print(f\"Training {self.n}-gram model...\")\n",
        "        num_words = 0\n",
        "        num_errors = 0\n",
        "\n",
        "        with open(train_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word = line.strip()\n",
        "                if not word:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Extract character-diacritic pairs\n",
        "                    pairs = self._extract_character_diacritic_pairs(word)\n",
        "                    chars, diacritics = zip(*pairs)\n",
        "\n",
        "                    # Update n-gram counts\n",
        "                    d = deque(self.START * (self.n-1), maxlen=self.n-1)\n",
        "                    for char, diacritic in zip(chars, diacritics):\n",
        "                        self.character_ngram[(*d, char), diacritic] += 1\n",
        "                        d.append(char)\n",
        "                    num_words += 1\n",
        "                except Exception as e:\n",
        "                    num_errors += 1\n",
        "                    print(f\"Error processing word '{word}': {str(e)}\")\n",
        "\n",
        "        self._save_model()\n",
        "        print(f\"Processed {num_words} words with {num_errors} errors\")\n",
        "\n",
        "    def diacritize_word(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Add diacritics to a word using the trained model\n",
        "        Args:\n",
        "            word: clean word without diacritics\n",
        "        Returns:\n",
        "            Diacritized word\n",
        "        \"\"\"\n",
        "        d = deque(self.START * (self.n-1), maxlen=self.n-1)\n",
        "        result = []\n",
        "\n",
        "        for char in word:\n",
        "            # Find most likely diacritic\n",
        "            best_diacritic = self.NULL_TAG\n",
        "            best_count = 0\n",
        "\n",
        "            for diacritic in self.STATES:\n",
        "                count = self.character_ngram[(*d, char), diacritic]\n",
        "                if count > best_count:\n",
        "                    best_diacritic = diacritic\n",
        "                    best_count = count\n",
        "\n",
        "            # Add character and diacritic\n",
        "            result.append(char)\n",
        "            if best_diacritic != self.NULL_TAG:\n",
        "                result.append(best_diacritic)\n",
        "\n",
        "            d.append(char)\n",
        "\n",
        "        return ''.join(result)\n",
        "\n",
        "    def diacritize_sentence(self, sentence: str) -> str:\n",
        "      if not sentence.strip():\n",
        "          return \"\"  # Handle empty or whitespace-only sentences\n",
        "\n",
        "      # Split the sentence into words\n",
        "      words = sentence.split()\n",
        "\n",
        "      # Diacritize each word\n",
        "      diacritized_words = []\n",
        "      for word in words:\n",
        "          try:\n",
        "              # Assume `diacritize_word` is an existing method or replace with your word-level logic\n",
        "              diacritized_word = self.diacritize_word(word)\n",
        "              diacritized_words.append(diacritized_word)\n",
        "          except Exception as e:\n",
        "              print(f\"Error diacritizing word '{word}': {e}\")\n",
        "              diacritized_words.append(word)  # Keep original word if error occurs\n",
        "\n",
        "      # Rejoin words into a diacritized sentence\n",
        "      return \" \".join(diacritized_words)\n",
        "\n",
        "    def diacritize_text(self, input_file: str, output_file: str):\n",
        "        \"\"\"\n",
        "        Diacritize all words in a file\n",
        "        Args:\n",
        "            input_file: path to input file with clean words\n",
        "            output_file: path to save diacritized words\n",
        "        \"\"\"\n",
        "        with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
        "             open(output_file, 'w', encoding='utf-8') as fout:\n",
        "\n",
        "            for line in fin:\n",
        "                word = line.strip()\n",
        "                if word:\n",
        "                    diacritized = self.diacritize_word(word)\n",
        "                    fout.write(f\"{diacritized}\\n\")\n",
        "                else:\n",
        "                    fout.write(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hmm = HMM(n=5)\n",
        "    hmm.train(\"arabic_data/train/train.txt\")\n",
        "\n",
        "    test_word = \"اللغة العربية جميلة\"\n",
        "    predicted = hmm.diacritize_sentence(test_word)\n",
        "    print(f\"Input: {test_word}\")\n",
        "    print(f\"Prediction: {predicted}\")\n",
        "\n",
        "    hmm.diacritize_text(\n",
        "        \"arabic_data/test/test_clean.txt\",\n",
        "        \"arabic_data/test/test_predictions.txt\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EXl2uY79Blr",
        "outputId": "7fb63fc7-b101-40c1-aad0-e7eadc98a7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new 5-gram model\n",
            "Training 5-gram model...\n",
            "Saved model to models/5gram_model.pkl\n",
            "Processed 1674517 words with 0 errors\n",
            "Input: اللغة العربية جميلة\n",
            "Prediction: الْلّغَةِ الْعَرَبِيةِ جَمِيلَةً\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fe3iAsJBvul"
      },
      "source": [
        "## CBHG + HMM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Encoders"
      ],
      "metadata": {
        "id": "jQaeTHGFu5jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Dict, Tuple, Union, Any\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "def basic_cleaners(text: str) -> str:\n",
        "    \"\"\"Basic Arabic text cleaning - keeps only valid Arabic characters\"\"\"\n",
        "    # Define valid Arabic characters\n",
        "    valid_chars = \"بض.غىهظخة؟:طس،؛فندؤلوئآك-يذاصشحزءمأجإ ترقعث\"\n",
        "    return ''.join(c for c in text if c in valid_chars)\n",
        "\n",
        "class TextEncoder:\n",
        "    pad = \"P\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_chars: List[str],\n",
        "        target_charts: List[str],\n",
        "        cleaner_fn: Optional[str] = None,\n",
        "        reverse_input: bool = False,\n",
        "        reverse_target: bool = False,\n",
        "    ):\n",
        "        # Instead of dynamic import, use direct function reference\n",
        "        if cleaner_fn == \"basic_cleaners\":\n",
        "            self.cleaner_fn = basic_cleaners\n",
        "        else:\n",
        "            self.cleaner_fn = None\n",
        "\n",
        "        self.input_symbols: List[str] = [TextEncoder.pad] + input_chars\n",
        "        # Add 'O' to target symbols if not present\n",
        "        if 'O' not in target_charts:\n",
        "            target_charts = ['O'] + target_charts\n",
        "        self.target_symbols: List[str] = [TextEncoder.pad] + target_charts\n",
        "\n",
        "        self.input_symbol_to_id: Dict[str, int] = {\n",
        "            s: i for i, s in enumerate(self.input_symbols)\n",
        "        }\n",
        "        self.input_id_to_symbol: Dict[int, str] = {\n",
        "            i: s for i, s in enumerate(self.input_symbols)\n",
        "        }\n",
        "\n",
        "        self.target_symbol_to_id: Dict[str, int] = {\n",
        "            s: i for i, s in enumerate(self.target_symbols)\n",
        "        }\n",
        "        self.target_id_to_symbol: Dict[int, str] = {\n",
        "            i: s for i, s in enumerate(self.target_symbols)\n",
        "        }\n",
        "\n",
        "        self.reverse_input = reverse_input\n",
        "        self.reverse_target = reverse_target\n",
        "        self.input_pad_id = self.input_symbol_to_id[self.pad]\n",
        "        self.target_pad_id = self.target_symbol_to_id[self.pad]\n",
        "        self.start_symbol_id = None\n",
        "\n",
        "    def input_to_sequence(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert input text to sequence of indices\"\"\"\n",
        "        if self.cleaner_fn:\n",
        "            text = self.cleaner_fn(text)\n",
        "\n",
        "        if self.reverse_input:\n",
        "            text = text[::-1]\n",
        "\n",
        "        sequence = []\n",
        "        for char in text:\n",
        "            if char in self.input_symbol_to_id:\n",
        "                sequence.append(self.input_symbol_to_id[char])\n",
        "        return sequence\n",
        "\n",
        "    def target_to_sequence(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert target text to sequence of indices\"\"\"\n",
        "        if self.reverse_target:\n",
        "            text = text[::-1]\n",
        "\n",
        "        sequence = []\n",
        "        for char in text:\n",
        "            if char in self.target_symbol_to_id:\n",
        "                sequence.append(self.target_symbol_to_id[char])\n",
        "        return sequence\n",
        "\n",
        "    def sequence_to_input(self, sequence: List[int]) -> str:\n",
        "        \"\"\"Convert sequence of indices back to input text\"\"\"\n",
        "        text = ''.join([\n",
        "            self.input_id_to_symbol[idx]\n",
        "            for idx in sequence\n",
        "            if idx in self.input_id_to_symbol and idx != self.input_pad_id\n",
        "        ])\n",
        "\n",
        "        if self.reverse_input:\n",
        "            text = text[::-1]\n",
        "        return text\n",
        "\n",
        "    def sequence_to_target(self, sequence: List[int]) -> str:\n",
        "        \"\"\"Convert sequence of indices back to target text\"\"\"\n",
        "        text = ''.join([\n",
        "            self.target_id_to_symbol[idx]\n",
        "            for idx in sequence\n",
        "            if idx in self.target_id_to_symbol and idx != self.target_pad_id\n",
        "        ])\n",
        "\n",
        "        if self.reverse_target:\n",
        "            text = text[::-1]\n",
        "        return text\n",
        "\n",
        "    def combine_text_and_haraqat(self, input_ids: List[int], output_ids: List[int]) -> str:\n",
        "        \"\"\"Combine input text with diacritics\"\"\"\n",
        "        input_text = self.sequence_to_input(input_ids)\n",
        "        output_text = self.sequence_to_target(output_ids)\n",
        "\n",
        "        result = \"\"\n",
        "        for i, char in enumerate(input_text):\n",
        "            result += char\n",
        "            if i < len(output_text) and output_text[i] != 'O':\n",
        "                result += output_text[i]\n",
        "        return result\n",
        "\n",
        "class BasicArabicEncoder(TextEncoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cleaner_fn=\"basic_cleaners\",\n",
        "        reverse_input: bool = False,\n",
        "        reverse_target: bool = False,\n",
        "    ):\n",
        "        input_chars: List[str] = list(\"بض.غىهظخة؟:طس،؛فندؤلوئآك-يذاصشحزءمأجإ ترقعث\")\n",
        "        target_charts: List[str] = ['ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ّ', 'ٌّ', 'ًّ', 'ٍّ', 'ُّ', 'َّ', 'ِّ', 'ّْ']\n",
        "\n",
        "        super().__init__(\n",
        "            input_chars,\n",
        "            target_charts,\n",
        "            cleaner_fn=cleaner_fn,\n",
        "            reverse_input=reverse_input,\n",
        "            reverse_target=reverse_target,\n",
        "        )\n",
        "\n",
        "class ArabicEncoderWithStartSymbol(TextEncoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cleaner_fn=\"basic_cleaners\",\n",
        "        reverse_input: bool = False,\n",
        "        reverse_target: bool = False,\n",
        "    ):\n",
        "        input_chars: List[str] = list(\"بض.غىهظخة؟:طس،؛فندؤلوئآك-يذاصشحزءمأجإ ترقعث\")\n",
        "\n",
        "        # Extended list of diacritics including all possible combinations\n",
        "        target_charts: List[str] = [\n",
        "            # Basic diacritics\n",
        "            'ُ', 'َ', 'ِ',      # Damma, Fatha, Kasra\n",
        "            'ْ', 'ّ',          # Sukoon, Shadda\n",
        "            'ٌ', 'ً', 'ٍ',      # Tanween (Double) Damma, Fatha, Kasra\n",
        "\n",
        "            # Shadda combinations\n",
        "            'َّ', 'ُّ', 'ِّ',    # Shadda + Basic\n",
        "            'ّْ',              # Shadda + Sukoon\n",
        "            'ٌّ', 'ًّ', 'ٍّ',    # Shadda + Tanween\n",
        "\n",
        "            # Additional combinations\n",
        "            'ْٰ', 'ٰ',         # Superscript Alef\n",
        "            'ٖ', 'ٗ',         # Other diacritics\n",
        "\n",
        "            # Start symbol\n",
        "            's'\n",
        "        ]\n",
        "\n",
        "        super().__init__(\n",
        "            input_chars,\n",
        "            target_charts,\n",
        "            cleaner_fn=cleaner_fn,\n",
        "            reverse_input=reverse_input,\n",
        "            reverse_target=reverse_target,\n",
        "        )\n",
        "\n",
        "        self.start_symbol_id = self.target_symbol_to_id[\"s\"]"
      ],
      "metadata": {
        "id": "hl7iumW3We_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gabh-nEBvum"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_iterator(word: Optional[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extract character-diacritic pairs from a word.\n",
        "    Handles combined diacritics correctly.\n",
        "    \"\"\"\n",
        "    if word is None:  # Handle None input\n",
        "        return []\n",
        "\n",
        "    # Sort by length for proper matching\n",
        "    DIACRITICS_LIST.sort(key=len, reverse=True)\n",
        "\n",
        "    output = []\n",
        "    current_char = \"\"\n",
        "    current_diacritics = \"\"\n",
        "\n",
        "    for char in word:\n",
        "        # Check if char is a diacritic or part of a combined diacritic\n",
        "        is_diacritic = False\n",
        "        for diacritic in DIACRITICS_LIST:\n",
        "            if char in diacritic:\n",
        "                current_diacritics += char\n",
        "                is_diacritic = True\n",
        "                break\n",
        "\n",
        "        if not is_diacritic:\n",
        "            if current_char:\n",
        "                # Process any accumulated diacritics\n",
        "                final_diacritic = ''\n",
        "                for diacritic in DIACRITICS_LIST:\n",
        "                    if diacritic in current_diacritics:\n",
        "                        final_diacritic = diacritic\n",
        "                        break\n",
        "                output.append((current_char, final_diacritic if final_diacritic else 'O'))\n",
        "            current_char = char\n",
        "            current_diacritics = \"\"\n",
        "\n",
        "    # Handle last character\n",
        "    if current_char:\n",
        "        final_diacritic = ''\n",
        "        for diacritic in DIACRITICS_LIST:\n",
        "            if diacritic in current_diacritics:\n",
        "                final_diacritic = diacritic\n",
        "                break\n",
        "        output.append((current_char, final_diacritic if final_diacritic else 'O'))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def evaluate_word(gold_word: str, pred_word: str, analysis: bool = False) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate prediction against gold standard\"\"\"\n",
        "    gold_pairs = word_iterator(gold_word)\n",
        "    pred_pairs = word_iterator(pred_word)\n",
        "\n",
        "    # Ensure same length\n",
        "    min_len = min(len(gold_pairs), len(pred_pairs))\n",
        "    gold_pairs = gold_pairs[:min_len]\n",
        "    pred_pairs = pred_pairs[:min_len]\n",
        "\n",
        "    correct = sum(1 for (_, g_d), (_, p_d) in zip(gold_pairs, pred_pairs) if g_d == p_d)\n",
        "    total = len(gold_pairs)\n",
        "\n",
        "    if analysis:\n",
        "        return correct, total\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "def take_first_n_lines(input_file: str, output_file: str, n: int = 2000):\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as fin:\n",
        "            lines = [next(fin) for _ in range(n)]  # Read first `n` lines\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as fout:\n",
        "            fout.writelines(lines)  # Write lines to output file\n",
        "\n",
        "        print(f\"Successfully wrote first {n} lines to {output_file}\")\n",
        "    except StopIteration:\n",
        "        print(f\"Input file has less than {n} lines. Writing all available lines.\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as fout:\n",
        "            fout.writelines(lines)  # Write available lines to output file\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {e}\")"
      ],
      "metadata": {
        "id": "pU0lR8X3WxMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKtWMj_8Bvuo"
      },
      "source": [
        "### Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicDiacritizationDataset(Dataset):\n",
        "    \"\"\"Dataset class for Arabic diacritization data\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, max_samples: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Initialize dataset\n",
        "        Args:\n",
        "            data_path: Path to the data file (train.txt, val.txt, etc.)\n",
        "            max_samples: Maximum number of samples to load (for faster training)\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.max_samples = max_samples\n",
        "        self.text_encoder = ArabicEncoderWithStartSymbol()\n",
        "        self.data: List[Dict[str, str]] = []\n",
        "\n",
        "        # Load and process data file\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load data from preprocessed file\"\"\"\n",
        "        try:\n",
        "            with open(self.data_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "                # Take subset if max_samples is specified\n",
        "                if self.max_samples is not None:\n",
        "                    lines = lines[:self.max_samples]\n",
        "\n",
        "                # Use tqdm for loading progress\n",
        "                for line in tqdm(lines, desc=f\"Loading {os.path.basename(self.data_path)}\"):\n",
        "                    word = line.strip()\n",
        "                    if word:  # Skip empty lines\n",
        "                        # Store both diacritized and clean versions\n",
        "                        self.data.append({\n",
        "                            \"diacritized\": word,\n",
        "                            \"clean\": remove_diacritics(word)\n",
        "                        })\n",
        "            print(f\"Loaded {len(self.data)} words from {self.data_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from {self.data_path}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset\n",
        "        Returns:\n",
        "            Dictionary containing:\n",
        "            - input: tensor of input character indices\n",
        "            - target: tensor of target diacritic indices\n",
        "            - raw_text: original diacritized text\n",
        "        \"\"\"\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Convert clean text to input sequence\n",
        "        input_seq = self.text_encoder.input_to_sequence(item[\"clean\"])\n",
        "\n",
        "        # Get target sequence from diacritized text\n",
        "        target_seq = []\n",
        "        for _, diacritic in word_iterator(item[\"diacritized\"]):\n",
        "            target_seq.append(self.text_encoder.target_symbol_to_id[diacritic])\n",
        "\n",
        "        # Convert to tensors\n",
        "        input_tensor = torch.LongTensor(input_seq)\n",
        "        target_tensor = torch.LongTensor(target_seq)\n",
        "\n",
        "        return {\n",
        "            \"input\": input_tensor,\n",
        "            \"target\": target_tensor,\n",
        "            \"raw_text\": item[\"diacritized\"],\n",
        "            \"input_len\": len(input_seq),\n",
        "            \"target_len\": len(target_seq)\n",
        "        }\n",
        "\n",
        "    def collate_fn(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Custom collate function to handle batching\n",
        "        Args:\n",
        "            batch: List of individual samples\n",
        "        Returns:\n",
        "            Batched tensors with padding\n",
        "        \"\"\"\n",
        "        # Get max sequence length in batch\n",
        "        max_input_len = max(x[\"input_len\"] for x in batch)\n",
        "        max_target_len = max(x[\"target_len\"] for x in batch)\n",
        "\n",
        "        # Prepare tensors\n",
        "        batch_size = len(batch)\n",
        "        input_tensor = torch.full(\n",
        "            (batch_size, max_input_len),\n",
        "            self.text_encoder.input_pad_id,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        target_tensor = torch.full(\n",
        "            (batch_size, max_input_len),  # Make target same length as input\n",
        "            self.text_encoder.target_pad_id,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Fill tensors\n",
        "        for i, item in enumerate(batch):\n",
        "            input_len = item[\"input_len\"]\n",
        "            target_len = item[\"target_len\"]\n",
        "\n",
        "            # Ensure target length matches input length\n",
        "            min_len = min(input_len, target_len)\n",
        "\n",
        "            input_tensor[i, :min_len] = item[\"input\"][:min_len]\n",
        "            target_tensor[i, :min_len] = item[\"target\"][:min_len]\n",
        "\n",
        "        return {\n",
        "            \"input\": input_tensor,\n",
        "            \"target\": target_tensor,\n",
        "            \"raw_text\": [x[\"raw_text\"] for x in batch],\n",
        "            \"lengths\": torch.LongTensor([min(x[\"input_len\"], x[\"target_len\"]) for x in batch])\n",
        "        }"
      ],
      "metadata": {
        "id": "7DZht7tET6h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaiGj1iABvuo"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionType(Enum):\n",
        "    \"\"\"Type of attention used during training\"\"\"\n",
        "\n",
        "    LocationSensitive = 1\n",
        "    Content_Based = 2\n",
        "    MultiHead = 3\n",
        "\n",
        "\n",
        "class LearningRateType(Enum):\n",
        "    \"\"\"Type of learning rate used during training\"\"\"\n",
        "\n",
        "    Learning_Rate_Decay = 1\n",
        "    Cosine_Scheduler = 2\n",
        "    SquareRoot_Scheduler = 3\n",
        "\n",
        "\n",
        "class OptimizerType(Enum):\n",
        "    \"\"\"Type of optimizer used during training\"\"\"\n",
        "\n",
        "    Adam = 1\n",
        "    SGD = 2\n",
        "    AdamW = 3\n",
        "\n",
        "\n",
        "class LossType(Enum):\n",
        "    \"\"\"Type of loss function used during training\"\"\"\n",
        "\n",
        "    L1_LOSS = 1\n",
        "    MSE_LOSS = 2\n",
        "    L1_LOSS_MASKED = 3\n",
        "    MSE_LOSS_MASKED = 4\n",
        "    BOTH = 5\n",
        "    BOTH_MASKED = 6"
      ],
      "metadata": {
        "id": "2y-AMWc0XfED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim: int, n_heads: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hid_dim % n_heads == 0\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o = nn.Linear(hid_dim * 2, hid_dim)\n",
        "\n",
        "        if dropout != 0.0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_dropout = dropout != 0.0\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        device = query.device\n",
        "\n",
        "        # Move scale to the correct device\n",
        "        self.scale = self.scale.to(device)\n",
        "\n",
        "        # query = [batch size, query len, hid dim]\n",
        "        # key = [batch size, key len, hid dim]\n",
        "        # value = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q = [batch size, query len, hid dim]\n",
        "        # K = [batch size, key len, hid dim]\n",
        "        # V = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q = [batch size, n heads, query len, head dim]\n",
        "        # K = [batch size, n heads, key len, head dim]\n",
        "        # V = [batch size, n heads, value len, head dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy = [batch size, n heads, query len, key len]\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(1)  # Add dims for heads\n",
        "            energy = energy.masked_fill(mask == 0, -float(\"inf\"))\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "        if self.use_dropout:\n",
        "            context_vector = torch.matmul(self.dropout(attention), V)\n",
        "        else:\n",
        "            context_vector = torch.matmul(attention, V)\n",
        "\n",
        "        # x = [batch size, n heads, query len, head dim]\n",
        "\n",
        "        context_vector = context_vector.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # x = [batch size, query len, n heads, head dim]\n",
        "\n",
        "        context_vector = context_vector.view(batch_size, -1, self.hid_dim)\n",
        "\n",
        "        x = torch.cat((query, context_vector), dim=-1)\n",
        "\n",
        "        # x = [batch size, query len, hid dim * 2]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        # x = [batch size, query len, hid dim]\n",
        "\n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "2hpQROWqX2bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhjGwLnMBvup"
      },
      "source": [
        "###HMM Model (For Combined Model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HMM:\n",
        "    \"\"\"Hidden Markov Model for Arabic diacritization\"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 5, model_dir: str = \"models\"):\n",
        "        \"\"\"\n",
        "        Initialize HMM model\n",
        "        Args:\n",
        "            n: n-gram size (default: 3)\n",
        "            model_dir: directory to save/load model files\n",
        "        \"\"\"\n",
        "        assert n >= 2, \"n-gram size must be >= 2\"\n",
        "\n",
        "        self.n = n\n",
        "        self.model_dir = model_dir\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # Model constants\n",
        "        self.START = '*'\n",
        "        self.NULL_TAG = 'O'  # Tag for non-diacritized characters\n",
        "\n",
        "        # Define possible diacritic states\n",
        "                # Define possible diacritic states\n",
        "        self.STATES = {\n",
        "            # Basic diacritics\n",
        "            'ُ', 'َ', 'ِ',      # Damma, Fatha, Kasra\n",
        "            'ْ', 'ّ',          # Sukoon, Shadda\n",
        "            'ٌ', 'ً', 'ٍ',      # Tanween (Double) Damma, Fatha, Kasra\n",
        "\n",
        "            # Shadda combinations\n",
        "            'َّ', 'ُّ', 'ِّ',    # Shadda + Basic\n",
        "            'ّْ',              # Shadda + Sukoon\n",
        "            'ٌّ', 'ًّ', 'ٍّ',    # Shadda + Tanween\n",
        "\n",
        "            # Additional combinations\n",
        "            'ْٰ', 'ٰ',         # Superscript Alef\n",
        "            'ٖ', 'ٗ',         # Other diacritics\n",
        "\n",
        "            'O'               # No diacritic\n",
        "        }\n",
        "\n",
        "        # Initialize or load n-gram model\n",
        "        self.model_path = os.path.join(model_dir, f\"{n}gram_model.pkl\")\n",
        "        self.character_ngram = self._load_model()\n",
        "\n",
        "    def _load_model(self) -> Dict:\n",
        "        \"\"\"Load existing model or create new one\"\"\"\n",
        "        try:\n",
        "            with open(self.model_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "                print(f\"Loaded existing {self.n}-gram model\")\n",
        "                return model\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Creating new {self.n}-gram model\")\n",
        "            return defaultdict(int)\n",
        "\n",
        "    def _save_model(self):\n",
        "        \"\"\"Save model to disk\"\"\"\n",
        "        with open(self.model_path, 'wb') as f:\n",
        "            pickle.dump(self.character_ngram, f)\n",
        "        print(f\"Saved model to {self.model_path}\")\n",
        "\n",
        "    def load_trained_model(self, model_path: str):\n",
        "        \"\"\"Load a pre-trained model from a .pkl file\"\"\"\n",
        "        with open(model_path, 'rb') as f:\n",
        "            self.character_ngram = pickle.load(f)\n",
        "        print(f\"Loaded pre-trained {self.n}-gram model from {model_path}\")\n",
        "        return self\n",
        "\n",
        "    def test_on_sentences(self, sentences: list):\n",
        "        \"\"\"\n",
        "        Test the model on a list of sentences\n",
        "        Args:\n",
        "            sentences: list of sentences (words) to diacritize\n",
        "        Returns:\n",
        "            List of diacritized sentences\n",
        "        \"\"\"\n",
        "        diacritized_sentences = []\n",
        "        for sentence in sentences:\n",
        "            diacritized_sentence = self.diacritize_word(sentence)\n",
        "            diacritized_sentences.append(diacritized_sentence)\n",
        "        return diacritized_sentences\n",
        "\n",
        "    def _extract_character_diacritic_pairs(self, word: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Extract character-diacritic pairs from a word\"\"\"\n",
        "        pairs = []\n",
        "        current_char = \"\"\n",
        "        current_diacritics = \"\"\n",
        "\n",
        "        for char in word:\n",
        "            if char in self.STATES:\n",
        "                current_diacritics += char\n",
        "            else:\n",
        "                if current_char:\n",
        "                    pairs.append((current_char,\n",
        "                                current_diacritics if current_diacritics else self.NULL_TAG))\n",
        "                current_char = char\n",
        "                current_diacritics = \"\"\n",
        "\n",
        "        # Handle last character\n",
        "        if current_char:\n",
        "            pairs.append((current_char,\n",
        "                         current_diacritics if current_diacritics else self.NULL_TAG))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def train(self, train_file: str):\n",
        "        \"\"\"\n",
        "        Train the HMM model on diacritized text\n",
        "        Args:\n",
        "            train_file: path to training file with diacritized words\n",
        "        \"\"\"\n",
        "        print(f\"Training {self.n}-gram model...\")\n",
        "        num_words = 0\n",
        "        num_errors = 0\n",
        "\n",
        "        with open(train_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word = line.strip()\n",
        "                if not word:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Extract character-diacritic pairs\n",
        "                    pairs = self._extract_character_diacritic_pairs(word)\n",
        "                    chars, diacritics = zip(*pairs)\n",
        "\n",
        "                    # Update n-gram counts\n",
        "                    d = deque(self.START * (self.n-1), maxlen=self.n-1)\n",
        "                    for char, diacritic in zip(chars, diacritics):\n",
        "                        self.character_ngram[(*d, char), diacritic] += 1\n",
        "                        d.append(char)\n",
        "                    num_words += 1\n",
        "                except Exception as e:\n",
        "                    num_errors += 1\n",
        "                    print(f\"Error processing word '{word}': {str(e)}\")\n",
        "\n",
        "        self._save_model()\n",
        "        print(f\"Processed {num_words} words with {num_errors} errors\")\n",
        "\n",
        "    def diacritize_word(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Add diacritics to a word using the trained model\n",
        "        Args:\n",
        "            word: clean word without diacritics\n",
        "        Returns:\n",
        "            Diacritized word\n",
        "        \"\"\"\n",
        "        d = deque(self.START * (self.n-1), maxlen=self.n-1)\n",
        "        result = []\n",
        "\n",
        "        for char in word:\n",
        "            # Find most likely diacritic\n",
        "            best_diacritic = self.NULL_TAG\n",
        "            best_count = 0\n",
        "\n",
        "            for diacritic in self.STATES:\n",
        "                count = self.character_ngram[(*d, char), diacritic]\n",
        "                if count > best_count:\n",
        "                    best_diacritic = diacritic\n",
        "                    best_count = count\n",
        "\n",
        "            # Add character and diacritic\n",
        "            result.append(char)\n",
        "            if best_diacritic != self.NULL_TAG:\n",
        "                result.append(best_diacritic)\n",
        "\n",
        "            d.append(char)\n",
        "\n",
        "        return ''.join(result)\n",
        "\n",
        "    def diacritize_sentence(self, sentence: str) -> str:\n",
        "      if not sentence.strip():\n",
        "          return \"\"  # Handle empty or whitespace-only sentences\n",
        "\n",
        "      # Split the sentence into words\n",
        "      words = sentence.split()\n",
        "\n",
        "      # Diacritize each word\n",
        "      diacritized_words = []\n",
        "      for word in words:\n",
        "          try:\n",
        "              # Assume `diacritize_word` is an existing method or replace with your word-level logic\n",
        "              diacritized_word = self.diacritize_word(word)\n",
        "              diacritized_words.append(diacritized_word)\n",
        "          except Exception as e:\n",
        "              print(f\"Error diacritizing word '{word}': {e}\")\n",
        "              diacritized_words.append(word)  # Keep original word if error occurs\n",
        "\n",
        "      # Rejoin words into a diacritized sentence\n",
        "      return \" \".join(diacritized_words)\n",
        "\n",
        "    def diacritize_text(self, input_file: str, output_file: str):\n",
        "        \"\"\"\n",
        "        Diacritize all words in a file\n",
        "        Args:\n",
        "            input_file: path to input file with clean words\n",
        "            output_file: path to save diacritized words\n",
        "        \"\"\"\n",
        "        with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
        "             open(output_file, 'w', encoding='utf-8') as fout:\n",
        "\n",
        "            for line in fin:\n",
        "                word = line.strip()\n",
        "                if word:\n",
        "                    diacritized = self.diacritize_word(word)\n",
        "                    fout.write(f\"{diacritized}\\n\")\n",
        "                else:\n",
        "                    fout.write(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hmm = HMM(n=3)\n",
        "\n",
        "    # Train model\n",
        "    hmm.train(\"arabic_data/train/train.txt\")\n",
        "\n",
        "    # Test on single word\n",
        "    test_word = \"مقدمة\"\n",
        "    predicted = hmm.diacritize_word(test_word)\n",
        "    print(f\"Input: {test_word}\")\n",
        "    print(f\"Prediction: {predicted}\")\n",
        "\n",
        "    # Diacritize test file\n",
        "    hmm.diacritize_text(\n",
        "        \"arabic_data/test/test_clean.txt\",\n",
        "        \"arabic_data/test/test_predictions.txt\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgHhwqEfYKeJ",
        "outputId": "a840d862-7519-4e04-b97f-1a2ac40235ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing 3-gram model\n",
            "Training 3-gram model...\n",
            "Saved model to models/3gram_model.pkl\n",
            "Processed 1674517 words with 0 errors\n",
            "Input: مقدمة\n",
            "Prediction: مَقَدَمَةِ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxFC7MGkBvup"
      },
      "source": [
        "### CBHG Model (For Combined Model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "    A nn.Conv1d followed by an optional activation function, and nn.BatchNorm1d\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int,\n",
        "        out_dim: int,\n",
        "        kernel_size: int,\n",
        "        stride: int,\n",
        "        padding: int,\n",
        "        activation: Optional[Any] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_dim,\n",
        "            out_dim,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        # Replace BatchNorm1d with LayerNorm\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1d(x)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        # Apply LayerNorm across the feature dimension\n",
        "        x = x.transpose(1, 2)  # [batch, time, channels]\n",
        "        x = self.norm(x)\n",
        "        x = x.transpose(1, 2)  # [batch, channels, time]\n",
        "        return x\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        \"\"\"Override train method to handle training mode\"\"\"\n",
        "        super().train(mode)\n",
        "        if mode:\n",
        "            self.conv1d.train()\n",
        "            self.norm.train()\n",
        "        return self\n",
        "\n",
        "\n",
        "class Prenet(nn.Module):\n",
        "    \"\"\"A prenet is a collection of linear layers with dropout(0.5),\n",
        "    and RELU activation function\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, in_dim: int, prenet_depth: List[int] = [256, 128], dropout: int = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in_sizes = [in_dim] + prenet_depth[:-1]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                nn.Linear(in_size, out_size)\n",
        "                for (in_size, out_size) in zip(in_sizes, prenet_depth)\n",
        "            ]\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        for linear in self.layers:\n",
        "            inputs = self.dropout(self.relu(linear(inputs)))\n",
        "        return inputs\n",
        "\n",
        "class CBHG(nn.Module):\n",
        "    \"\"\"The CBHG module\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int,\n",
        "        out_dim: int,\n",
        "        K: int,\n",
        "        projections: List[int],\n",
        "        highway_layers: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1d_banks = nn.ModuleList(\n",
        "            [\n",
        "                BatchNormConv1d(\n",
        "                    in_dim,\n",
        "                    in_dim,\n",
        "                    kernel_size=k,\n",
        "                    stride=1,\n",
        "                    padding=k // 2,\n",
        "                    activation=self.relu,\n",
        "                )\n",
        "                for k in range(1, K + 1)\n",
        "            ]\n",
        "        )\n",
        "        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        in_sizes = [K * in_dim] + projections[:-1]\n",
        "        activations = [self.relu] * (len(projections) - 1) + [None]\n",
        "        self.conv1d_projections = nn.ModuleList(\n",
        "            [\n",
        "                BatchNormConv1d(\n",
        "                    in_size, out_size, kernel_size=3, stride=1, padding=1, activation=ac\n",
        "                )\n",
        "                for (in_size, out_size, ac) in zip(in_sizes, projections, activations)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n",
        "        self.highways = nn.ModuleList([Highway(in_dim, in_dim) for _ in range(4)])\n",
        "\n",
        "        self.gru = nn.GRU(in_dim, out_dim, 1, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        \"\"\"Override train method to ensure proper training mode\"\"\"\n",
        "        super().train(mode)\n",
        "        if mode:\n",
        "            self.gru.train()\n",
        "        return self\n",
        "\n",
        "    def forward(self, inputs, input_lengths=None):\n",
        "        # (B, T_in, in_dim)\n",
        "        x = inputs\n",
        "        x = x.transpose(1, 2)\n",
        "        T = x.size(-1)\n",
        "\n",
        "        # (B, in_dim*K, T_in)\n",
        "        # Concat conv1d bank outputs\n",
        "        x = torch.cat([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], dim=1)\n",
        "        assert x.size(1) == self.in_dim * len(self.conv1d_banks)\n",
        "        x = self.max_pool1d(x)[:, :, :T]\n",
        "\n",
        "        for conv1d in self.conv1d_projections:\n",
        "            x = conv1d(x)\n",
        "\n",
        "        # (B, T_in, in_dim)\n",
        "        # Back to the original shape\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        if x.size(-1) != self.in_dim:\n",
        "            x = self.pre_highway(x)\n",
        "\n",
        "        # Residual connection\n",
        "        x += inputs\n",
        "        for highway in self.highways:\n",
        "            x = highway(x)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True)\n",
        "\n",
        "        # Initialize hidden states\n",
        "        batch_size = inputs.size(0)\n",
        "        hidden = torch.zeros(2, batch_size, self.out_dim, device=inputs.device)\n",
        "\n",
        "        # (B, T_in, in_dim*2)\n",
        "        self.gru.flatten_parameters()\n",
        "        outputs, _ = self.gru(x, hidden)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Highway(nn.Module):\n",
        "    \"\"\"Highway Networks\"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        self.H = nn.Linear(in_size, out_size)\n",
        "        self.H.bias.data.zero_()\n",
        "        self.T = nn.Linear(in_size, out_size)\n",
        "        self.T.bias.data.fill_(-1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        H = self.relu(self.H(inputs))\n",
        "        T = self.sigmoid(self.T(inputs))\n",
        "        return H * T + inputs * (1.0 - T)\n",
        "\n",
        "\n",
        "\n",
        "class CBHGModel(nn.Module):\n",
        "    \"\"\"CBHG model implementation\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp_vocab_size: int,\n",
        "        targ_vocab_size: int,\n",
        "        embedding_dim: int = 256,\n",
        "        use_prenet: bool = True,\n",
        "        prenet_sizes: List[int] = [512, 256],\n",
        "        cbhg_gru_units: int = 256,\n",
        "        cbhg_filters: int = 16,\n",
        "        cbhg_projections: List[int] = [128, 256],\n",
        "        post_cbhg_layers_units: List[int] = [256, 256],\n",
        "        post_cbhg_use_batch_norm: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_dim)\n",
        "\n",
        "        if use_prenet:\n",
        "            self.prenet = Prenet(embedding_dim, prenet_depth=prenet_sizes)\n",
        "            cbhg_input_dim = prenet_sizes[-1]\n",
        "        else:\n",
        "            cbhg_input_dim = embedding_dim\n",
        "\n",
        "        self.use_prenet = use_prenet\n",
        "\n",
        "        self.cbhg = CBHG(\n",
        "            cbhg_input_dim,\n",
        "            cbhg_gru_units,\n",
        "            K=cbhg_filters,\n",
        "            projections=cbhg_projections,\n",
        "        )\n",
        "\n",
        "        # Adjust dimensions for LSTM layers\n",
        "        layers = []\n",
        "        current_dim = cbhg_gru_units * 2  # CBHG output is bidirectional, so *2\n",
        "\n",
        "        for target_dim in post_cbhg_layers_units:\n",
        "            # Add dimension reduction layer if needed\n",
        "            if current_dim != target_dim * 2:  # *2 because LSTM is bidirectional\n",
        "                layers.append(nn.Linear(current_dim, target_dim * 2))\n",
        "                if post_cbhg_use_batch_norm:\n",
        "                    layers.append(nn.LayerNorm(target_dim * 2))  # Use LayerNorm instead of BatchNorm\n",
        "                layers.append(nn.ReLU())\n",
        "                current_dim = target_dim * 2\n",
        "\n",
        "            # Add LSTM layer\n",
        "            lstm = nn.LSTM(\n",
        "                input_size=target_dim * 2,\n",
        "                hidden_size=target_dim,\n",
        "                bidirectional=True,\n",
        "                batch_first=True,\n",
        "            )\n",
        "            layers.append(lstm)\n",
        "\n",
        "            if post_cbhg_use_batch_norm:\n",
        "                layers.append(nn.LayerNorm(target_dim * 2))  # Use LayerNorm instead of BatchNorm\n",
        "\n",
        "            current_dim = target_dim * 2  # Output dim is target_dim * 2 due to bidirectional\n",
        "\n",
        "        self.post_cbhg_layers = nn.ModuleList(layers)\n",
        "        self.projections = nn.Linear(current_dim, targ_vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        lengths: Optional[torch.Tensor] = None,\n",
        "        target: Optional[torch.Tensor] = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Compute forward propagation\"\"\"\n",
        "\n",
        "        embedding_out = self.embedding(src)\n",
        "\n",
        "        cbhg_input = embedding_out\n",
        "        if self.use_prenet:\n",
        "            cbhg_input = self.prenet(embedding_out)\n",
        "\n",
        "        outputs = self.cbhg(cbhg_input, lengths)\n",
        "\n",
        "        # Process through layers\n",
        "        for layer in self.post_cbhg_layers:\n",
        "            if isinstance(layer, nn.LayerNorm):\n",
        "                outputs = layer(outputs)\n",
        "            elif isinstance(layer, nn.LSTM):\n",
        "                # Initialize hidden states\n",
        "                batch_size = outputs.size(0)\n",
        "                hidden = torch.zeros(2, batch_size, layer.hidden_size, device=outputs.device)\n",
        "                cell = torch.zeros(2, batch_size, layer.hidden_size, device=outputs.device)\n",
        "                outputs, _ = layer(outputs, (hidden, cell))\n",
        "            else:  # Linear or ReLU\n",
        "                outputs = layer(outputs)\n",
        "\n",
        "        predictions = self.projections(outputs)\n",
        "\n",
        "        return {\"diacritics\": predictions}\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        \"\"\"Override train method to handle RNN training mode\"\"\"\n",
        "        super().train(mode)\n",
        "        if mode:\n",
        "            # Set CBHG module to training mode\n",
        "            self.cbhg.train()\n",
        "            # Set all LSTM layers to training mode\n",
        "            for layer in self.post_cbhg_layers:\n",
        "                if isinstance(layer, (nn.LSTM, nn.GRU)):\n",
        "                    layer.train()\n",
        "        return self"
      ],
      "metadata": {
        "id": "yZ9U0TTEYu1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkuXIf9gBvuq"
      },
      "source": [
        "### Final Combined Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDiacritizationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined model that uses both HMM and CBHG approaches for Arabic diacritization\n",
        "    with attention mechanisms\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int = 256,\n",
        "        use_prenet: bool = False,\n",
        "        prenet_sizes: List[int] = [512, 256],\n",
        "        cbhg_gru_units: int = 256,\n",
        "        cbhg_filters: int = 16,\n",
        "        cbhg_projections: List[int] = [128, 256],\n",
        "        post_cbhg_layers_units: List[int] = [256, 256],\n",
        "        post_cbhg_use_batch_norm: bool = True,\n",
        "        hmm_n_gram: int = 3,\n",
        "        n_attention_heads: int = 8,\n",
        "        attention_dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize text encoder\n",
        "        self.text_encoder = ArabicEncoderWithStartSymbol()\n",
        "\n",
        "        # Initialize CBHG model\n",
        "        self.cbhg_model = CBHGModel(\n",
        "            inp_vocab_size=len(self.text_encoder.input_symbols),\n",
        "            targ_vocab_size=len(self.text_encoder.target_symbols),\n",
        "            embedding_dim=embedding_dim,\n",
        "            use_prenet=use_prenet,\n",
        "            prenet_sizes=prenet_sizes,\n",
        "            cbhg_gru_units=cbhg_gru_units,\n",
        "            cbhg_filters=cbhg_filters,\n",
        "            cbhg_projections=cbhg_projections,\n",
        "            post_cbhg_layers_units=post_cbhg_layers_units,\n",
        "            post_cbhg_use_batch_norm=post_cbhg_use_batch_norm\n",
        "        )\n",
        "\n",
        "        # Initialize HMM model\n",
        "        self.hmm_model = HMM(n=hmm_n_gram)\n",
        "\n",
        "        # Add attention layers\n",
        "        hidden_dim = len(self.text_encoder.target_symbols)\n",
        "        # Ensure hidden_dim is divisible by n_attention_heads\n",
        "        attention_dim = ((hidden_dim + n_attention_heads - 1) // n_attention_heads) * n_attention_heads\n",
        "\n",
        "        # Add projection layers to match attention dimensions\n",
        "        self.cbhg_projection = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.hmm_projection = nn.Linear(hidden_dim, attention_dim)\n",
        "\n",
        "        # Self-attention for CBHG output\n",
        "        self.cbhg_self_attention = MultiHeadAttentionLayer(\n",
        "            hid_dim=attention_dim,\n",
        "            n_heads=n_attention_heads,\n",
        "            dropout=attention_dropout\n",
        "        )\n",
        "\n",
        "        # Self-attention for HMM output\n",
        "        self.hmm_self_attention = MultiHeadAttentionLayer(\n",
        "            hid_dim=attention_dim,\n",
        "            n_heads=n_attention_heads,\n",
        "            dropout=attention_dropout\n",
        "        )\n",
        "\n",
        "        # Cross-attention between CBHG and HMM outputs\n",
        "        self.cross_attention = MultiHeadAttentionLayer(\n",
        "            hid_dim=attention_dim,\n",
        "            n_heads=n_attention_heads,\n",
        "            dropout=attention_dropout\n",
        "        )\n",
        "\n",
        "        # Project back to original dimension\n",
        "        self.output_projection = nn.Linear(attention_dim, hidden_dim)\n",
        "\n",
        "        # Fusion layer with increased dimension to account for attention outputs\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(attention_dropout),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        lengths: Optional[torch.Tensor] = None,\n",
        "        target: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Forward pass of the combined model with attention\"\"\"\n",
        "\n",
        "        # Ensure input is on the correct device\n",
        "        device = src.device\n",
        "\n",
        "        # Get CBHG predictions\n",
        "        cbhg_output = self.cbhg_model(src, lengths, target)\n",
        "        cbhg_predictions = cbhg_output[\"diacritics\"]\n",
        "\n",
        "        # Get HMM predictions\n",
        "        batch_size = src.size(0)\n",
        "        seq_len = src.size(1)\n",
        "        hmm_predictions = torch.zeros_like(cbhg_predictions)\n",
        "\n",
        "        # Convert input tensors to text for HMM\n",
        "        for b in range(batch_size):\n",
        "            text = \"\".join(self.text_encoder.sequence_to_input(src[b].tolist()))\n",
        "            hmm_output = self.hmm_model.diacritize_word(text)\n",
        "\n",
        "            # Convert HMM output back to tensor format\n",
        "            hmm_seq = self.text_encoder.target_to_sequence(hmm_output)\n",
        "            hmm_tensor = torch.zeros(seq_len, len(self.text_encoder.target_symbols), device=device)\n",
        "            for i, idx in enumerate(hmm_seq):\n",
        "                if i >= seq_len:\n",
        "                    break\n",
        "                hmm_tensor[i][idx] = 1.0\n",
        "            hmm_predictions[b] = hmm_tensor\n",
        "\n",
        "        # Create attention mask for padding\n",
        "        if lengths is not None:\n",
        "            mask = torch.arange(seq_len, device=device).expand(batch_size, seq_len)\n",
        "            mask = mask < lengths.unsqueeze(1)\n",
        "        else:\n",
        "            mask = None\n",
        "\n",
        "        # Project inputs to attention dimension\n",
        "        cbhg_proj = self.cbhg_projection(cbhg_predictions)\n",
        "        hmm_proj = self.hmm_projection(hmm_predictions)\n",
        "\n",
        "        # Apply self-attention to both outputs\n",
        "        cbhg_attended, _ = self.cbhg_self_attention(\n",
        "            cbhg_proj,\n",
        "            cbhg_proj,\n",
        "            cbhg_proj,\n",
        "            mask\n",
        "        )\n",
        "\n",
        "        hmm_attended, _ = self.hmm_self_attention(\n",
        "            hmm_proj,\n",
        "            hmm_proj,\n",
        "            hmm_proj,\n",
        "            mask\n",
        "        )\n",
        "\n",
        "        # Apply cross-attention between CBHG and HMM outputs\n",
        "        cross_attended, _ = self.cross_attention(\n",
        "            cbhg_attended,  # query from CBHG\n",
        "            hmm_attended,   # key/value from HMM\n",
        "            hmm_attended,\n",
        "            mask\n",
        "        )\n",
        "\n",
        "        # Project back to original dimension\n",
        "        cbhg_attended = self.output_projection(cbhg_attended)\n",
        "        cross_attended = self.output_projection(cross_attended)\n",
        "\n",
        "        # Combine all attention outputs\n",
        "        combined_input = torch.cat([\n",
        "            cbhg_predictions,    # Original CBHG output\n",
        "            hmm_predictions,     # Original HMM output\n",
        "            cbhg_attended,       # Self-attended CBHG\n",
        "            cross_attended       # Cross-attended output\n",
        "        ], dim=-1)\n",
        "\n",
        "        # Pass through fusion layer\n",
        "        final_predictions = self.fusion_layer(combined_input)\n",
        "\n",
        "        return {\n",
        "            \"diacritics\": final_predictions,\n",
        "            \"cbhg_predictions\": cbhg_predictions,\n",
        "            \"hmm_predictions\": hmm_predictions\n",
        "        }\n",
        "\n",
        "    def predict(self, text: str) -> str:\n",
        "        \"\"\"Make prediction for a single text input\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Prepare input\n",
        "            clean_text = remove_diacritics(text)\n",
        "            input_seq = self.text_encoder.input_to_sequence(clean_text)\n",
        "            input_tensor = torch.LongTensor(input_seq).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get prediction\n",
        "            output = self(input_tensor)\n",
        "            pred_seq = output[\"diacritics\"].argmax(dim=-1)[0]\n",
        "\n",
        "            # Convert back to text\n",
        "            result = self.text_encoder.combine_text_and_haraqat(\n",
        "                input_seq,\n",
        "                pred_seq.cpu().tolist()\n",
        "            )\n",
        "\n",
        "            return result\n",
        "\n",
        "    def evaluate_text(self, actual_text: str, pred_text: str) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate predictions using WER and DER metrics\"\"\"\n",
        "        correct_chars, total_chars = evaluate_word(actual_text, pred_text, analysis=True)\n",
        "\n",
        "        # Word Error Rate\n",
        "        wer = 0.0 if correct_chars == total_chars else 1.0\n",
        "\n",
        "        # Diacritic Error Rate\n",
        "        der = 1.0 - (correct_chars / total_chars)\n",
        "\n",
        "        return {\n",
        "            \"WER\": wer,\n",
        "            \"DER\": der\n",
        "        }\n"
      ],
      "metadata": {
        "id": "-rRzWkwrZHzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxjny4NYBvur"
      },
      "source": [
        "### Training Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(name):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    if not name:\n",
        "        return\n",
        "    name = str(name)\n",
        "    if not os.path.isdir(name):\n",
        "        os.makedirs(name)\n",
        "\n",
        "class CombinedModelTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: CombinedDiacritizationModel,\n",
        "        train_data: str,\n",
        "        val_data: str,\n",
        "        batch_size: int = 32,\n",
        "        learning_rate: float = 0.001,  # Ensure learning_rate is passed as float\n",
        "        checkpoint_dir: str = \"checkpoints\",\n",
        "        device: str = None,\n",
        "        max_samples: Optional[int] = None\n",
        "    ):\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Setup data\n",
        "        self.train_dataset = ArabicDiacritizationDataset(train_data, max_samples=max_samples)\n",
        "        self.val_dataset = ArabicDiacritizationDataset(val_data, max_samples=max_samples)\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.train_dataset.collate_fn\n",
        "        )\n",
        "        self.val_loader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            collate_fn=self.val_dataset.collate_fn\n",
        "        )\n",
        "\n",
        "        # Setup training\n",
        "        from torch.optim import Adam  # Correct PyTorch Adam optimizer import\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=learning_rate)  # PyTorch optimizer setup\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Setup checkpointing\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        create_dir(checkpoint_dir)\n",
        "\n",
        "    def train_epoch(self) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_wer = 0\n",
        "        total_der = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
        "        for batch in pbar:\n",
        "            src = batch[\"input\"].to(self.device)\n",
        "            tgt = batch[\"target\"].to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(src)\n",
        "            loss = self.criterion(\n",
        "                output[\"diacritics\"].view(-1, output[\"diacritics\"].size(-1)),\n",
        "                tgt.view(-1)\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            with torch.no_grad():\n",
        "                pred_text = self.model.predict(batch[\"raw_text\"][0])\n",
        "                metrics = self.model.evaluate_text(batch[\"raw_text\"][0], pred_text)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_wer += metrics[\"WER\"]\n",
        "            total_der += metrics[\"DER\"]\n",
        "            num_batches += 1\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'WER': f'{metrics[\"WER\"]:.4f}',\n",
        "                'DER': f'{metrics[\"DER\"]:.4f}'\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            \"loss\": total_loss / num_batches,\n",
        "            \"WER\": total_wer / num_batches,\n",
        "            \"DER\": total_der / num_batches\n",
        "        }\n",
        "\n",
        "    def validate(self) -> Dict[str, float]:\n",
        "        total_loss = 0\n",
        "        total_wer = 0\n",
        "        total_der = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(self.val_loader, desc=\"Validating\")\n",
        "            for batch in pbar:\n",
        "                src = batch[\"input\"].to(self.device)\n",
        "                tgt = batch[\"target\"].to(self.device)\n",
        "\n",
        "                output = self.model(src)\n",
        "                loss = self.criterion(\n",
        "                    output[\"diacritics\"].view(-1, output[\"diacritics\"].size(-1)),\n",
        "                    tgt.view(-1)\n",
        "                )\n",
        "\n",
        "                pred_text = self.model.predict(batch[\"raw_text\"][0])\n",
        "                metrics = self.model.evaluate_text(batch[\"raw_text\"][0], pred_text)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_wer += metrics[\"WER\"]\n",
        "                total_der += metrics[\"DER\"]\n",
        "                num_batches += 1\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'val_loss': f'{loss.item():.4f}',\n",
        "                    'val_WER': f'{metrics[\"WER\"]:.4f}',\n",
        "                    'val_DER': f'{metrics[\"DER\"]:.4f}'\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": total_loss / num_batches,\n",
        "            \"val_WER\": total_wer / num_batches,\n",
        "            \"val_DER\": total_der / num_batches\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": self.model.state_dict(),\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "        path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "    def train(self, num_epochs: int, validate_every: int = 1):\n",
        "        print(f\"Training on device: {self.device}\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_metrics = self.train_epoch()\n",
        "            print(f\"Training - Loss: {train_metrics['loss']:.4f}, WER: {train_metrics['WER']:.4f}, DER: {train_metrics['DER']:.4f}\")\n",
        "\n",
        "            # Validation\n",
        "            if (epoch + 1) % validate_every == 0:\n",
        "                val_metrics = self.validate()\n",
        "                print(f\"Validation - Loss: {val_metrics['val_loss']:.4f}, WER: {val_metrics['val_WER']:.4f}, DER: {val_metrics['val_DER']:.4f}\")\n",
        "\n",
        "                # Save checkpoint\n",
        "                self.save_checkpoint(epoch + 1, {**train_metrics, **val_metrics})"
      ],
      "metadata": {
        "id": "YaBgzp2GZhVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq-Q3BbBvus"
      },
      "source": [
        "### Training Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data_directories():\n",
        "    \"\"\"Setup and verify data directories\"\"\"\n",
        "    base_dir = \"arabic_data\"\n",
        "    preprocessor = ArabicPreprocessor(base_dir)\n",
        "\n",
        "    # Check if preprocessed data exists\n",
        "    train_file = os.path.join(preprocessor.train_dir, \"train.txt\")\n",
        "    val_file = os.path.join(preprocessor.val_dir, \"val.txt\")\n",
        "    test_file = os.path.join(preprocessor.test_dir, \"test.txt\")\n",
        "\n",
        "    if not all(os.path.exists(f) for f in [train_file, val_file, test_file]):\n",
        "        print(\"Preprocessing data...\")\n",
        "        preprocessor = preprocess_arabic_data(\n",
        "            input_dir=\"raw_data\",\n",
        "            output_dir=base_dir,\n",
        "            train_ratio=0.8,\n",
        "            val_ratio=0.1,\n",
        "            test_ratio=0.1\n",
        "        )\n",
        "        print(\"Data preprocessing completed!\")\n",
        "    else:\n",
        "        print(\"Using existing preprocessed data\")\n",
        "\n",
        "    # Verify data files\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_words = f.readlines()\n",
        "    with open(val_file, 'r', encoding='utf-8') as f:\n",
        "        val_words = f.readlines()\n",
        "\n",
        "    print(f\"Found {len(train_words)} training words and {len(val_words)} validation words\")\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "def get_model_config():\n",
        "    \"\"\"Get model configuration\"\"\"\n",
        "    return {\n",
        "        \"embedding_dim\": 256,\n",
        "        \"use_prenet\": False,\n",
        "        \"prenet_sizes\": [512, 256],\n",
        "        \"cbhg_gru_units\": 256,\n",
        "        \"cbhg_filters\": 16,\n",
        "        \"cbhg_projections\": [128, 256],\n",
        "        \"post_cbhg_layers_units\": [256, 256],\n",
        "        \"post_cbhg_use_batch_norm\": True,\n",
        "        \"hmm_n_gram\": 3,\n",
        "        \"n_attention_heads\": 8,\n",
        "        \"attention_dropout\": 0.1\n",
        "    }\n",
        "\n",
        "def train_model(preprocessor: ArabicPreprocessor):\n",
        "    \"\"\"Initialize and train the model\"\"\"\n",
        "    # Get model configuration\n",
        "    model_config = get_model_config()\n",
        "\n",
        "    # Initialize model\n",
        "    model = CombinedDiacritizationModel(**model_config)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup trainer with device and max_samples\n",
        "    trainer = CombinedModelTrainer(\n",
        "        model=model,\n",
        "        train_data=os.path.join(preprocessor.train_dir, \"train.txt\"),\n",
        "        val_data=os.path.join(preprocessor.val_dir, \"val.txt\"),\n",
        "        batch_size=32,\n",
        "        learning_rate=0.001,  # Fixed: Change 'lr' to 'learning_rate'\n",
        "        checkpoint_dir=\"checkpoints\",\n",
        "        device=device,\n",
        "        max_samples=3000  # Use only 3000 samples for faster training\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer.train(\n",
        "        num_epochs=20,\n",
        "        validate_every=1\n",
        "    )\n",
        "\n",
        "    # Save final model and training state\n",
        "    final_checkpoint = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": trainer.optimizer.state_dict(),\n",
        "        \"model_config\": model_config,\n",
        "        \"epoch\": 10,\n",
        "        \"device\": str(device)\n",
        "    }\n",
        "\n",
        "    # Save to both checkpoint directory and current directory\n",
        "    checkpoint_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
        "    torch.save(final_checkpoint, checkpoint_path)\n",
        "    torch.save(final_checkpoint, \"final_model.pt\")\n",
        "\n",
        "    print(f\"Model saved to {checkpoint_path} and final_model.pt\")\n",
        "\n",
        "    return model, trainer\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        print(\"CUDA is available. Training on GPU.\")\n",
        "    else:\n",
        "        print(\"Training on CPU.\")\n",
        "\n",
        "    try:\n",
        "        # Setup data\n",
        "        preprocessor = setup_data_directories()\n",
        "\n",
        "        # Train model\n",
        "        model, trainer = train_model(preprocessor)\n",
        "\n",
        "        # Test the model on some examples\n",
        "        test_examples = [\n",
        "            \"مقدمة\",\n",
        "            \"الطبري\",\n",
        "            \"شيخ\",\n",
        "            \"الدين\"\n",
        "        ]\n",
        "\n",
        "        print(\"\\nTesting model on examples:\")\n",
        "        for text in test_examples:\n",
        "            # Ensure predict() method is defined in your model\n",
        "            prediction = model.predict(text)\n",
        "            print(f\"Input: {text}\")\n",
        "            print(f\"Prediction: {prediction}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJeIT8DsZ6bd",
        "outputId": "c8b29b76-4da9-4f75-e59c-13d016bf948c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Training on GPU.\n",
            "Using existing preprocessed data\n",
            "Found 1674517 training words and 209315 validation words\n",
            "Loaded existing 3-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading train.txt: 100%|██████████| 3000/3000 [00:00<00:00, 124209.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3000 words from arabic_data/train/train.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading val.txt: 100%|██████████| 3000/3000 [00:00<00:00, 88807.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3000 words from arabic_data/val/val.txt\n",
            "Training on device: cuda\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:07<00:00, 11.83it/s, loss=0.6769, WER=1.0000, DER=0.7500]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.8640, WER: 0.8936, DER: 0.4932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 30.04it/s, val_loss=0.4934, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.5401, WER: 0.7979, DER: 0.4025\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 16.59it/s, loss=0.3189, WER=1.0000, DER=0.1429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.4373, WER: 0.7553, DER: 0.3397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.17it/s, val_loss=0.3267, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4132, WER: 0.7234, DER: 0.3370\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:06<00:00, 15.53it/s, loss=0.3007, WER=1.0000, DER=0.6667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.3342, WER: 0.6809, DER: 0.3249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.47it/s, val_loss=0.2225, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3721, WER: 0.7128, DER: 0.3280\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.87it/s, loss=0.3421, WER=1.0000, DER=0.2000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.2740, WER: 0.5213, DER: 0.1905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 26.20it/s, val_loss=0.1934, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3279, WER: 0.6489, DER: 0.2750\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 18.14it/s, loss=0.2181, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.2378, WER: 0.5319, DER: 0.2277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.51it/s, val_loss=0.1941, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3305, WER: 0.6809, DER: 0.3151\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:06<00:00, 15.10it/s, loss=0.1493, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.2042, WER: 0.5319, DER: 0.2057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 32.79it/s, val_loss=0.2372, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3301, WER: 0.6064, DER: 0.2992\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.75it/s, loss=0.2361, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.1775, WER: 0.4362, DER: 0.1729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:04<00:00, 19.31it/s, val_loss=0.2129, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3246, WER: 0.6064, DER: 0.2905\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.54it/s, loss=0.2151, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.1532, WER: 0.4894, DER: 0.2176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 30.85it/s, val_loss=0.2564, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3675, WER: 0.5319, DER: 0.2279\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 15.87it/s, loss=0.1999, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.1341, WER: 0.4894, DER: 0.2066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 31.48it/s, val_loss=0.2186, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3558, WER: 0.5957, DER: 0.2571\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 15.75it/s, loss=0.1166, WER=1.0000, DER=0.7500]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.1127, WER: 0.4362, DER: 0.1590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 31.61it/s, val_loss=0.2767, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.3656, WER: 0.7021, DER: 0.3164\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.88it/s, loss=0.0343, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.1038, WER: 0.3404, DER: 0.1410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 27.31it/s, val_loss=0.2441, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4015, WER: 0.5638, DER: 0.2433\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.23it/s, loss=0.1118, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0922, WER: 0.3511, DER: 0.1766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.22it/s, val_loss=0.2996, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4141, WER: 0.5638, DER: 0.2758\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:06<00:00, 15.27it/s, loss=0.0990, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0793, WER: 0.4043, DER: 0.1410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 31.73it/s, val_loss=0.2793, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4075, WER: 0.5319, DER: 0.2456\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 16.14it/s, loss=0.0711, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0710, WER: 0.4043, DER: 0.1474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 24.67it/s, val_loss=0.2768, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4505, WER: 0.5851, DER: 0.2501\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.31it/s, loss=0.0425, WER=1.0000, DER=0.2500]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0610, WER: 0.2340, DER: 0.0894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 32.05it/s, val_loss=0.3104, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4580, WER: 0.5638, DER: 0.2579\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 15.70it/s, loss=0.0392, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0547, WER: 0.3085, DER: 0.1370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.92it/s, val_loss=0.4267, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4938, WER: 0.5745, DER: 0.2606\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:06<00:00, 15.54it/s, loss=0.0625, WER=1.0000, DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0440, WER: 0.4255, DER: 0.1883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 31.23it/s, val_loss=0.3420, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.4945, WER: 0.5532, DER: 0.2452\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 17.35it/s, loss=0.0325, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0408, WER: 0.2979, DER: 0.1285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:03<00:00, 26.91it/s, val_loss=0.4392, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.5612, WER: 0.6064, DER: 0.2723\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:05<00:00, 16.87it/s, loss=0.0234, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0372, WER: 0.2766, DER: 0.1445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 32.32it/s, val_loss=0.3743, val_WER=1.0000, val_DER=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.5768, WER: 0.6170, DER: 0.2780\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:06<00:00, 15.46it/s, loss=0.0334, WER=0.0000, DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Loss: 0.0350, WER: 0.3298, DER: 0.1523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 94/94 [00:02<00:00, 33.32it/s, val_loss=0.4802, val_WER=0.0000, val_DER=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Loss: 0.6246, WER: 0.5745, DER: 0.2690\n",
            "Model saved to checkpoints/final_model.pt and final_model.pt\n",
            "\n",
            "Testing model on examples:\n",
            "Input: مقدمة\n",
            "Prediction: مُقْدَمَةٍ\n",
            "\n",
            "Input: الطبري\n",
            "Prediction: الطُبَرِيُ\n",
            "\n",
            "Input: شيخ\n",
            "Prediction: شَيْخُ\n",
            "\n",
            "Input: الدين\n",
            "Prediction: الدَيْنِ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRip94S8Bvut"
      },
      "source": [
        "### Testing Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_model(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Load a saved model\"\"\"\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Create model with saved config\n",
        "    model = CombinedDiacritizationModel(**checkpoint[\"model_config\"])\n",
        "\n",
        "    # Load state dict\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Move to correct device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"Loaded model from {model_path}\")\n",
        "    return model\n",
        "\n",
        "def diacritize_sentence(model: CombinedDiacritizationModel, sentence: str, actual_sentence: Optional[str] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Diacritize a full sentence by processing each word and calculate metrics if actual sentence is provided\n",
        "    Args:\n",
        "        model: The diacritization model\n",
        "        sentence: Input sentence to diacritize\n",
        "        actual_sentence: Optional actual standard sentence for metric calculation\n",
        "    Returns:\n",
        "        Dictionary containing diacritized text and metrics if actual_sentence is provided\n",
        "    \"\"\"\n",
        "    words = sentence.strip().split()\n",
        "    diacritized_words = []\n",
        "\n",
        "    # Word-level metrics\n",
        "    total_wer = 0\n",
        "    total_der = 0\n",
        "    word_metrics = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word:  # Skip empty strings\n",
        "            diacritized = model.predict(word)\n",
        "            diacritized_words.append(diacritized)\n",
        "\n",
        "            # Calculate metrics if actual sentence is provided\n",
        "            if actual_sentence:\n",
        "                actual_words = actual_sentence.strip().split()\n",
        "                if i < len(actual_words):\n",
        "                    metrics = model.evaluate_text(actual_words[i], diacritized)\n",
        "                    total_wer += metrics[\"WER\"]\n",
        "                    total_der += metrics[\"DER\"]\n",
        "                    word_metrics.append({\n",
        "                        \"word\": word,\n",
        "                        \"prediction\": diacritized,\n",
        "                        \"actual\": actual_words[i],\n",
        "                        \"WER\": metrics[\"WER\"],\n",
        "                        \"DER\": metrics[\"DER\"]\n",
        "                    })\n",
        "\n",
        "    diacritized_text = \" \".join(diacritized_words)\n",
        "\n",
        "    result = {\n",
        "        \"input\": sentence,\n",
        "        \"prediction\": diacritized_text,\n",
        "    }\n",
        "\n",
        "    if actual_sentence:\n",
        "        result.update({\n",
        "            \"actual\": actual_sentence,\n",
        "            \"metrics\": {\n",
        "                \"WER\": total_wer / len(words),\n",
        "                \"DER\": total_der / len(words)\n",
        "            },\n",
        "            \"word_metrics\": word_metrics\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "def test_from_file(model: CombinedDiacritizationModel,\n",
        "                  test_file: str = \"arabic_data/test/test_clean.txt\",\n",
        "                  actual_file: str = \"arabic_data/test/test.txt\",\n",
        "                  max_samples: Optional[int] = 100):  # Test only max_samples sentences\n",
        "    \"\"\"Test model on a subset of sentences from test file\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_wer = 0\n",
        "    total_der = 0\n",
        "    num_sentences = 0\n",
        "    results = []\n",
        "\n",
        "    print(f\"Testing model on {max_samples} sentences from test file...\")\n",
        "\n",
        "    try:\n",
        "        with open(test_file, 'r', encoding='utf-8') as test_f, \\\n",
        "             open(actual_file, 'r', encoding='utf-8') as actual_f:\n",
        "\n",
        "            # Read test lines\n",
        "            test_lines = test_f.readlines()\n",
        "            actual_lines = actual_f.readlines()\n",
        "\n",
        "            # Take subset if max_samples specified\n",
        "            if max_samples:\n",
        "                test_lines = test_lines[:max_samples]\n",
        "                actual_lines = actual_lines[:max_samples]\n",
        "\n",
        "            # Process each line\n",
        "            for test_line, actual_line in tqdm(zip(test_lines, actual_lines)):\n",
        "                test_word = test_line.strip()\n",
        "                actual_word = actual_line.strip()\n",
        "\n",
        "                if not test_word or not actual_word:  # Skip empty lines\n",
        "                    continue\n",
        "\n",
        "                # Get prediction\n",
        "                with torch.no_grad():\n",
        "                    pred_word = model.predict(test_word)\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = model.evaluate_text(actual_word, pred_word)\n",
        "\n",
        "                total_wer += metrics[\"WER\"]\n",
        "                total_der += metrics[\"DER\"]\n",
        "                num_sentences += 1\n",
        "\n",
        "                # Store result\n",
        "                results.append({\n",
        "                    \"input\": test_word,\n",
        "                    \"prediction\": pred_word,\n",
        "                    \"actual\": actual_word,\n",
        "                    \"WER\": metrics[\"WER\"],\n",
        "                    \"DER\": metrics[\"DER\"]\n",
        "                })\n",
        "\n",
        "                # Print some examples\n",
        "                if num_sentences <= 5:  # Show first 5 examples\n",
        "                    print(f\"\\nInput: {test_word}\")\n",
        "                    print(f\"Prediction: {pred_word}\")\n",
        "                    print(f\"Actual: {actual_word}\")\n",
        "                    print(f\"WER: {metrics['WER']:.4f}, DER: {metrics['DER']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during testing: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = {\n",
        "        \"WER\": total_wer / num_sentences,\n",
        "        \"DER\": total_der / num_sentences\n",
        "    }\n",
        "\n",
        "    # Save detailed results\n",
        "    with open(\"test_results_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"average_metrics\": avg_metrics,\n",
        "            \"results\": results\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nOverall Results:\")\n",
        "    print(f\"Tested on {num_sentences} words\")\n",
        "    print(f\"Average WER: {avg_metrics['WER']:.4f}\")\n",
        "    print(f\"Average DER: {avg_metrics['DER']:.4f}\")\n",
        "\n",
        "    return avg_metrics, results\n",
        "\n",
        "def test_custom_sentences(model: CombinedDiacritizationModel,\n",
        "                         sentences: List[str],\n",
        "                         actual_sentences: Optional[List[str]] = None):\n",
        "    \"\"\"\n",
        "    Test the model on custom input sentences\n",
        "    Args:\n",
        "        model: The diacritization model\n",
        "        sentences: List of input sentences\n",
        "        actual_sentences: Optional list of actual standard sentences for metric calculation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    print(\"\\nTesting custom sentences:\")\n",
        "    results = []\n",
        "    total_wer = 0\n",
        "    total_der = 0\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Get prediction with metrics if actual sentence is available\n",
        "        actual_sentence = actual_sentences[i] if actual_sentences and i < len(actual_sentences) else None\n",
        "        result = diacritize_sentence(model, sentence, actual_sentence)\n",
        "        results.append(result)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nInput: {sentence}\")\n",
        "        print(f\"Prediction: {result['prediction']}\")\n",
        "\n",
        "        if actual_sentence:\n",
        "            print(f\"Actual: {actual_sentence}\")\n",
        "            print(f\"Sentence WER: {result['metrics']['WER']:.4f}\")\n",
        "            print(f\"Sentence DER: {result['metrics']['DER']:.4f}\")\n",
        "\n",
        "            # Print word-level metrics\n",
        "            print(\"\\nWord-level metrics:\")\n",
        "            for word_metric in result[\"word_metrics\"]:\n",
        "                print(f\"Word: {word_metric['word']}\")\n",
        "                print(f\"Prediction: {word_metric['prediction']}\")\n",
        "                print(f\"Actual: {word_metric['actual']}\")\n",
        "                print(f\"DER: {word_metric['DER']:.4f}\")\n",
        "                print()\n",
        "\n",
        "            total_wer += result['metrics']['WER']\n",
        "            total_der += result['metrics']['DER']\n",
        "\n",
        "    # Calculate and print average metrics if actual sentences were provided\n",
        "    if actual_sentences:\n",
        "        avg_metrics = {\n",
        "            \"WER\": total_wer / len(sentences),\n",
        "            \"DER\": total_der / len(sentences)\n",
        "        }\n",
        "        print(\"\\nOverall Metrics:\")\n",
        "        print(f\"Average WER: {avg_metrics['WER']:.4f}\")\n",
        "        print(f\"Average DER: {avg_metrics['DER']:.4f}\")\n",
        "\n",
        "        # Save detailed results\n",
        "        with open(\"sentence_test_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\n",
        "                \"average_metrics\": avg_metrics,\n",
        "                \"results\": results\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the model\n",
        "    model = load_saved_model(\"final_model.pt\")\n",
        "\n",
        "    # Test custom sentences with actual standard\n",
        "    test_sentences = [\n",
        "        \"مقدمة الطبري شيخ الدين\",\n",
        "        \"فجاء فيه بالعجب العجاب\",\n",
        "        \"ونثر فيه ألباب الألباب\"\n",
        "    ]\n",
        "\n",
        "    actual_sentences = [\n",
        "        \"مُقَدِّمَةُ الطَّبَرِيِّ شَيْخِ الدِّينِ\",\n",
        "        \"فَجَاءَ فِيهِ بِالْعَجَبِ الْعُجَابِ\",\n",
        "        \"وَنَثَرَ فِيهِ أَلْبَابَ الْأَلْبَابِ\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting custom sentences:\")\n",
        "    results = test_custom_sentences(model, test_sentences, actual_sentences)\n",
        "\n",
        "    # Test on subset of test file\n",
        "    print(\"\\nTesting on subset of test set:\")\n",
        "    metrics, file_results = test_from_file(\n",
        "        model,\n",
        "        max_samples=1000  # Test only 100 samples\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wts_Z5yCadIv",
        "outputId": "1e7f259d-32a8-4f98-f1a7-75dd54e67cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-9d3f0a66a93c>:4: FutureWarning:\n",
            "\n",
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing 3-gram model\n",
            "Loaded model from final_model.pt\n",
            "\n",
            "Testing custom sentences:\n",
            "\n",
            "Testing custom sentences:\n",
            "\n",
            "Input: مقدمة الطبري شيخ الدين\n",
            "Prediction: مُقْدَمَةٍ الطُبَرِيُ شَيْخُ الدَيْنِ\n",
            "Actual: مُقَدِّمَةُ الطَّبَرِيِّ شَيْخِ الدِّينِ\n",
            "Sentence WER: 1.0000\n",
            "Sentence DER: 0.4167\n",
            "\n",
            "Word-level metrics:\n",
            "Word: مقدمة\n",
            "Prediction: مُقْدَمَةٍ\n",
            "Actual: مُقَدِّمَةُ\n",
            "DER: 0.6000\n",
            "\n",
            "Word: الطبري\n",
            "Prediction: الطُبَرِيُ\n",
            "Actual: الطَّبَرِيِّ\n",
            "DER: 0.3333\n",
            "\n",
            "Word: شيخ\n",
            "Prediction: شَيْخُ\n",
            "Actual: شَيْخِ\n",
            "DER: 0.3333\n",
            "\n",
            "Word: الدين\n",
            "Prediction: الدَيْنِ\n",
            "Actual: الدِّينِ\n",
            "DER: 0.4000\n",
            "\n",
            "\n",
            "Input: فجاء فيه بالعجب العجاب\n",
            "Prediction: فَجَاءَ فِيهِ بِالْعَجْبِ الْعَجَابِ\n",
            "Actual: فَجَاءَ فِيهِ بِالْعَجَبِ الْعُجَابِ\n",
            "Sentence WER: 0.5000\n",
            "Sentence DER: 0.0833\n",
            "\n",
            "Word-level metrics:\n",
            "Word: فجاء\n",
            "Prediction: فَجَاءَ\n",
            "Actual: فَجَاءَ\n",
            "DER: 0.0000\n",
            "\n",
            "Word: فيه\n",
            "Prediction: فِيهِ\n",
            "Actual: فِيهِ\n",
            "DER: 0.0000\n",
            "\n",
            "Word: بالعجب\n",
            "Prediction: بِالْعَجْبِ\n",
            "Actual: بِالْعَجَبِ\n",
            "DER: 0.1667\n",
            "\n",
            "Word: العجاب\n",
            "Prediction: الْعَجَابِ\n",
            "Actual: الْعُجَابِ\n",
            "DER: 0.1667\n",
            "\n",
            "\n",
            "Input: ونثر فيه ألباب الألباب\n",
            "Prediction: وَنَثْرُ فِيهِ أَلْبَابِ الْأَلْبَابِ\n",
            "Actual: وَنَثَرَ فِيهِ أَلْبَابَ الْأَلْبَابِ\n",
            "Sentence WER: 0.5000\n",
            "Sentence DER: 0.1750\n",
            "\n",
            "Word-level metrics:\n",
            "Word: ونثر\n",
            "Prediction: وَنَثْرُ\n",
            "Actual: وَنَثَرَ\n",
            "DER: 0.5000\n",
            "\n",
            "Word: فيه\n",
            "Prediction: فِيهِ\n",
            "Actual: فِيهِ\n",
            "DER: 0.0000\n",
            "\n",
            "Word: ألباب\n",
            "Prediction: أَلْبَابِ\n",
            "Actual: أَلْبَابَ\n",
            "DER: 0.2000\n",
            "\n",
            "Word: الألباب\n",
            "Prediction: الْأَلْبَابِ\n",
            "Actual: الْأَلْبَابِ\n",
            "DER: 0.0000\n",
            "\n",
            "\n",
            "Overall Metrics:\n",
            "Average WER: 0.6667\n",
            "Average DER: 0.2250\n",
            "\n",
            "Testing on subset of test set:\n",
            "Testing model on 1000 sentences from test file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: إلخ\n",
            "Prediction: إلْخَ\n",
            "Actual: إلَخْ\n",
            "WER: 1.0000, DER: 0.6667\n",
            "\n",
            "Input: أي\n",
            "Prediction: أَيْ\n",
            "Actual: أَيْ\n",
            "WER: 0.0000, DER: 0.0000\n",
            "\n",
            "Input: فلا\n",
            "Prediction: فَلَا\n",
            "Actual: فَلَا\n",
            "WER: 0.0000, DER: 0.0000\n",
            "\n",
            "Input: تنفذ\n",
            "Prediction: تَنْفَذَ\n",
            "Actual: تَنْفُذُ\n",
            "WER: 1.0000, DER: 0.5000\n",
            "\n",
            "Input: وصيته\n",
            "Prediction: وَصِيَتُهُ\n",
            "Actual: وَصِيَّتُهُ\n",
            "WER: 0.0000, DER: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [00:09, 103.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Results:\n",
            "Tested on 1000 words\n",
            "Average WER: 0.5480\n",
            "Average DER: 0.2116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing for RNN and BiLSTM"
      ],
      "metadata": {
        "id": "rwToeMTlNj1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "VU5H0m34wr5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits data into a list of sentences, clauses, or lines shorter than 500 characters.\n",
        "def split_data(data_raw):\n",
        "    # initialize returned list\n",
        "    data_new = list()\n",
        "\n",
        "    # create new lines at paranthesis or puctuation\n",
        "    for line in data_raw:\n",
        "        line = line.replace('.', '.\\n')\n",
        "        line = line.replace(',', ',\\n')\n",
        "        line = line.replace('،', '،\\n')\n",
        "        line = line.replace(':', ':\\n')\n",
        "        line = line.replace(';', ';\\n')\n",
        "        line = line.replace('؛', '؛\\n')\n",
        "        line = line.replace('(', '\\n(')\n",
        "        line = line.replace(')', ')\\n')\n",
        "        line = line.replace('[', '\\n[')\n",
        "        line = line.replace(']', ']\\n')\n",
        "        line = line.replace('{', '\\n{')\n",
        "        line = line.replace('}', '}\\n')\n",
        "        line = line.replace('«', '\\n«')\n",
        "        line = line.replace('»', '»\\n')\n",
        "\n",
        "\n",
        "        # loop on created new lines\n",
        "        for sub_line in line.split('\\n'):\n",
        "            # do nothing if line is empty\n",
        "            if len(remove_diacritics(sub_line).strip()) == 0:\n",
        "                continue\n",
        "            # append line to list if line, without diacritics, is shorter than 500 characters\n",
        "            if len(remove_diacritics(sub_line).strip()) > 0 and len(remove_diacritics(sub_line).strip()) <= 500:\n",
        "                data_new.append(sub_line.strip())\n",
        "\n",
        "            # split line if its longer than 500 characters\n",
        "            else:\n",
        "                sub_line = sub_line.split()\n",
        "                tmp_line = ''\n",
        "                for word in sub_line:\n",
        "                    # append line without current word if new word will make it exceed 500 characters and start new line\n",
        "                    if len(remove_diacritics(tmp_line).strip()) + len(remove_diacritics(word).strip()) + 1 > 500:\n",
        "                        if len(remove_diacritics(tmp_line).strip()) > 0:\n",
        "                            data_new.append(tmp_line.strip())\n",
        "                        tmp_line = word\n",
        "                    else:\n",
        "                        # set new line to current word if line is still empty\n",
        "                        if tmp_line == '':\n",
        "                            tmp_line = word\n",
        "                        # add whitespace and word to line if line is not empty but shorter than 500 characters\n",
        "                        else:\n",
        "                            tmp_line += ' '\n",
        "                            tmp_line += word\n",
        "                if len(remove_diacritics(tmp_line).strip()) > 0:\n",
        "                    data_new.append(tmp_line.strip())\n",
        "\n",
        "    return data_new"
      ],
      "metadata": {
        "id": "UqtHaxeYNrU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits data lines into an array of characters as integers and an array of diacritics as one-hot encodings.\n",
        "def map_data(data_raw):\n",
        "\n",
        "    # Initialize data and diacritics lists\n",
        "    X = list()\n",
        "    Y = list()\n",
        "\n",
        "    # Loop on data lines\n",
        "    for line in data_raw:\n",
        "        # Initialize line data and diacritics lists and add start of sentence character\n",
        "        x = [CHARACTERS_MAPPING['<SOS>']]\n",
        "        y = [CLASSES_MAPPING['<SOS>']]\n",
        "\n",
        "        # Loop on all characters in line\n",
        "        for idx, char in enumerate(line):\n",
        "            # Skip character if it is only a diacritic\n",
        "            if char in DIACRITICS_LIST:\n",
        "                continue\n",
        "            # Append character mapping to data list\n",
        "            x.append(CHARACTERS_MAPPING[char])\n",
        "\n",
        "            # If character is not an Arabic letter, append whitespace to diacritics list\n",
        "            if char not in ARABIC_LETTERS_LIST:\n",
        "                y.append(CLASSES_MAPPING[''])\n",
        "            # If character is an Arabic letter, append its diacritics to the diacritics list\n",
        "            else:\n",
        "                char_diac = ''\n",
        "                if idx + 1 < len(line) and line[idx + 1] in DIACRITICS_LIST:\n",
        "                    char_diac = line[idx + 1]\n",
        "                    if idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and char_diac + line[idx + 2] in CLASSES_MAPPING:\n",
        "                        char_diac += line[idx + 2]\n",
        "                    elif idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and line[idx + 2] + char_diac in CLASSES_MAPPING:\n",
        "                        char_diac = line[idx + 2] + char_diac\n",
        "                y.append(CLASSES_MAPPING[char_diac])\n",
        "\n",
        "        # Assert characters list length equals diacritics list length\n",
        "        assert(len(x) == len(y))\n",
        "\n",
        "        # Append end of sentence character\n",
        "        x.append(CHARACTERS_MAPPING['<EOS>'])\n",
        "        y.append(CLASSES_MAPPING['<EOS>'])\n",
        "\n",
        "        # Convert diacritics integers to one_hot_encodings\n",
        "        y = to_categorical(y, len(CLASSES_MAPPING))\n",
        "\n",
        "        # Append line's data and diacritics lists to total data and diacritics lists\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "\n",
        "    # Determine the maximum length for padding\n",
        "    max_length = max(len(seq) for seq in X)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, maxlen=max_length, padding='post')\n",
        "    Y = pad_sequences(Y, maxlen=max_length, padding='post', dtype='float32')\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "rEPutnuNNuSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform split_data on training and validation data\n",
        "train_split = split_data(train_raw)\n",
        "val_split = split_data(val_raw)\n",
        "\n",
        "#Iterate over each element in the train_split list and apply clean_text function.\n",
        "for i in range(len(train_split)):\n",
        "    train_split[i] =  clean_text(train_split[i])\n",
        "\n",
        "#Iterate over each element in the val_split list and apply clean_text function.\n",
        "for i in range(len(val_split)):\n",
        "    val_split[i] =  clean_text(val_split[i])\n",
        "\n",
        "print('The data before cleaning: ',train_raw[0])\n",
        "print('The data after cleaning: ',train_split[0])"
      ],
      "metadata": {
        "id": "ZU_Ydf6-Nw_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccd1384-e207-4e0b-9eeb-f8020deb02d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data before cleaning:  وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ، أَوْ مِنْ الثَّانِيَةِ ، فَإِنْ لَمْ يَطُلْ تَدَارَكَ ، وَإِلَّا فَبَاطِلَةٌ وَلَا جَمَعَ ، وَلَوْ جَهِلَ أَعَادَهُمَا لِوَقْتَيْهِمَا\n",
            "\n",
            "The data after cleaning:  وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ،\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data generator will help us load the data in batches instead of loading it all at once, this will make it more efficient for the model."
      ],
      "metadata": {
        "id": "hy3B9TLTXzRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    ''' Costumized data generator to input line batches into the model '''\n",
        "    def __init__(self, lines, batch_size):\n",
        "        self.lines = lines\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch, Y_batch = map_data(lines)\n",
        "\n",
        "        X_max_seq_len = np.max([len(x) for x in X_batch])\n",
        "        Y_max_seq_len = np.max([len(y) for y in Y_batch])\n",
        "\n",
        "        assert(X_max_seq_len == Y_max_seq_len)\n",
        "\n",
        "        X = list()\n",
        "        for x in X_batch:\n",
        "            x = list(x)\n",
        "            x.extend([CHARACTERS_MAPPING['<PAD>']] * (X_max_seq_len - len(x)))\n",
        "            X.append(np.asarray(x))\n",
        "\n",
        "        Y_tmp = list()\n",
        "        for y in Y_batch:\n",
        "            y_new = list(y)\n",
        "            y_new.extend(to_categorical([CLASSES_MAPPING['<PAD>']] * (Y_max_seq_len - len(y)), len(CLASSES_MAPPING)))\n",
        "            Y_tmp.append(np.asarray(y_new))\n",
        "        Y_batch = Y_tmp\n",
        "\n",
        "        Y_batch = np.array(Y_batch)\n",
        "\n",
        "        return (np.array(X), Y_batch)"
      ],
      "metadata": {
        "id": "92RaAXPfKnBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Script"
      ],
      "metadata": {
        "id": "acmOxZoKxJcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are fitting the model, we specify the numebr of epochs, batch size, and the splits of both training and validation sets. It fits the splits using the generator code up."
      ],
      "metadata": {
        "id": "0rnXtAf6bC1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(model, epochs, batch_size, train_split,val_split):\n",
        "    ''' Fits model '''\n",
        "\n",
        "\n",
        "    # create training and validation generators\n",
        "    training_generator = DataGenerator(train_split, batch_size)\n",
        "    val_generator = DataGenerator(val_split, batch_size)\n",
        "\n",
        "\n",
        "    # fit model\n",
        "    history = model.fit(x=training_generator,\n",
        "              validation_data=val_generator,\n",
        "              epochs=epochs\n",
        "              )\n",
        "\n",
        "    # return history\n",
        "    return history"
      ],
      "metadata": {
        "id": "g8BjWIp1KufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Script"
      ],
      "metadata": {
        "id": "HwSgLjRz1qJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(line, model):\n",
        "    ''' predict test line '''\n",
        "    X, _ = map_data([line])\n",
        "    predictions = model.predict(X).squeeze()\n",
        "    # get most probable diacritizations for each character\n",
        "    predictions = predictions[1:]\n",
        "\n",
        "    # initialize empty output line\n",
        "    output = ''\n",
        "    # loop on input characters and predicted diacritizations\n",
        "    for char, prediction in zip(remove_diacritics(line), predictions):\n",
        "        # append character\n",
        "        output += char\n",
        "        # if character is not an arabic letter continue\n",
        "        if char not in ARABIC_LETTERS_LIST:\n",
        "            continue\n",
        "\n",
        "        if '<' in REV_CLASSES_MAPPING[np.argmax(prediction)]:\n",
        "            continue\n",
        "\n",
        "        # if character in arabic letters append predicted diacritization\n",
        "        output += REV_CLASSES_MAPPING[np.argmax(prediction)]\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "fVhdK698WXrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Models **(no need to re-train / run cells)**"
      ],
      "metadata": {
        "id": "3c1bk6OR5Z5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple RNN Model Architecture"
      ],
      "metadata": {
        "id": "tAZk8qhbv0Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model2():\n",
        "\n",
        "    inputs = Input(shape=(None,))\n",
        "\n",
        "\n",
        "    embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING),\n",
        "                           output_dim=25,\n",
        "                           embeddings_initializer=glorot_normal(seed=961))(inputs)\n",
        "\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128,\n",
        "                               return_sequences=True,\n",
        "                               kernel_initializer=glorot_normal(seed=961)))(embeddings)\n",
        "\n",
        "\n",
        "    dropout = Dropout(0.3)(blstm)\n",
        "\n",
        "\n",
        "    dense = TimeDistributed(Dense(units=256,\n",
        "                                  activation='relu',\n",
        "                                  kernel_initializer=glorot_normal(seed=961)))(dropout)\n",
        "\n",
        "\n",
        "    output = TimeDistributed(Dense(units=len(CLASSES_MAPPING),\n",
        "                                   activation='softmax',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dense)\n",
        "\n",
        "\n",
        "    model = Model(inputs, output)\n",
        "\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "gS6BzUTpmXAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the simple model2\n",
        "model2 = create_model2()\n",
        "model2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "llU90LXHpgrz",
        "outputId": "4fec35a0-f440-44e7-e5ea-91b5b152f7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)            │           \u001b[38;5;34m1,925\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m157,696\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_8 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_9 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)            │           \u001b[38;5;34m4,883\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,925</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">157,696</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,883</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m230,296\u001b[0m (899.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">230,296</span> (899.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m230,296\u001b[0m (899.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">230,296</span> (899.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist_2 = fit_model(model2, 5, 256, train_split, val_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L66HNgam2BQ",
        "outputId": "7def3d4f-6e0b-4b21-a043-c5c6c27f73c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 2s/step - loss: 0.1274 - val_loss: 0.0551\n",
            "Epoch 2/5\n",
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 2s/step - loss: 0.0502 - val_loss: 0.0367\n",
            "Epoch 3/5\n",
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 2s/step - loss: 0.0368 - val_loss: 0.0296\n",
            "Epoch 4/5\n",
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 2s/step - loss: 0.0302 - val_loss: 0.0262\n",
            "Epoch 5/5\n",
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 2s/step - loss: 0.0273 - val_loss: 0.0233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save(\"model2.h5\")\n",
        "model2.save_weights('model2.weights.h5')"
      ],
      "metadata": {
        "id": "17wMvnYuoJXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756183a1-b2c9-4d67-a0ea-9445969a8ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = load_model('model2.h5')\n",
        "model2.load_weights('/content/model2.weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiazs9_NUClF",
        "outputId": "96b2e458-6d4f-48e1-dca5-ed7c1c29420b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist_2.history['loss'])\n",
        "plt.plot(hist_2.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "8s_mVALJTcQd",
        "outputId": "f089504b-ab6d-4f14-927c-9c958072ec69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('اكل محمد التفاحة',model2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5QOodMGTwQ-",
        "outputId": "068ddee1-3959-4832-b1b6-f70b1ce1a12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step\n",
            "اكْلٌ مُحَمَّدِ التَّفَاحَةِ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('الحمدلله رب العالمين',model2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXVR7s3rT_Ck",
        "outputId": "e2aad843-8fe5-4b37-fc91-1a833020b130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505ms/step\n",
            "الْحَمْدَلِلَهُ رَبُّ الْعَالِمِينَ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BiLSTM"
      ],
      "metadata": {
        "id": "vtUnWyf_28gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model architecture was taken by Fadel, Ali, Ibraheem et al."
      ],
      "metadata": {
        "id": "y-wYj8xPl4Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    # Creates diacritization model\n",
        "\n",
        "\n",
        "    SelectedLSTM = LSTM\n",
        "\n",
        "    inputs = Input(shape=(None,))\n",
        "\n",
        "    embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING),\n",
        "                           output_dim=25,\n",
        "                           embeddings_initializer=glorot_normal(seed=961))(inputs)\n",
        "\n",
        "    blstm1 = Bidirectional(SelectedLSTM(units=256,\n",
        "                                     return_sequences=True,\n",
        "                                     kernel_initializer=glorot_normal(seed=961)))(embeddings)\n",
        "    dropout1 = Dropout(0.5)(blstm1)\n",
        "    blstm2 = Bidirectional(SelectedLSTM(units=256,\n",
        "                                     return_sequences=True,\n",
        "                                     kernel_initializer=glorot_normal(seed=961)))(dropout1)\n",
        "    dropout2 = Dropout(0.5)(blstm2)\n",
        "\n",
        "    dense1 = TimeDistributed(Dense(units=512,\n",
        "                                   activation='relu',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dropout2)\n",
        "    dense2 = TimeDistributed(Dense(units=512,\n",
        "                                   activation='relu',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dense1)\n",
        "\n",
        "    output = TimeDistributed(Dense(units=len(CLASSES_MAPPING),\n",
        "                                   activation='softmax',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dense2)\n",
        "\n",
        "    model = Model(inputs, output)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7EL6tiinEFoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Soq5OzosKkLH",
        "outputId": "4aece6ac-d90c-4e42-b5dc-5ac0bc4037ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)            │           \u001b[38;5;34m1,925\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │         \u001b[38;5;34m577,536\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m1,574,912\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_3 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)            │           \u001b[38;5;34m9,747\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,925</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">577,536</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,747</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,689,432\u001b[0m (10.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689,432</span> (10.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,689,432\u001b[0m (10.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689,432</span> (10.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to limited resource we decided to go with 2 epochs and batch size of 256 (we tried to run it with more epochs but it kept crashing)"
      ],
      "metadata": {
        "id": "zquhoj1Pbnh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_1 = fit_model(model, 2, 256, train_split, val_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpPEI1hnKx9Y",
        "outputId": "1fdfb505-b89e-4f55-c830-2c639324993d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1807s\u001b[0m 6s/step - loss: 0.1366 - val_loss: 0.0471\n",
            "Epoch 2/2\n",
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1821s\u001b[0m 6s/step - loss: 0.0422 - val_loss: 0.0284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The loss value of the model after the training stage is :',hist_1.history['val_loss'][1])"
      ],
      "metadata": {
        "id": "lELh9oH9RAw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57eb5b18-2845-4284-da1b-f626c0c2368f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss value of the model after the training stage is : 0.028357742354273796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model1.h5\")"
      ],
      "metadata": {
        "id": "K3qmNkiIcXac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1b6b8d-20dc-42d9-e750-925c9f985743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('model1.weights.h5')"
      ],
      "metadata": {
        "id": "CXpQXdiqchQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist_1.history['loss'])\n",
        "plt.plot(hist_1.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRpdWaFYLv7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "394f2dbc-785d-48a0-ebcf-ef55f4e8b257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we go through the epochs, both training and validation loss decrease"
      ],
      "metadata": {
        "id": "0MHS3PYLtYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('model1.h5')\n",
        "model.load_weights('/content/model1.weights.h5')"
      ],
      "metadata": {
        "id": "pj09X5b5cKwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd38fdf-83cd-43a3-e959-f5b9af9b50e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 40 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('اكل محمد التفاحة',model))"
      ],
      "metadata": {
        "id": "B5MOakL2Wa5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45dd510-f68d-4588-84d8-64cc73e02957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "اكِلُ مُحَمَّدُ التَّفَاحَةِ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('الحمدلله رب العالمين',model))"
      ],
      "metadata": {
        "id": "d0owSya1eLGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce825484-7b5e-4cc4-a389-f34f26329c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "الْحَمْدِلُلُهُ رَبُّ الْعَالِمِيْنِ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation For All **(run all cells until BiLSTM for this to work)**"
      ],
      "metadata": {
        "id": "Trw1jYUG-FoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"arabic_data/test/test.txt\"\n",
        "output_file = \"arabic_data/test/test_first_2000.txt\"\n",
        "input_file1 = \"arabic_data/test/test_clean.txt\"\n",
        "output_file1 = \"arabic_data/test/test_clean_first_2000.txt\"\n",
        "\n",
        "take_first_n_lines(input_file, output_file, n=2000)\n",
        "take_first_n_lines(input_file1, output_file1, n=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QoHit1MC8Cf",
        "outputId": "ecd0fb6e-c107-4414-a122-b5926b987357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully wrote first 2000 lines to arabic_data/test/test_first_2000.txt\n",
            "Successfully wrote first 2000 lines to arabic_data/test/test_clean_first_2000.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseEvaluator:\n",
        "    \"\"\"Abstract base class for evaluation.\"\"\"\n",
        "    def load_model(self, model_path: str):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, text: str) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def evaluate_text(self, gold: str, pred: str) -> dict:\n",
        "        \"\"\"\n",
        "        Calculate WER and DER for a single pair of gold and predicted text.\n",
        "        \"\"\"\n",
        "        wer = evaluate_word(gold, pred)\n",
        "        der = self.calculate_der(gold, pred)\n",
        "        return {\"WER\": wer, \"DER\": der}\n",
        "\n",
        "    def calculate_der(self, gold: str, pred: str) -> float:\n",
        "        \"\"\"Custom implementation for calculating DER.\"\"\"\n",
        "        gold_pairs = word_iterator(gold)\n",
        "        pred_pairs = word_iterator(pred)\n",
        "\n",
        "        min_len = min(len(gold_pairs), len(pred_pairs))\n",
        "        gold_pairs = gold_pairs[:min_len]\n",
        "        pred_pairs = pred_pairs[:min_len]\n",
        "\n",
        "        incorrect = sum(1 for (_, g_d), (_, p_d) in zip(gold_pairs, pred_pairs) if g_d != p_d)\n",
        "        return incorrect / len(gold_pairs) if gold_pairs else 0.0\n",
        "\n",
        "\n",
        "class CBHGHMMModelEvaluator(BaseEvaluator):\n",
        "    def __init__(self, device: str = None):\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "\n",
        "    def load_model(self, model_path: str):\n",
        "        checkpoint = torch.load(model_path)\n",
        "        self.model = CombinedDiacritizationModel(**checkpoint[\"model_config\"])\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        print(f\"Loaded neural model from {model_path}\")\n",
        "\n",
        "    def predict(self, text: str) -> str:\n",
        "        return self.model.predict(text)\n",
        "\n",
        "\n",
        "class HMMModelEvaluator(BaseEvaluator):\n",
        "    def __init__(self):\n",
        "        self.hmm_model = HMM().load_trained_model(\"models/5gram_model.pkl\")\n",
        "\n",
        "    def predict(self, text: str) -> str:\n",
        "        if not text:  # Handle None or empty text\n",
        "            return \"\"\n",
        "        return self.hmm_model.diacritize_word(text)\n",
        "\n",
        "    def predict_sentence(self, text: str) -> str:\n",
        "        if not text:  # Handle None or empty text\n",
        "            return \"\"\n",
        "        return self.hmm_model.diacritize_sentence(text)\n",
        "\n",
        "class LSTMEvaluator(BaseEvaluator):\n",
        "    def __init__(self, model_path: str, weights_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the LSTMEvaluator with the model and weights.\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.weights_path = weights_path\n",
        "        self.model = None\n",
        "        self.load_model(model_path, weights_path)\n",
        "\n",
        "    def load_model(self, model_path: str, weights_path: str):\n",
        "        \"\"\"\n",
        "        Load the TensorFlow LSTM model and its weights.\n",
        "        \"\"\"\n",
        "        self.model = load_model(model_path)\n",
        "        self.model.load_weights(weights_path)\n",
        "        print(f\"Loaded LSTM model from {model_path} with weights {weights_path}\")\n",
        "\n",
        "    def predict(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Predict diacritization for the given text using the LSTM model.\n",
        "        \"\"\"\n",
        "        if not text:  # Handle None or empty text\n",
        "            return \"\"\n",
        "\n",
        "        # Prepare data for the model (assumes map_data function is defined elsewhere)\n",
        "        X, _ = map_data([text])\n",
        "        predictions = self.model.predict(X).squeeze()[1:]  # Skip start token\n",
        "\n",
        "        # Generate predicted text with diacritization\n",
        "        output = \"\"\n",
        "        for char, prediction in zip(remove_diacritics(text), predictions):\n",
        "            output += char\n",
        "            if char not in ARABIC_LETTERS_LIST:\n",
        "                continue\n",
        "            if '<' in REV_CLASSES_MAPPING[np.argmax(prediction)]:\n",
        "                continue\n",
        "            output += REV_CLASSES_MAPPING[np.argmax(prediction)]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class EvaluationPipeline:\n",
        "    def __init__(self, evaluators: List[BaseEvaluator], test_sentences: Optional[List[str]] = None,\n",
        "                 actual_sentences: Optional[List[str]] = None, test_file: Optional[str] = None,\n",
        "                 actual_file: Optional[str] = None, max_samples: Optional[int] = None, use_sentences: bool = False):\n",
        "\n",
        "        self.evaluators = evaluators\n",
        "        self.test_sentences = test_sentences\n",
        "        self.actual_sentences = actual_sentences\n",
        "        self.test_file = test_file\n",
        "        self.actual_file = actual_file\n",
        "        self.max_samples = max_samples\n",
        "        self.use_sentences = use_sentences\n",
        "\n",
        "        # Load from files if sentences are not provided\n",
        "        if self.use_sentences:\n",
        "            if not self.test_sentences or not self.actual_sentences:\n",
        "                raise ValueError(\"Both test_sentences and actual_sentences must be provided when use_sentences is True.\")\n",
        "        else:\n",
        "            if not self.test_file or not self.actual_file:\n",
        "                raise ValueError(\"Either test_sentences or test_file must be provided.\")\n",
        "\n",
        "            if not self.test_sentences:\n",
        "                with open(self.test_file, 'r', encoding='utf-8') as test_f:\n",
        "                    self.test_sentences = test_f.readlines()\n",
        "\n",
        "            if not self.actual_sentences:\n",
        "                with open(self.actual_file, 'r', encoding='utf-8') as actual_f:\n",
        "                    self.actual_sentences = actual_f.readlines()\n",
        "\n",
        "        # Trim sentences if max_samples is provided\n",
        "        if self.max_samples:\n",
        "            self.test_sentences = self.test_sentences[:self.max_samples]\n",
        "            self.actual_sentences = self.actual_sentences[:self.max_samples]\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluate the models using the provided test and actual sentences.\n",
        "        \"\"\"\n",
        "        # Ensure the number of test and actual sentences are the same\n",
        "        if len(self.test_sentences) != len(self.actual_sentences):\n",
        "            raise ValueError(\"The number of test sentences does not match the number of actual sentences.\")\n",
        "\n",
        "        # Evaluate for each evaluator\n",
        "        for evaluator in self.evaluators:\n",
        "            print(f\"\\nEvaluating with {type(evaluator).__name__}\")\n",
        "            results = []\n",
        "            total_wer = 0\n",
        "            total_der = 0\n",
        "\n",
        "            for test_sentence, actual_sentence in zip(self.test_sentences, self.actual_sentences):\n",
        "                test_sentence = test_sentence.strip() if test_sentence else None\n",
        "                actual_sentence = actual_sentence.strip() if actual_sentence else None\n",
        "\n",
        "                if not test_sentence or not actual_sentence:\n",
        "                    continue  # Skip None or empty sentences\n",
        "\n",
        "                if isinstance(evaluator, CBHGHMMModelEvaluator):\n",
        "                    pred = evaluator.predict(test_sentence)\n",
        "                elif isinstance(evaluator, HMMModelEvaluator) and self.test_sentences:\n",
        "                    pred = evaluator.predict_sentence(test_sentence)\n",
        "                else:\n",
        "                  pred = evaluator.predict(test_sentence)\n",
        "                metrics = evaluator.evaluate_text(actual_sentence, pred)\n",
        "\n",
        "                total_wer += metrics[\"WER\"]\n",
        "                total_der += metrics[\"DER\"]\n",
        "\n",
        "                results.append({\n",
        "                    \"input\": test_sentence,\n",
        "                    \"prediction\": pred,\n",
        "                    \"actual\": actual_sentence,\n",
        "                    \"metrics\": metrics\n",
        "                })\n",
        "\n",
        "            # Calculate averages\n",
        "            avg_wer = total_wer / len(results)\n",
        "            avg_der = total_der / len(results)\n",
        "\n",
        "            print(f\"Average WER: {avg_wer:.4f}\")\n",
        "            print(f\"Average DER: {avg_der:.4f}\")\n",
        "\n",
        "            # Save detailed results\n",
        "            with open(f\"{type(evaluator).__name__}_sentences{self.use_sentences}_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump({\"average_metrics\": {\"WER\": avg_wer, \"DER\": avg_der}, \"results\": results}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# Load evaluators\n",
        "neural_evaluator = CBHGHMMModelEvaluator()\n",
        "neural_evaluator.load_model(\"final_model.pt\")\n",
        "\n",
        "hmm_evaluator = HMMModelEvaluator()\n",
        "\n",
        "lstm_evaluator = LSTMEvaluator(\n",
        "    model_path=\"model1.h5\",\n",
        "    weights_path=\"/content/model1.weights.h5\"\n",
        ")\n",
        "\n",
        "lstm_evaluator2 = LSTMEvaluator(\n",
        "    model_path=\"model2.h5\",\n",
        "    weights_path=\"/content/model2.weights.h5\"\n",
        ")\n",
        "\n",
        "# Test sentences and actual sentences for direct input\n",
        "test_sentences = [\n",
        "    \"اللغة العربية جميلة\",\n",
        "    \"أنا أحب تعلم البرمجة\",\n",
        "    \"هذا نص للاختبار\"\n",
        "]\n",
        "\n",
        "actual_sentences = [\n",
        "    \"اللغة العربية رائعة\",\n",
        "    \"أنا أستمتع بتعلم البرمجة\",\n",
        "    \"هذا نص تجريبي للاختبار\"\n",
        "]\n",
        "\n",
        "# Run evaluation pipeline with files\n",
        "pipeline_with_files = EvaluationPipeline(\n",
        "    evaluators=[neural_evaluator, hmm_evaluator, lstm_evaluator, lstm_evaluator2],\n",
        "    test_file=\"arabic_data/test/test_clean_first_2000.txt\",\n",
        "    actual_file=\"arabic_data/test/test_first_2000.txt\",\n",
        "    max_samples=100\n",
        ")\n",
        "pipeline_with_files.evaluate()\n",
        "\n",
        "# Run evaluation pipeline with direct sentences\n",
        "pipeline_with_sentences = EvaluationPipeline(\n",
        "    evaluators=[neural_evaluator, hmm_evaluator],\n",
        "    test_sentences=test_sentences,\n",
        "    actual_sentences=actual_sentences,\n",
        "    use_sentences=True\n",
        ")\n",
        "pipeline_with_sentences.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTuaiuXM9Oma",
        "outputId": "865fb276-61e0-41ea-d73b-9d3cedcd9584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-7be8ff1f0dbe>:36: FutureWarning:\n",
            "\n",
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing 3-gram model\n",
            "Loaded neural model from final_model.pt\n",
            "Loaded existing 5-gram model\n",
            "Loaded pre-trained 5-gram model from models/5gram_model.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning:\n",
            "\n",
            "Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 40 variables. \n",
            "\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning:\n",
            "\n",
            "Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded LSTM model from model1.h5 with weights /content/model1.weights.h5\n",
            "Loaded LSTM model from model2.h5 with weights /content/model2.weights.h5\n",
            "\n",
            "Evaluating with CBHGHMMModelEvaluator\n",
            "Average WER: 0.7918\n",
            "Average DER: 0.2082\n",
            "\n",
            "Evaluating with HMMModelEvaluator\n",
            "Average WER: 0.8188\n",
            "Average DER: 0.1812\n",
            "\n",
            "Evaluating with LSTMEvaluator\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Average WER: 0.8144\n",
            "Average DER: 0.1856\n",
            "\n",
            "Evaluating with LSTMEvaluator\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Average WER: 0.8105\n",
            "Average DER: 0.1895\n",
            "\n",
            "Evaluating with CBHGHMMModelEvaluator\n",
            "Average WER: 0.1702\n",
            "Average DER: 0.8298\n",
            "\n",
            "Evaluating with HMMModelEvaluator\n",
            "Average WER: 0.3330\n",
            "Average DER: 0.6670\n"
          ]
        }
      ]
    }
  ]
}